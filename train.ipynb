{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import fastai.vision.all as fv\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Enable TFloat\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "from torchvision.ops import Permute\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Optional\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "\n",
    "from arch import Net, NetTr, NetMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pl.read_csv('/mnt/ssd/kaggle/test.csv')\n",
    "weightings = pd.read_csv('/mnt/ssd/kaggle/sample_submission.csv', nrows=1)\n",
    "#train_df = pl.read_csv('/mnt/ssd/kaggle/train.csv', n_rows=1_000_000, n_threads=10) # dtypes=[pl.datatypes.String] + [pl.Float32]*924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weightings.iloc[0, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.write_parquet('/mnt/ssd/kaggle/train3.parquet')\n",
    "#test_df.write_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "#test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "train_df = pl.read_parquet('/mnt/ssd/kaggle/train2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config(tbl_cols=-1)\n",
    "train_df = pl.read_parquet('/mnt/ssd/kaggle/train2.parquet')\n",
    "\n",
    "NUM_VERT = 60\n",
    "NUM_VERT_FEAT = 9\n",
    "NUM_VERT_FEAT_Y = 6\n",
    "\n",
    "FEAT_COLS = train_df.columns[1:557]\n",
    "TARGET_COLS= train_df.columns[557:]\n",
    "\n",
    "\n",
    "NUM_2D_FEAT = len(FEAT_COLS) - NUM_VERT*NUM_VERT_FEAT\n",
    "NUM_2D_FEAT_Y = len(TARGET_COLS) - NUM_VERT*NUM_VERT_FEAT_Y\n",
    "\n",
    "# Predict a multiplier of q for q_tends\n",
    "FRAC_IDXS = (NUM_VERT, NUM_VERT*4)\n",
    "\n",
    "NUM_2D_FEAT, NUM_2D_FEAT_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GRID_CELLS = 384\n",
    "emb_idxs = np.arange(len(train_df), dtype=np.int64) % NUM_GRID_CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.select(FEAT_COLS).to_numpy()\n",
    "y_train = train_df.select(TARGET_COLS).to_numpy()*weights[None, :]\n",
    "\n",
    "x_train, x_val, emb_train, emb_val, y_train, y_val = train_test_split(x_train, emb_idxs, y_train, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = test_df.select(FEAT_COLS).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del train_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3983526/2352397551.py:1: RuntimeWarning: divide by zero encountered in divide\n",
      "  std_weights = 1.0/(weights)\n"
     ]
    }
   ],
   "source": [
    "std_weights = 1.0/(weights)\n",
    "std_weights[weights == 0] = 0\n",
    "assert np.isfinite(std_weights).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000130088962e-15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Norm():\n",
    "    def __init__(self, fname=None, stds=None, means=None, zero_mask=None, dataset=None, eps=1e-14):\n",
    "        if dataset is not None:        \n",
    "            self.means, self.stds = np.mean(dataset, axis=0), np.std(dataset, axis=0)\n",
    "            with open(fname, 'w') as f:\n",
    "                f.write(json.dumps({'means' : self.means.tolist(), 'stds' : self.stds.tolist()}))\n",
    "        elif means is not None and stds is not None:\n",
    "            self.stds = stds.copy()\n",
    "            self.means = means.copy()\n",
    "        else:\n",
    "            with open(fname) as f:\n",
    "                stats_dict = json.loads(f.read())\n",
    "                \n",
    "            self.means = np.asarray(stats_dict['means'])\n",
    "            self.stds = np.asarray(stats_dict['stds'])\n",
    "            \n",
    "        self.means = self.means[None, :]\n",
    "        self.stds = self.stds[None, :]\n",
    "\n",
    "        self.zero_mask = self.stds[0] <= eps if zero_mask is None else zero_mask\n",
    "         \n",
    "        self.stds[:, self.zero_mask] = 1.0\n",
    "        \n",
    "        self.eps = eps\n",
    "        #self.df = pd.DataFrame({'col' : names, 'std' : self.stds, 'mean' : self.means})\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        out = (data - self.means) / self.stds\n",
    "        out[:, self.zero_mask] = 0\n",
    "        \n",
    "        return out.astype(np.float32)\n",
    "        \n",
    "    def denorm(self, data):\n",
    "        data = data.astype(np.float64)\n",
    "        out = data * self.stds + self.means \n",
    "        \n",
    "        out[:, self.zero_mask] = 0#self.means[:, self.zero_mask]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 368)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.22775704e-05, 4.44396428e-05, 5.29264435e-05, 6.88978325e-05,\n",
       "       9.13713666e-05, 1.10314264e-04, 1.03480364e-04, 7.88111647e-05,\n",
       "       5.02760849e-05, 3.87126165e-05, 2.95069094e-05, 2.26639477e-05,\n",
       "       1.67192611e-05, 1.25890556e-05, 9.31455907e-06, 7.36806487e-06,\n",
       "       6.69344763e-06, 7.78252797e-06, 1.08996301e-05, 1.37459374e-05,\n",
       "       1.50304659e-05, 1.58900893e-05, 1.76646390e-05, 2.02129868e-05,\n",
       "       2.32400416e-05, 2.70909641e-05, 3.17592094e-05, 3.71773858e-05,\n",
       "       4.28878302e-05, 4.88764999e-05, 5.43901442e-05, 5.84405570e-05,\n",
       "       6.12077129e-05, 6.30606082e-05, 6.41811348e-05, 6.45261607e-05,\n",
       "       6.40522412e-05, 6.32996162e-05, 6.26008696e-05, 6.19947605e-05,\n",
       "       6.14956589e-05, 6.10802963e-05, 6.09866947e-05, 6.12540025e-05,\n",
       "       6.16194520e-05, 6.17596161e-05, 6.11949872e-05, 6.00755338e-05,\n",
       "       5.88045259e-05, 5.73713623e-05, 5.58433348e-05, 5.42547896e-05,\n",
       "       5.25417854e-05, 5.07579280e-05, 4.89998238e-05, 4.76935238e-05,\n",
       "       4.71822132e-05, 4.74191620e-05, 5.14458443e-05, 7.31106265e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.14740942e-12, 9.23171912e-13, 6.80111349e-12, 2.81246137e-11,\n",
       "       2.84467744e-11, 2.16983896e-11, 4.05033680e-11, 8.81579462e-11,\n",
       "       1.85300997e-10, 4.08271084e-10, 8.83215945e-10, 1.72548309e-09,\n",
       "       3.02829117e-09, 4.81663243e-09, 6.91657220e-09, 9.09642672e-09,\n",
       "       1.12731184e-08, 1.35465053e-08, 1.57193192e-08, 1.74671690e-08,\n",
       "       1.88764471e-08, 2.01397015e-08, 2.14845954e-08, 2.31655175e-08,\n",
       "       2.51734598e-08, 2.75351812e-08, 3.02503089e-08, 3.34794272e-08,\n",
       "       3.70612163e-08, 4.09563548e-08, 4.48960940e-08, 4.86536891e-08,\n",
       "       5.20393151e-08, 5.50427579e-08, 5.71367984e-08, 5.82413300e-08,\n",
       "       5.88049112e-08, 5.88288032e-08, 5.85381059e-08, 5.80451029e-08,\n",
       "       5.73203067e-08, 5.63145370e-08, 5.45075203e-08, 5.15448662e-08,\n",
       "       4.87644982e-08, 4.45044783e-08, 4.26766249e-08, 3.81618754e-08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 1.00000000e-15, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 1.00000000e-15, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 2.72195515e-13, 2.69247428e-12,\n",
       "       7.03276257e-11, 3.33126471e-10, 6.97950364e-10, 1.13045473e-09,\n",
       "       1.78530624e-09, 2.59032085e-09, 3.48029650e-09, 4.49027171e-09,\n",
       "       5.52274937e-09, 6.46743237e-09, 7.24145588e-09, 7.89853072e-09,\n",
       "       8.47688675e-09, 9.00853703e-09, 9.50688683e-09, 9.98321426e-09,\n",
       "       1.04636824e-08, 1.09340483e-08, 1.12521876e-08, 1.12574838e-08,\n",
       "       1.09411449e-08, 1.03535607e-08, 9.56867297e-09, 8.62849081e-09,\n",
       "       7.57921992e-09, 6.47278764e-09, 5.46006751e-09, 4.64789052e-09,\n",
       "       4.04598977e-09, 3.74822617e-09, 3.58275098e-09, 2.69875300e-09,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.13937971e-12, 8.51189236e-13, 7.87026314e-13, 4.60250137e-11,\n",
       "       3.16510096e-10, 9.16881515e-10, 1.37368850e-09, 2.60145994e-09,\n",
       "       3.43893980e-09, 4.29731939e-09, 5.06412556e-09, 5.73687586e-09,\n",
       "       6.22911545e-09, 6.51181109e-09, 6.57375221e-09, 6.53101440e-09,\n",
       "       6.49538112e-09, 6.50471366e-09, 6.46000364e-09, 6.12196649e-09,\n",
       "       5.55515012e-09, 4.99190422e-09, 4.52991689e-09, 4.14437462e-09,\n",
       "       3.80417742e-09, 3.51557006e-09, 3.27138339e-09, 3.05245562e-09,\n",
       "       2.85328428e-09, 2.67404987e-09, 2.51005949e-09, 2.36111708e-09,\n",
       "       2.22001684e-09, 2.08830619e-09, 1.96772798e-09, 1.86112425e-09,\n",
       "       1.76412085e-09, 1.68174963e-09, 1.61364266e-09, 1.55910052e-09,\n",
       "       1.50763602e-09, 1.45080004e-09, 1.39235257e-09, 1.34417411e-09,\n",
       "       1.31272160e-09, 1.29380961e-09, 1.24377930e-09, 1.29458422e-09,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.16739977e-07, 5.00172860e-07, 1.10541669e-06, 2.30508431e-06,\n",
       "       4.82622227e-06, 9.27333258e-06, 1.73466815e-05, 2.46265845e-05,\n",
       "       2.09468526e-05, 1.93538308e-05, 1.77184957e-05, 1.65432939e-05,\n",
       "       1.53253386e-05, 1.39041676e-05, 1.27340554e-05, 1.19872020e-05,\n",
       "       1.14893683e-05, 1.10632045e-05, 1.06402913e-05, 1.02482109e-05,\n",
       "       9.85918996e-06, 9.55742507e-06, 9.46207638e-06, 9.61884507e-06,\n",
       "       1.00350908e-05, 1.06055222e-05, 1.11713243e-05, 1.16407036e-05,\n",
       "       1.20794857e-05, 1.26338737e-05, 1.33604435e-05, 1.42088202e-05,\n",
       "       1.52858338e-05, 1.66795235e-05, 1.82600779e-05, 1.98560392e-05,\n",
       "       2.16391345e-05, 2.38109260e-05, 2.65307935e-05, 2.95554255e-05,\n",
       "       3.14010213e-05, 3.13144410e-05, 3.17916383e-05, 3.32166237e-05,\n",
       "       3.70949747e-05, 3.60229969e-05, 3.34970500e-05, 5.22645460e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.31234614e-07, 3.17622209e-07, 7.64283527e-07, 1.85008480e-06,\n",
       "       4.64603909e-06, 9.75165221e-06, 1.46084203e-05, 1.97267491e-05,\n",
       "       1.94221830e-05, 1.91921008e-05, 1.85118715e-05, 1.79031722e-05,\n",
       "       1.65707497e-05, 1.44946525e-05, 1.26427221e-05, 1.14188706e-05,\n",
       "       1.06203834e-05, 9.81319317e-06, 8.94878758e-06, 8.16588818e-06,\n",
       "       7.57078578e-06, 7.09011238e-06, 6.83274357e-06, 6.85151645e-06,\n",
       "       7.16862360e-06, 7.78156482e-06, 8.57151281e-06, 9.30592705e-06,\n",
       "       9.97407096e-06, 1.06259777e-05, 1.13071137e-05, 1.20867971e-05,\n",
       "       1.29767186e-05, 1.40186530e-05, 1.51515778e-05, 1.61814933e-05,\n",
       "       1.71711090e-05, 1.82764907e-05, 1.96750298e-05, 2.17112010e-05,\n",
       "       2.45457431e-05, 2.75210659e-05, 2.94277124e-05, 2.97710176e-05,\n",
       "       2.94213805e-05, 2.75687617e-05, 2.42816604e-05, 3.42534695e-05,\n",
       "       2.46693466e+02, 7.20335312e+01, 7.40028749e-09, 8.18349477e-08,\n",
       "       1.10247169e+02, 1.16415993e+02, 4.64321098e+01, 2.97334728e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 556)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y_train*weights[None, :]\n",
    "# my = y.mean(axis=0)\n",
    "# sy = (y*y).mean(axis=0)\n",
    "# sy[sy < 1e-14] = 1e-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # with open('stats_y_squared.json', 'w') as f:\n",
    " #    f.write(json.dumps({'means' : my.tolist(), 'stds' : sy.tolist()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x = Norm(fname='x_stats8.json', eps=1e-7)\n",
    "# Set means to zero for q vars so we can predict a multipler\n",
    "norm_x.means[:, FRAC_IDXS[0]:FRAC_IDXS[1]] = 0.0\n",
    "\n",
    "norm_y = Norm(stds=np.ones(y_train.shape[1]), means=np.zeros_like(std_weights), zero_mask=std_weights<1e-13)# dataset=y_train)\n",
    "# This variable still seems to have norm issues.\n",
    "indxs = [TARGET_COLS.index(a) for a  in ['ptend_q0002_26', 'ptend_q0002_25']]\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indxs] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable still seems to have norm issues.\n",
    "indxs = [TARGET_COLS.index(a) for a  in ['ptend_q0002_26', 'ptend_q0002_25']]\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indxs] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_t = norm_y(y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_norm = norm_x(x_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_norm.max(), pred_norm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = norm_y.denorm(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_train[0:1000] - pred).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    def __init__(self, data_dict, norm_dict):\n",
    "        self.data_dict = data_dict\n",
    "        self.norm_dict = norm_dict\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_dict['x'][idx].copy()\n",
    "        y = self.data_dict['y'][idx].copy() if 'y' in self.data_dict else np.zeros(1)\n",
    "        x = self.norm_dict['x'](x[None, :])[0]\n",
    "        y = self.norm_dict['y'](y[None, :])[0] if 'y' in self.data_dict else np.zeros(1)\n",
    "        emb_idx = self.data_dict['emb'][idx]\n",
    "        return (x.astype(np.float32), emb_idx), y.astype(np.float32)\n",
    "    \n",
    "    def __len__(self,):\n",
    "        return self.data_dict['x'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 200000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_train), len(emb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Loader({'x' : x_train, 'y' : y_train, 'emb' : emb_train}, {'x' : norm_x, 'y' : norm_y})\n",
    "val_ds = Loader({'x' : x_val, 'y' : y_val, 'emb' : emb_val}, {'x' : norm_x, 'y' : norm_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 200000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds), #len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "bs = 128\n",
    "train_loader = fv.DataLoader(train_ds, batch_size=bs, drop_last=True, \n",
    "                          shuffle=True, num_workers=0, pin_memory=False)\n",
    "valid_loader = fv.DataLoader(val_ds, batch_size=bs, drop_last=True,\n",
    "                             shuffle=False, num_workers=0, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# x = batch[0]\n",
    "# y = batch[1]\n",
    "# x[0].mean(), x[0].std(), y.mean(), y.std(), x[0].min(), x[0].max(), y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[0][:, FRAC_IDXS[0]:FRAC_IDXS[1]].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.compile(net)\n",
    "dls = fv.DataLoaders(train_loader, valid_loader).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(NUM_2D_FEAT, NUM_VERT_FEAT, NUM_2D_FEAT_Y, NUM_VERT_FEAT_Y, frac_idxs=None, \n",
    "          dim=512, depth=8)\n",
    "#net(batch[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.compile(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(pred, tar):\n",
    "    diff = torch.abs(tar - pred)\n",
    "    return diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_total = np.mean((y_val - y_val.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88578206"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, tar):\n",
    "    diff = tar - pred\n",
    "    return ((diff)**2).mean()\n",
    "\n",
    "def r_squared(pred, tar, mask=None):\n",
    "    # tar = norm_y.denorm(tar.cpu().numpy())\n",
    "    # pred = norm_y.denorm(pred.detach().cpu().numpy())\n",
    "    #pred = pred*weights[None, :]\n",
    "    #tar = tar*weights[None, :]\n",
    "    # mask = ~norm_y.zero_mask\n",
    "    # if mask is not None:\n",
    "    #     tar = tar[:, mask]\n",
    "    #     pred = pred[:, mask]\n",
    "    #tar_m = norm_y.means[:, mask]\n",
    "    return 1 - (torch.mean((tar - pred)**2) / s_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(temp_data['model'])#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = fv.Learner(dls, net, loss_func=nn.HuberLoss(delta=10.0), metrics=[fv.mae, fv.mse, r_squared], #nn.HuberLoss(delta=2.0)\n",
    "                   wd=0.001, opt_func=fv.ranger, cbs=[fv.SaveModelCallback(monitor='r_squared', comp=np.greater,)\n",
    "                                                      , fv.GradientClip()]).to_fp16()#.load('model_temp') #fv.SaveModelCallback(monitor='r_squared', comp=np.greater,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'hook_fn' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/hook.py:21)\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: ___check_type_id(L['module'], 97391516797472)               \n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'to_detach' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:237)\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: tensor 'L['b'][0]' stride mismatch at index 0. expected 30720, actual 122880\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: '_track' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/hook.py:149)\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: ___check_type_id(L['m'], 97391516797472)                    \n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'apply' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:220)\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: tensor 'L['x']' stride mismatch at index 0. expected 122880, actual 61440\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab269bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_119'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb91120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_60'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e0b04c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_59'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2239e8c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_70'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e226c3d00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_10'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2058fa30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_71'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2191a0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_11'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabb817e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_80'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22d2b370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_20'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabd7edd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_81'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2038dea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_21'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabb11240>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_78'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21976170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_18'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabd7f1c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_76'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb079a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_74'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20615ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_16'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabc29990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_77'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22389bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_17'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ea21900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_91'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21cfb370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_31'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab853010>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_99'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2095c160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_39'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab73e290>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_97'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21a5f130>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_37'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab91a050>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_95'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e208aa5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_93'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22c532e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_35'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab885870>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_96'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21daa0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_36'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e223ce3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_62'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22e56f80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_2'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2058e680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_72'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20f40790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_12'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0dffb9a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_82'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e202fa680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_22'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ec05870>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_90'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2081a9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_30'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f3c2cb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_68'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e228c8670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_8'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f264e50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_88'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f743520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_28'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e02b520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_66'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2056d6c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_64'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ed5360>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_6'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ebbb010>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_67'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ba7910>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_7'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e0b3640>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_86'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e207ac670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_84'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f78f400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_26'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e920160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_87'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f4635b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_27'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20aee4d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_108'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ee43400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_48'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab127ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_118'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20c07400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_58'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab41c820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_117'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e226e05e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_57'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb99990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_109'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20d011b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_49'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab448a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_115'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2238a320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_55'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabc2a680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_113'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab945b40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_111'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0edc0670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_53'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab6511b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_114'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f56fbe0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_54'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab793b50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_100'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ea33f40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_40'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f375ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_106'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ee15120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_46'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f233d00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_104'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e208aac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_102'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e223cc160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_44'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e49f7f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_105'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f230af0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_45'\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule (Input shape: 128 x torch.Size([128]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     128 x 960 x 1       \n",
       "Conv1d                                    1920       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 16 x 60       \n",
       "Rearrange                                                      \n",
       "____________________________________________________________________________\n",
       "                     128 x 9 x 60        \n",
       "Rearrange                                                      \n",
       "____________________________________________________________________________\n",
       "                     128 x 496 x 60      \n",
       "Conv1d                                    4960       True      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Rearrange                                                      \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Rearrange                                                      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 6 x 60        \n",
       "Conv1d                                    3078       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 360           \n",
       "Rearrange                                                      \n",
       "Linear                                    129960     True      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 8 x 60        \n",
       "Conv1d                                    4104       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 8             \n",
       "Reduce                                                         \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 72,678,102\n",
       "Total trainable params: 72,678,102\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function ranger at 0x764daf6353f0>\n",
       "Loss function: HuberLoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - CastToTensor\n",
       "  - MixedPrecision\n",
       "  - GradientClip\n",
       "  - Recorder\n",
       "  - ProgressCallback\n",
       "  - SaveModelCallback"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ab7370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_4'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22d2b400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_14'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f798dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_24'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20db3760>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_33'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e229b09d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_42'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e33dbd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_51'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e238bf0a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_0'\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0005207948270253837)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=300, end_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.00022387212084140629)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAG1CAYAAAD6GvACAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACDMklEQVR4nO3deXhU5fk38O8syWTfd8jGGvYlgCwCoqxaRK0Fl4K0UEHQirQ/lRdsLS7Yiko3VNSqWEVcqEulQlCQICoQA7LvISErWSf7JDPn/ePMObNkkkySyZwk8/1cVy7JmTMnzxwhc8/93M/9qARBEEBERETkodRKD4CIiIhISQyGiIiIyKMxGCIiIiKPxmCIiIiIPBqDISIiIvJoDIaIiIjIozEYIiIiIo/GYIiIiIg8mlbpAXRFJpMJeXl5CAwMhEqlUno4RERE5ARBEFBZWYm4uDio1c7nexgMOZCXl4f4+Hilh0FERETtkJOTg969ezt9PoMhBwIDAwGINzMoKEjh0RAREZEz9Ho94uPj5fdxZzEYckCaGgsKCmIwRERE1M20tcSFBdRERETk0RgMERERkUfjNFkHGI1GNDQ0KD2MHsvLywsajUbpYRARUQ/HYKgdBEFAQUEBysvLlR5KjxcSEoKYmBi2OCAiok7DYKgdpEAoKioKfn5+fKPuBIIgoKamBkVFRQCA2NhYhUdEREQ9FYOhNjIajXIgFB4ervRwejRfX18AQFFREaKiojhlRkREnYIF1G0k1Qj5+fkpPBLPIN1n1mYREVFnYTDUTpwacw/eZyIi6mwMhoiIiMijMRgiIiIij8ZgSEkmI3A5HTj+kfhfk1HpEbUoKSkJmzZtkr9XqVT45JNPFBsPERGRK3A1mVJOfQZ8+Rigz7McC4oDZv8ZGHyrcuMiIiLyMIpnhjZv3ozk5GT4+PggNTUV6enpzZ574MABTJo0CeHh4fD19UVKSgpeeuklm3Nee+01TJ48GaGhoQgNDcX06dNx6NChzn4ZbXPqM+CDRbaBEADo88Xjpz5TZlxERESd7ERuBea/8h2e+u8ppYciUzQY2r59O1atWoW1a9ciMzMTkydPxpw5c5Cdne3wfH9/fzz44IPYv38/Tp8+jXXr1mHdunXYsmWLfM6+fftw9913Y+/evfjuu++QkJCAmTNnIjc3110vq2Umo5gRguDgQfOxLx93+ZTZq6++il69esFkMtkcv/XWW3Hffffh4sWLmDdvHqKjoxEQEICxY8diz549bfoZubm5WLBgAUJDQxEeHo558+YhKysLALB//354eXmhoKDA5jm/+93vMGXKlA69NiIi6j4uF1fjUFYpfrparvRQZIoGQy+++CKWLFmCpUuXYtCgQdi0aRPi4+Px8ssvOzx/1KhRuPvuuzFkyBAkJSXhl7/8JWbNmmWTTXr33XexYsUKjBw5EikpKXjttddgMpnw1VdfuetltezKwaYZIRsCoM8Vz3OhX/ziFyguLsbevXvlY2VlZdi1axfuvfdeVFVV4eabb8aePXuQmZmJWbNmYe7cuc0GpvZqamowbdo0BAQEYP/+/Thw4AACAgIwe/ZsGAwGTJkyBX369ME777wjP6exsRH//ve/8atf/cqlr5WIiLqu/IpaAEBciK/CI7FQLBgyGAzIyMjAzJkzbY7PnDkTBw86FwhkZmbi4MGDmDp1arPn1NTUoKGhAWFhYc2eU19fD71eb/PVaaoKXXuek8LCwjB79my899578rEPP/wQYWFhuOmmmzBixAgsW7YMw4YNQ//+/fH000+jT58++Owz56bs3n//fajVarz++usYNmwYBg0ahDfffBPZ2dnYt28fAGDJkiV488035ed88cUXqKmpwfz58136WomIqOvKK68DAMQGMxhCcXExjEYjoqOjbY5HR0c3mUqx17t3b+h0OowZMwYrV67E0qVLmz338ccfR69evTB9+vRmz9mwYQOCg4Plr/j4+La9mLYIiG79nLac1wb33nsvPv74Y9TX1wMQs2h33XUXNBoNqqur8eijj2Lw4MEICQlBQEAAzpw543RmKCMjAxcuXEBgYCACAgIQEBCAsLAw1NXV4eLFiwCAxYsX48KFC/j+++8BAP/6178wf/58+Pv7u/y1EhFR15RXLmWGfBQeiYXiq8nsOwwLgtBq1+H09HRUVVXh+++/x+OPP45+/frh7rvvbnLeX/7yF2zbtg379u2Dj0/zN33NmjVYvXq1/L1er++8gChxorhqTJ8Px3VDKvHxxIku/9Fz586FyWTCF198gbFjxyI9PR0vvvgiAOD//u//sGvXLmzcuBH9+vWDr68v7rzzThgMBqeubTKZkJqainfffbfJY5GRkQCAqKgozJ07F2+++Sb69OmDnTt3ylkjIiLyDPkVYmYorgtlhhQLhiIiIqDRaJpkgYqKippki+wlJycDAIYNG4bCwkI8+eSTTYKhjRs34tlnn8WePXswfPjwFq+n0+mg0+na8SraQa0Rl89/sAiACrYBkTkInP2ceJ6L+fr64o477sC7776LCxcuYMCAAUhNTQUgBpiLFy/G7bffDgCoqqqSi5+dMXr0aGzfvh1RUVEICgpq9rylS5firrvuQu/evdG3b19MmjSpQ6+JiIi6FykzFNuFMkOKTZN5e3sjNTUVaWlpNsfT0tIwcaLzWRFBEORpH8nzzz+Pp556Cl9++SXGjBnjkvG61OBbgflbgaBY2+NBceLxTuwzdO+99+KLL77Av/71L/zyl7+Uj/fr1w87duzA0aNHcezYMdxzzz1NVp61dt2IiAjMmzcP6enpuHz5Mr755hs8/PDDuHr1qnzerFmzEBwcjKeffpqF00REHqauwYiSanHGgZkhs9WrV2PhwoUYM2YMJkyYgC1btiA7OxvLly8HIE5f5ebmYuvWrQCAf/7zn0hISEBKSgoAse/Qxo0b8dBDD8nX/Mtf/oInnngC7733HpKSkuTMk1TH0mUMvhVIuUVcNVZVKNYIJU7slIyQtRtvvBFhYWE4e/Ys7rnnHvn4Sy+9hF//+teYOHEiIiIi8Nhjj7WpkNzPzw/79+/HY489hjvuuAOVlZXo1asXbrrpJptMkVqtxuLFi/Hss89i0aJFLn1tRETUtRWYp8h8vTQI8fNSeDQWigZDCxYsQElJCdavX4/8/HwMHToUO3fuRGJiIgAgPz/fpoDXZDJhzZo1uHz5MrRaLfr27YvnnnsOy5Ytk8/ZvHkzDAYD7rzzTpuf9cc//hFPPvmkW16X09QaIHmyW3+kRqNBXl7Tpf1JSUn4+uuvbY6tXLnS5nv7aTNBsK15iomJwdtvv93qGPLz83HzzTcjNja21XOJiKjnsJ4ia60+2J0UL6BesWIFVqxY4fCxt956y+b7hx56yCYL5Ehb6lzIvSoqKnD48GG8++67+PTTT5UeDhERuVmeOTPUqwv1GAK6QDBEnmPevHk4dOgQli1bhhkzZig9HCIicrN8KTMU3HWKpwEGQ+RGXEZPROTZ8iqkYKhrZYYU36iViIiIPIPUfbqrTZMxGGon++Jh6hy8z0REPYPJJOBCURWArtVjCGAw1GZeXuJSwJqaGoVH4hmk+yzddyIi6p72nC5Ebnktgny0GJ0QqvRwbLBmqI00Gg1CQkJQVFQEQOyv05WWB/YUgiCgpqYGRUVFCAkJgUbTuf2XiIioc72WfgkAcO/4RPjrulb40bVG003ExMQAgBwQUecJCQmR7zcREXVPP10tx+GsMnhpVFg8MUnp4TTBYKgdVCoVYmNjERUVhYaGBqWH02N5eXkxI0RE1AN8f6kEADBtYBSig7pWvRDAYKhDNBoN36yJiIhacbm4GgCQEhOo8EgcYwE1ERERdSopGEqO9Fd4JI4xGCIiIqJOJQdDEV1ow3QrDIaIiIio01TXN6JQXw8ASA5nZoiIiIg8jJQVCvf3RrBf1+wZx2CIiIiIOo1liqxrZoUABkNERETUiRgMERERkUfr6ivJAAZDRERE5GK1BiP0dWJT4kvmYKhPF84MsekiERERuYzRJODWfxxAWU0DPn5gAs4W6AEAfSO75rJ6gMEQERERudDRnDKcL6oCACx5+wjqGkzoFxWAflFdNxjiNBkRERG5zO5ThfKfL5iDorvGxkOlUik1pFYxGCIiIiKXSTMHQ1Ls461R447RvRUcUesYDBEREZFLXLxWhUvXquGlUeHBaf0AAD8bHoswf2+FR9Yy1gwRERGRS+w9UwQAGN8nHI9MH4DRiaEYlxSm8Khax2CIiIiIXKKgog4AMDg2CGq1CtMGRik8IudwmoyIiIhcoqq+EQAQoOteuRYGQ0REROQScjDkw2CIiIiIPJAUDPkzM0RERESeqJrTZEREROTJKusYDBEREZEHqzZwmoyIiIg8WJU5MxTIAmoiIiLyRNX1RgDMDBEREZEHqm80wmA0AWDNEBEREXkgKSsEAP7eGgVH0nYMhoiIiKjDpHohXy8NtJruFV50r9ESERFRl9RdGy4CDIaIiIjIBaRl9d1tJRnAYIiIiIhcQJom89d1r3ohgMEQERERuUB33bEeYDBERERELsBgqAM2b96M5ORk+Pj4IDU1Fenp6c2ee+DAAUyaNAnh4eHw9fVFSkoKXnrpJZtzTp48iZ///OdISkqCSqXCpk2bOvkVEBERUXfdpBVQOBjavn07Vq1ahbVr1yIzMxOTJ0/GnDlzkJ2d7fB8f39/PPjgg9i/fz9Onz6NdevWYd26ddiyZYt8Tk1NDfr06YPnnnsOMTEx7nopREREHq2yrvuuJlMJgiAo9cOvu+46jB49Gi+//LJ8bNCgQbjtttuwYcMGp65xxx13wN/fH++8806Tx5KSkrBq1SqsWrWqTePS6/UIDg5GRUUFgoKC2vRcIiIiT/T0f0/h9QOXsWxKH6y5eZAiY2jv+7dimSGDwYCMjAzMnDnT5vjMmTNx8OBBp66RmZmJgwcPYurUqR0aS319PfR6vc0XEREROY81Q+1QXFwMo9GI6Ohom+PR0dEoKCho8bm9e/eGTqfDmDFjsHLlSixdurRDY9mwYQOCg4Plr/j4+A5dj4iIyNOw6WIHqFQqm+8FQWhyzF56ejqOHDmCV155BZs2bcK2bds6NIY1a9agoqJC/srJyenQ9YiIiDyNnBnqhk0XFRtxREQENBpNkyxQUVFRk2yRveTkZADAsGHDUFhYiCeffBJ33313u8ei0+mg0+na/XwiIiJPx9Vk7eDt7Y3U1FSkpaXZHE9LS8PEiROdvo4gCKivr3f18IiIiKgNpNVk3TEYUnTEq1evxsKFCzFmzBhMmDABW7ZsQXZ2NpYvXw5AnL7Kzc3F1q1bAQD//Oc/kZCQgJSUFABi36GNGzfioYcekq9pMBhw6tQp+c+5ubk4evQoAgIC0K9fPze/QiIiIs8g7U3WHWuGFB3xggULUFJSgvXr1yM/Px9Dhw7Fzp07kZiYCADIz8+36TlkMpmwZs0aXL58GVqtFn379sVzzz2HZcuWyefk5eVh1KhR8vcbN27Exo0bMXXqVOzbt89tr42IiMiTSHuTdceNWhXtM9RVsc8QERFR2wxY+z8YjCZ8+/iN6BXiq8gYul2fISIiIuoZ6huNMBhNALpnzRCDISIiIuqQ4ioDAMBLo0IggyEiIiLyNAUVtQCA6CAfqNUt9wrsihgMERERUYfkldcBAGKDfRQeSfswGCIiIqIOyTdnhmKDlSmc7igGQ0RERNQhcmYohJkhIiIi8kAFFWIwFMfMEBEREXkiyzQZM0NERETkgfIqpAJqZoaIPIK+rgEHLxaDzduJqLvT1zXgL1+eQVZxdbuvYWg0obhK3DCdNUNEHuK2f3yLe177Aenni5UeSrdWWdeg9BCIPN6r31zE5n0X8ejHP7X7GoX6OggC4K1VI9zf24Wjcx8GQ0RtkFVcjUvmT1D7zl5TeDTd1zfnrmH4n3bjn3svKD0UIo/21ekiAMChy6U4U6Bv1zXyKyw9hlSq7tdwEWAwRNQm24/kyH/W8F9Pux28UAxBAL5hQEmkmKtlNThTUCl//+/vr7TrOt29eBpgMETktAajCR8euSp/X1RZr+BourfL5uzahWtVCo+EyHN9fUbMCoWZp7Z2/JiLitq2T19LPYa667J6gMEQkdMyrpTJRYIAUKRnMNReWSViMFRabUBJFe8jkRL2mKfI7p/SBwOjA1FjMOKN9Ev49GgunvzspM3vu5ZImaGYbpwZ6n5byxIp5GqZ+A9eo1bBaBJQVFmn8Ii6J5NJQFZJjfz9+aIqhAfoFBwRkefJKa3BdxfFRSDTB0UhMcwPD7z7I17ZfwmGRhMA4Ivj+Xjll6ORmhjW4rWk3429Q/06d9CdiJkhIicV6sXgZ1ivYACcJmuvvIpa+ZctIAZDRNQxgiDgne+v4FhOOUwmAVv2X8Re8zSYIy+lnUODUcD1/SLQLyoQs4bEYEhckPxvM9jXC9cq6/Hge5moazACAGoNRpzO1zdpK5JTKn64iQ/jNBlRjye1mx/eWwyGKusa5V8S5Lys4hqb7y8yGCLqsK/PFOGJT07g128dxo7MXDy78wwe+eCow35oZwr0+M/RXADAo7MHAgDUahXW3jwIWrUKt4/qhW8fvxFxwT7Ir6jD2wezcL6wEnP+uh9z/pqO/VZtRQRBkDND8cwMkTOOX63AxA1f4c6XDyo9FGqHAnNmqH9UAHRa8Z8O64ba7rK5XkhagXu+qLKFs4moOZV1DXj0o2P46nQhvjJngUqqDVj7n+MAgPKaBjlQsfb2wSsQBODmYTEY3jtEPj6xXwRO/GkWXpw/AgE6LR6ZMQAA8MLuc5jz13R5evv7SyXyc4qrDKhtMEKl6r4NFwEGQ25lMJqQV1GHQtaadEvSNFl0kA+igsQaF9YNtZ3U6TY1IRQAcL6QmSGi9vj399n44MhVPPrRTzZTYvVW09AncitsnmM0CUg7VQAAuGtsQpNr+nhp5F5Bd4zujZSYQBiMJjSaLBmmk3mWfkRXy8QAKSbIBzqtxgWvShkMhtxIbf4kzF0cuidpmiwm2AdRgeInINYNtZ0UDE0fHA1AvIezN+3HfzKvwmgS8MHhHFxgtoioRYIg4OMfxVYfJdUG5FfUwVurRqifFwDL+82JPNtg6EhWKYqrDAj29cKEvuEt/gyNWoWtS8Zhy8JUpD86Df9ZMREAcCqvQp5+y+kBU2QAV5O5lRRtMxjqfhqNlr13xGDInBnSMzPkrGM55Xh1/0UcyioFIBaiD4wOxNnCSpwpqMSfPj+FYzkVeOtgFob3DsZnD16v8IiJuq6frlbggl293YQ+4Zg/Jh4fZuRgaFww/rH3Ak7k2naV/t8JMSs0fVA0vJzoHBsV6IOZQ2IAABEBOqhV4tTYS2nn8NbBLIxLFgOq3qHdt3gaYGbIrSyZIUZD3c21qnqYBECrViHCX2cJhpgZcsqB88W4+7XvsfN4ASrrGqFWAf2iArB1yTi8tmgMkiP8UV7TgLcOZgEQf9ETUfN2mLNCNwyMhK+XRv7zLcNj8davxuGmQVEAgJNWWRxBELDrpBgMzR4a0+af6eutQd/IAADA376+AH1dI/acLgQA9A5jZoicpDZnhkyMhbodae+dqEAd1GoVooI4TdaaipoGaDUqeGnUWPnej6gxGDGpXzimDohEckQAos33cMZgH5RVG5psFFnXYISPV/etQSDqLCaTgC+O5wMAFk9MwqwhMdh1sgB3jOotnzMoNggatQrFVQYUVdYjOsgH16rqkV9RB7UKmNw/ol0/e0hckMN2GPHMDFFbmZgZ6nYKzcFQtLnDaiQzQy2qrGvA1I17ces/DiCvvBYVtQ3w8VLjX4vH4v4pfTHDXC8kuW1UL/QKsf1l6mgVDJGnOJFbgfmvfoeMK2VNHjt6tRzFVQYE6rSY1C8Cd49LwFu/Godgc70QIBZC9430l68FAIUV4u+riABduz9oDI4Lcng8vptnhhgMuZGUGWIo1P1Iy+pjzNkM1gy1LDO7HOU1Dbh4rVreCTsuxLfZ1SbeWjX+vfQ6vHHfGAyMDgQA5JYzGCLP9enRXBy6XIr3D2U3eexr8zYaUwZGtlj3MzhWDFykzVjl32Md2DZjSJzYZ02lAm4ZHisfZ80QOU3FmqFuq8BqWT0AeTWZs3v3eJpjOeXyn7+/JBZM22d+7CVH+OOmQdHoZf6lKi3ZJfJEZTXihqnSPn7WpDqd6ea6oOZI9T2XronXkH6PSb+/2mNcchjmjojD72YMwP2T+wAAdFq1/EGxu2LNkBupuZqs2yqssP1EFREo7vJcWm2A0SRAI1XHe7jdJwsQF+KLo1bB0KHLYjDk7I7W0ifMXE6TkQcrNwdDl+06tl8tq8GZgkqoVcANA1oOhvpIwVCxWONTJGeG2r8XoJdGjb/fPQqA+MF+/bwhCPfXQevEyrSujMGQG0mZIdYMdT/202RhfmIwZBKAshoDIrjRKHJKa3D/OxkAIPc6AYDT5mkyZ7vTShkk1gyRJ6uoNQAQs8+VdQ0I9BH/TX1lniJLTQxFqL93i9foY64ZunStGoIgWHqluSiLo1KpsGhCkkuupbTuHcp1M/LSemWHQe0gvTHHmd+otRpLc7OSKoNi4+pKrLtxSyl+wJIJjWtlmkwi7XzNmiHyZOVW/4aulFiyQ/87Ia4imzWk9aXxSeFiMFRR24CymoYm0/1kwWDIrcxL67m2vlsxNJqQZ35jTgq3rJiQskGsGxLVGkwtPt5azZCkN2uGiFBeawmGLpu7thdX1cvTzs4EQ77eGvnf3aVrVfKWQh0poO6pGAy5ETND3dPVshqYBMDXSyMvqQeA8AAxRc1gSFRjaLT5PjnC3+b7WCd/AUsF1EWV9ahvNLpmcETdiCAIqLDKDElb2Ow+WQiTIHZvd3Ypu/VUmaunyXoSBkNuxO04uicpRZ0Y7if/PwSsM0Ndc5qs0dhypsbVahssgYuXRoUHpva1edzZabJwf2/4eKkhCEB+OVsXkOepbTDCYPXvV9otXpoia0v3aOlDyal8PfR14geWKAZDTTAYciNux9E9XTEvbU0Mt/0kJgVDJV0wM5RVXI1R69Pw7M7TbvuZ1fViMDR9UDTOPjUH80bFyYsGxADHuSZvKpWKRdTk0azrhQBxeX2j0SRPkc20a1rakj7mYOj7SyUAxAx3kA/XTtljMORGKnA7ju5I+lQmFSNKIszTZF2xgPpwVikq6xvx+bG8dj1/63dZeDHtXJueI02T+XlroFaroNNqEG3uZ+JsVkgi3WtpSTCRJ2kSDBVXI6ukBvWNJvh6WfYHc4a0vF5qvBgT7GOT4SYRgyE3kpsusmqoW8kulabJbIOh8C5cQC1tE5JfUYfS6rYFa0aTgKf+ewp/++o8ckqdL2KuNYiZIX+dJQMkFUM7Wy8kGRAjdqE+a/4FTuRJymvEf7NShrSk2oAfLouZnQExgVC3oa9Z/2jbwCk6iG1AHGEw5EaWPkPKjoPaJquZabJwc4+P4jYGG+5wzWrPtFN5+jY9t7zGgAaj+Jc0uw3BULU5GPL1sqTgpWCorZkhaUuOc4UMhsjzSCvJ4kJ85Gmuf38vbssxyPxBwVmxwb64bWSc/H24P4MhRxgMuZFaxeVk3Y3RJMjZkSY1Q+aVZcVdcLNW654/p/Ir2vTcEqvgri3L22vN02TWmaGfDY9DrxBfp5YBWxtolRlijR15is+P5WHk+t3YfbIAABDs643r+oQDAE7nix9qBrYxGAKAP8wdIv/Zz7t9G7T2dAyG3EgKhtiBuvvIr6hFg1GAl0aFWLvtJCLMn7BKquu73Bt2kd4SoJ1sY2bIugaqLQXMcmbI6pft9MHR+PbxGzGhb3ibxtAn0h8atQr6ukYU6uvRYDRh9fajeGT7UZzMa1twR9RdbNl/CeU1DfjUXOsX4ueF8X3CbM5JiXG8a3xLwvy9sfXX45CaGIpfX5/skrH2NCwpdyMmhrofaVl9fJhfk/3HpP3J6hpMqDYYEaDrOv+ciio7EAxVW57blmBIqhnyc3LVWEt0Wg2SI/xxoagKZwsr4XVNhR2ZuQCA/2Tm4s3FYzEtpeV9mYi6k9zyWhzPFQN96bNViK8Xxvex/SCR0o7MEABMGRCJKQMiOzTGnkzxzNDmzZuRnJwMHx8fpKamIj09vdlzDxw4gEmTJiE8PBy+vr5ISUnBSy+91OS8jz/+GIMHD4ZOp8PgwYPxn//8pzNfgtO4N1n389NV8ZfTgKimv4D8vLXwNb/xd6Xl9YIg2EyTXbpWJQcqziht5zSZvJrMRUGhXDdUUIljV22zQd+cu+aSn0HUVUhTY9ZC/LwQHWSpG4oO0rW6Hxm1j6LB0Pbt27Fq1SqsXbsWmZmZmDx5MubMmYPs7GyH5/v7++PBBx/E/v37cfr0aaxbtw7r1q3Dli1b5HO+++47LFiwAAsXLsSxY8ewcOFCzJ8/Hz/88IO7XlazpKX1jIW6D2kFx7jkMIePS9mhrtR4sbK+EXUNYsO2YF8vmATLZqnOsH4tOaVtnyZzVU3CAHMwdLawEsdyys3HxJUxUv0EUU+x+2Rhk2PB5g2hx5unmQe2Y4qMnKNoMPTiiy9iyZIlWLp0KQYNGoRNmzYhPj4eL7/8ssPzR40ahbvvvhtDhgxBUlISfvnLX2LWrFk22aRNmzZhxowZWLNmDVJSUrBmzRrcdNNN2LRpk5teVfOsZ1m6Wo0JNWU0CTiSVQag+WBIWpnRluX1J/Mqmmxd4UpSvVCgTouxSeK4v7tY4vTzrbNchZV1Tm+JUeviYGhgjBj4nMzT46er5QCABWMTAIg9U/hviHoKfV0DDmWJDRWlDaABcZoMAH55XSL6RvrjnnEJiozPEygWDBkMBmRkZGDmzJk2x2fOnImDBw86dY3MzEwcPHgQU6dOlY999913Ta45a9asFq9ZX18PvV5v89UZrBtdcXl913cqT4+q+kYE+mgxKNbxJzKp8eK7P2TbTE01590fruCWvx3AU/895dKxWpPGERmkww0DxRqBvWeKnH6+9TRZW7bEsDRddM00WWpiGLRqFU7n65FXUQeVCrh9VC9o1CpU1Fp24Cbq7q6W1sJoEhDu742JfSPk4yHmwGhwXBC++t0NbdqGg9pGsWCouLgYRqMR0dG2bcWjo6NRUNB07tRa7969odPpMGbMGKxcuRJLly6VHysoKGjzNTds2IDg4GD5Kz4+vh2vqHXMDHUv0hTZ2KSwJsXTkrvGJkCjVmH/uWuY/8p3MDQ2vx9YSVU9/vy/MwDalqlpK6nHUFSgTi4y/jG7TG7k1hr7jtrOFlHXuDgzFBmowyyrX/59IwMQ5u+NvuaNJzlVRj2F9AEkPMBbnh4GgFA/1ge5i+IF1PZtwQVBaLVVeHp6Oo4cOYJXXnkFmzZtwrZt2zp0zTVr1qCiokL+ysnJaeOrcI5UMwQwM9Qd/GDeB6i5KTJAXDr+2YOTEBGgQ1ZJDT45mtvsuRt3n5U3SrxSWoPq+s6ZKpOmyaICfdArxBcDowNhEpovOr5SUo2n/3sKheZMi7SazN8c1DhbRO3qYAgAFo1PlP88oncIAMvS4tP5bMhIPYP0by7M31ueHgbEmj9yD8WCoYiICGg0miYZm6KioiaZHXvJyckYNmwYfvOb3+CRRx7Bk08+KT8WExPT5mvqdDoEBQXZfHUGldXd5pYcXVtdg1HO3tgvbbU3JC4Yv5ks9u54ed9FrP/8FDbvu9DkvP8eE3ecVqvE6acznbTVhDRNFmVuCnlDijhV9tVpx1NlL++7iNcPXMbW77IAWJouDu0VDMD5zJClZsh1LQbGJYfJq8pGJYQAAFJixe876/4RuZucGfLX2RRJB/sxGHIXxYIhb29vpKamIi0tzeZ4WloaJk6c6PR1BEFAfb2l4HPChAlNrrl79+42XbOzWOemOEvWtX1z7hqq6hsRG+yD4eagoCX3jk9EkI8Wl4ur8a9vL+MvX55F+nlLJqbG0IhKcyZoVEIoAOBUJ03zSD2Gosx7EEk7XH92LA+fOshcSeM4X1iFBqNJ3iRyRHwIAOcyQw1GEwxGcYrQlZkhlUqFv909Cr+9sR/uTO0NABhkfrM4w2ky6iGkYCjM3xuJYX64KSUKs4fEILAL9S7r6RS906tXr8bChQsxZswYTJgwAVu2bEF2djaWL18OQJy+ys3NxdatWwEA//znP5GQkICUlBQAYt+hjRs34qGHHpKv+fDDD2PKlCn485//jHnz5uHTTz/Fnj17cODAAfe/QDtqq6k6BkNd2xc/iVmcm4fFOrUpYoBOi9/e1B9Pf3EaMUE+KNDX4c9fnsGkvhFQq1VyHY+Plxpjk8KQcaWs02perKfJALEQ+VeTkvDmt1n4/YfHkBITJLf0N5oEef+vS8XVKDPXFalUwGBz0Xh+ReuFyjVWfYxcmRkCxO0HBsYMlL8fEieO68K1KmSX1CDBbpsUou5GamcRHuANtVqFNxaPVXhEnkfRYGjBggUoKSnB+vXrkZ+fj6FDh2Lnzp1ITBTrBPLz8216DplMJqxZswaXL1+GVqtF37598dxzz2HZsmXyORMnTsT777+PdevW4YknnkDfvn2xfft2XHfddW5/ffasy5bYeLHrqmswYs9psefHLcNjnX7e0sl9cNe4BNQ3GDH1+X04kavHzhP5+NnwOEu2JtAHg+OkmpfOCYaumZfGRwZaNmR84pbBOJmrx6GsUnx7oVgOhrJLa+SeRFdKquVAKszPG73Mm6w6FwyJWS+tWgVvbecmnKOCfDC5fwTSzxfj7e+y8MTPBnfqzyPqbKXmmqFwNlRUjOI5uBUrVmDFihUOH3vrrbdsvn/ooYdsskDNufPOO3HnnXe6YnguZZMZUnAc1LL9566hxmBErxBfjDJPFTkrQKdFgE6Le65LwJb9l3DgfDF+NjzOZoXX4FjLJqQmk+BU5qktKuvEaS7r4ku1WoXhvYNxKKsUeeWWGqCzVs0YG4wCjpqbG4YHeCMmSMwsFVTUtboIocbBvmSd6dfXJyP9fDG2H87Bqun9EejD2grqvizTZNxRXimKrybzJMwMdQ/Z5l3qRyeGtrqysTn9IsUVIXnmrEqReaVWZKAOyREB8PFSo8ZgRFZJtQtGbKvKvGIt0Mf2s46U6cm1CobsV2QdMTd+C/P3RnSQD1QqwGA02fQeckQqnvZ38RRZc6b2j0SfSH9U1Tfik6N5bvmZRJ2lxKpmiJTBYMiNrJfWMxbquqQC4tAOrOSIDZGyKmLgIU1dRQXqoFGr0CdCDJZcHQwZTYK8LYb9xrFxIU2DobN2K7IOmztuhwfo4K1VIyJA/KTa2lSZ1CbAlcXTLVGrVbh1RBwA4CdzNouou7LuM0TKYDDkRmy62D1IRcQhHWh4FhtsrrcplzJDtnU88WHi423ZFd4Z1VbbfATYZ4bMwZDNNJm5eHq0edm6FChJy/Jjgy1TZS2paXDvNBkA9IsSA8qL16rc9jOJXK3RagUnM0PKYTDkRiquJnO5itoG5JintXb8eBXL3jnS4WaG5bUuyAyZg4jK+kZU1jXYFFADQO9QcQWUq4MhaYrMW6OGTmsbmPQ2T5MVVxlQ12BErdU03c3DbAvFfz5aXMYu1Q3lmzNc//7+Cla8m9FkbzV3T5MBYkdqALh4rZofLqjbKjMHQioVO04ricGQG6lZM+Ryy9/JwE0vfoPLxdV4Yfc57DpZiPTzxR26ZrmcGWp/MOSv0yLInJkpqKiTC6ilzJAUmEiBnKtUmQNB+6wQIBZUS9NYueW1yC2vgSCItUXXJVsaS94yPFZuuCgFdfnmIuoX085h5/EC7Dpp29hUCkDdmRlKjvCHSiUGxCWt1DQRdVVS9+kQX69mt/2hzsdgyI1UXE3mUg1GE45cKYWh0YQPj+TIUzzObJjakrJq8ZNaR6bJAEuNTl5FnZwZsgRDnZMZqjRnhuzrhQDx75/1VFmheeouJsgHfSL95WB99YwB8nNizecXVNShQF8n1zbYB5y15mkyf537giEfL40cVF4s4lQZdU+lVSye7goYDLmZFA8xM9RxOaU1aDCK9/HdHyz9qAo7uJt5hTxN1rFfTlJW5WpZjdxHJMouM+Tsvl/OkjNDzXSulVeUldVatu0I0sFfp8UHyybgo+UT5Okn69eQX1GHU3mWZfgHzhfbTE3JS+u93Nutw3qqjKg7KpGLp7msXkkMhtxMyg0xFuq4S1ZvgFIAA0DOeLSXVEDdkZohAIgxF1GfyNXDJIjTpNIvPCkoKatpkAMYV6hqITME2K4ok+5TtLmOaUxSGMYk2W5Ka10zZB0MFVXW41xhlRwQ1bh5NZnEEgwxM0Tdk2VfMmaGlMRgyM2kxosMhjquuTdAaUqqPeobjXKWI8S3g9Nk5qzKT1fLAYgN1aSagCAfL7kpYq4Lp8qqW6gZAiwrysRgSMoM+TR7PXlVXEUdTubZdsy+9/XvMfqpNFwoqrLsWO/GaTKAwRB1f+wx1DUwGHIzTpO5zqVmpkaKOjBNVmFe2aFWNW1a2FZSvY0UREQF2qbB2ztVJggCrpbVOFxBVdnKNFlv62kyve3UnSPRweJj9Y0mfHtRrBOaNjASgLgqraymAW9+e1leWu/n9mkyfwAMhqj74lYcXQODITeTiqgZCnWc9AYoZTskHakZkpa5hvh5d3ibDKneRiLtIi+xBENtywx9eOQqrv/zXrx1MKvJY/I0WTOBnDRNdtWqZii6hcyQTquR769UnL32lsEY3ycMN5iDos+O5qHE3FTS7dNk5l5DV8tq5eX9RN2JNF0d0cKHEup8DIbcTHp7NZkYDnWUFAz9ZnIyAGDKAPHNuaymAfWN7XtjlBsu+nZ8ryv7YEjqOi2Jl1eUtS0zlJkjdok+frWiyWNV9WIwF9hMZqhPhJhJyauoRVaJ+HPtgzR7j84eaPN9v6gAvH//BPzrvrGID/NFZX0j9p69BsD902Th/t6IDNRBEIBT+U3vB1FXUtdgxP/7z3H89yfLFjLSSkj73w/kXgyG3Ezdzr2uyFZptUHO4swfG489q6di872j4a0R/0oXtbOIulzODLkiGLJkrPpE+OPBG/vZPN7ezFCuuau1tMWHtdZWk4UH6OTgQep9JBVQN2feyF5YOD4RAOQtMABxS4wFY+IBAIZGE1QqYGB0YJteS0epVCqM6C32RDqWw2CIurbPj+XhvR+y8cQnJ2A0CahrsDQ+7R/NYEhJiu9a72lYM+Qal8xZobhgH/h5a+WtGaKCdPIUUHyYX5uvWy6vJOv4/L2vtwa/HJ+A3LJavDh/JELtagISw8UszYU29sjJNWeSrjkoFK9sZZoMAFJiAm2e21pmCAD+dOsQzBwSjaFxwTbH7x6XgLTTRQj398aq6f0xvHeIMy/BpYb3DsGe00VyoTpRV5V2qhCAmL0+drUcPloNTAIQ5KNtsXaPOh+DITfjajLXuFQsfprqE2n7aSo6yAdXy2rbvbxe2oqjow0XJU/fNqzZx4bEBQEQp/uq6xvh30w2x5ogCMgzZ4aKq5p2XW4tMwSIwZDUNDHIRwsfr9anttRqFSb3j2xyPDxAh09XTmr1+Z1puDkz9JODaUOirqKuwWjTrHTfmSK55m1AdKBNU15yP06TuRkzQ64hLUeXNjyVRJuzHO0toi5zwVYczooK8kF0kA4mATiVr2/9CRCn8aRuz6XV9WgwmnDgfLFcPCwVULe0Ei4lJshmDN2dlI26VFxt02+KSEn2qz2/vVAs/9sFgK/PFuF8oZgV7u/m6WVqisGQm8lNFxUdRfcnbRxqXZcDWDZCbW+vofLqjm/S2hbDeoUAcFwMnXaqEPNf+c5m2Xiu1Y7zJgH414HL+OUbP+DZnacBWGeGmh9/SqzlF2+0E1NkXV2Yv7ccFJ/IZXaIlHc0pxwD1v0P/9x7QT62+6Q4RXaLeVPkE7l6uV1F/yjWCymNwZCbScu1uct2x+RXiJkf+xVb0jLx9maGymulzJB7en4MM2+IetzuTbyqvhGPf/wTDmWV4vkvz8rH88pti62lGoTdpwogCIJTNUP9ogLk5o9RrRRPdxdSdugY64aoC9jx41U0GAV8nHEVAHAspxwf/yj++d7rEuSi/8zscgDiNBkpi8GQm3E7jtYdvFCMOzZ/a7P9gz0pKIgLsc8MiZmO9q4mK3PhajJnSPUuR3PK8XHGVZwtqAQAvLb/ktyZdtepAlw210jZB0NSnUyhvh4Xr1U5VTOk02rkJfbOFE93B1L91Tnz/SNS0oELYsbnUnE1rpbVYNX2o2g0CbhlWCwm9A3HqukDbM4fwJVkimMw5GZSATXbDDXv128fxo/Z5Vj0r0MOHxcEofMyQy5cTeaMoebM0OXiavzuw2N4ZPtRVNQ04LX0SwDEvcEEAXjd/H1ehe3rMhhN8p/TzxfL23G01j17RHwIACDJvKKtu0s2vw6pd5KkrsGIf39/BVdKuJEruUd+Ra1Nd/w1O47jcnE1YoJ88MztQ6FSqTAtJQq/SO0tnxPJlWSK42oyN5MKqAVWDTWrrkF8gy920EcHAPS1jfJeWPY1Q+0toL5aVoM1O47Ly9zdlRmKDNTB10sjF1aeytfj+8slqDEYkRTuhw13DMfdr32PDzOu4pEZA2xqhuztOV2IRnOU3VJmCAAem52C8X3C8bPhsa57MQqS2hRYBz2NRhMefC8Te04XYmLfcLz3m/FKDY88yLcXSmy+l1aQ3T+lj830+zO3D0NkoA5DewVzJVkXwMyQm0l/6U2mVk70YNZ79Djq1J1nLp4O9fOCr932D9LqKH1dY5u2Z/j0aB7SzxfDJIiNA/tGui9t/ciM/kiJsdQM/O94PgBgWO8QjO8ThhHxITA0mrD1YJY8TZYY3rSHkvRLWKVqfVuMyEAd7kzt7dSy+u5Auh9lNQ3y/nLP/e8M9pwWa6p+uFwKfR1XmlHnO2ieIpOaqgKAVq3CvJFxNud5a9V4dHYKbh7WMz6QdHcMhtzMspqMmaHmWC/3dtSdubmVZIDUN8fchbrS+eyQlIVaPDEJX66a7NYg4f4pffHlqilITQwFAOw2F0WnxIi9R5ZN6QMA2Pr9FWSZa4dGWDU3HBgdaBNABnhrPe6Tpr9OK081XCmthiAIeP9wjviYtwZGk4ADVj1eiDqDySTI9UIPTrN0nL8xJQrhAZwK68oYDLkZmy62rs6qF8fJvKZLpaWmg/bF04CYeZPqhtqyvL7UXKzcK8RXsUBCKqKUpgAHmZfAzxoSg8RwP5TXNMgF3lLhNQD0ifTHzCEx8vctrSTryZLM2aGskhoUVxlQVd8ItQq401ybsfdMkZLDIw9wPLcCRZX18PPW4LZRveSaxjut6oOoa2Iw5GZsuti6SqvpjJMOVpRJmaG4EMfLwqW9ttpSNyQFQ2H+7imcdqR/lO3yWqk5okatwpo5gxAZqEOwrxduGRZr0zgxMdwfc61qf3rK1FdbyXVDxdVy7VBciC9mmQPFfeeucYNk6hRnCypxLKdcnpadOiASPl4a/P3uUXjqtqGYMTha4RFSazzzI6SCmBlqndQrB3DcmTm/XFpJ1jQzBFiWi7dlSw45GApQLhgaaFU3FOSjtVkpN3toDGYPtWR/zhValpAnR/jhuj7h8vfSMnxPY50ZijHfu6Rwf4xJCoO/twbXKutxKKsU4/uEw2gS5F5LRB0hCALuee17lFQbEGTOykrBz5ikMIxJClNyeOQkZoYUwsyQY4ZGE+obLdXlx3MrYGi0rTbPayUzJHehbk9myE1L6h2x3rU6JSaoxem6CKv6g8Rwf2jUKiS0Y2PansR6RdkV8xL7xHA/eGvVuNVcvPrc/85g5bs/YtiTu3DayS1QiFpS12CSe4Lp6xqhUatwY0qUwqOitmIw5GZq8x1nKOSY9RSZ9Gl+xbs/2gREea1khtq6vF4QBPmXmZLTZJEBOnlJv/WWGY6E+HohUKeFWgV55ds7S8ZhWK9g/PWukZ091C4pyarXUJZ5mkw69sj0AfDz1uBoTjm+OJ6PGoMR/7DaKoGovaoNjTbfj0kMdVsHe3IdBkNuZpkmYzjkiNRB2d9bg1cWpkKnVWPP6UJs3md545JWicU0s8mopfGic9Nk1QajHGyFKzhNplKpMMhcCzQ4NqjFc9VqFbYsGoNXF46RV1Elhvvj84eux7yRvTp9rF1RgnmarLiqXq41k5bcRwX5YPnUvgAAL434b/B/x/ORU1rj4EpEzquptyz4mD4oCr+bOVDB0VB7MRhyM27H0TLrvbUm94/EM7cPAwB8eOQqTCYBtQaj3JSxufoeuWbIyaX1ZeaskE6rhq/CxcdrbxmEB6f1w+2jWw9oJvQNZ2GmlWBfL3mqUKqbSo6wdNhePrUv1t0yCO/fPwFTBkTCJADP7zprs3qRqK2kD3CRgTq8ft9YjEtmjVB3xGDIzbgdR8ukxniBPuJ00c+GxyJQp0VueS0OZ5Wi1LxdhrdGDf9mGgvKS+udzAxJU2Th/t6K9+cZ2isYv581EDqtZ64I66ifj7YsYVapgHirOipvrRpLJ/dBamIolpt7N312LA/TX/ym3du3ENUYLNls6r4YDLmbtB0HU0MOVdXZ7q3l46XBnGHiKqr/ZObKWZxQf69mAxcpGKqqb5T36mpJabUYNCm5koxc4+epveT2FbFBPs22GZjYLwJ/u3sUogJ1uFpWi4/Mu4sTtVW1uS+Yfytb4FDXxmDIzZgZapk8TWb1i+X2UeKn/S+O58uf4FvaSDVAp5U/pTnTeLG0uqHVa1L30DvUD9f3iwBgWV3WnFtHxOF+c4YoM7us08dGPVONXOfIYKg7YzDkZtyOo2XSarIgH8tGqdclh8HXS4PKukacyBULY1tb9WW9e/0Hh3Mw/tmvcCK3aTdrwJIZCldwJRm5zrIpfeGlUeGmQa0vbx6VIG6BcjSnnNlaahepZshPx2my7ozBkJux6WLLpF8sgVZbSqjVKrmJ3pkCMRgKbSVwkYqoCyrq8OmxXBTo6+Q9g+xZltVz76Ce4Pr+ETi1fjaWTu7T6rlD4oLgpVGhuMrgcB88otZI2+cwM9S9MRhyM5VcM6TsOLoqR9NkABBlXj5+pkDsvNxac8T4ULFwNru0Btnm5dNSvZG90ipzATVrhnoML41zv9p8vDRyG4MfOVVG7SD1GfJjAXW3xmDIzVRyzRCjIUf0cgG1l81xKTMkNdNrLTOUZF5SfaGoSm7SWNpMMFRmXqHGmiHPZD1V1p00Gk14+P1MvJh2TumheDSpzxALqLs3BkNuZqkZIkekaTL7ndelGiAphgzzsw2W7EnN9r67VAKjuVq9uWCoK3SfJuWMSggBAGRmlys6jrbKuFKGT4/m4W9fnUdeOaf4lCI3imXNULfGYMjNpO04mBlyrFLuM+Q4GJK0mhkyryS6ZrWaTOpRZE8KkjhN5pmG9w4BAJzO18uBc3dw3GpBwOfH8hQciWerkafJmBnqzhgMuZlKbjSk7Di6KqlmKKhJMGRb3NxaFkfamsFac5kh6TinyTxTQpi4mWt9owm53aiI2np15GcMhhQj9xlizVC3pngwtHnzZiQnJ8PHxwepqalIT09v9twdO3ZgxowZiIyMRFBQECZMmIBdu3bZnNPQ0ID169ejb9++8PHxwYgRI/Dll1929stwmtocCzEz5FiVXEBtOw3WJDPUSuAS5OPVZKm8o2Co1mCUA7DIAK4m80QatQp9pBqza5UKj8Z51pmhk3l6XCiqUnA03dc/917A4jcP2WwS3RZynyHWDHVrigZD27dvx6pVq7B27VpkZmZi8uTJmDNnDrKzsx2ev3//fsyYMQM7d+5ERkYGpk2bhrlz5yIzM1M+Z926dXj11Vfx97//HadOncLy5ctx++2325yjKDZdbFFz02T2m7I6U99jnx2qrGtEg9Fkcyy3XFxpFqjTIsiXv8w8Vd+oAADoNgFFVX0jLpn3XxvROxgAsOd0oZJD6pZqDUb89avz2Hf2Gj480r4u5NUsoO4RFA2GXnzxRSxZsgRLly7FoEGDsGnTJsTHx+Pll192eP6mTZvw6KOPYuzYsejfvz+effZZ9O/fH59//rl8zjvvvIP/9//+H26++Wb06dMHDzzwAGbNmoUXXnjBXS+rRWpux9GiyrqmfYYAyDuzS5yZ0kpy0IHYfnl9jnlapFeor+L7kpFy+kV2r2DoZG4FBAGIDfbB9EHiZr3nCrtPVqur+OFyCQyN4gekd3+40q7fy1xa3zMoFgwZDAZkZGRg5syZNsdnzpyJgwcPOnUNk8mEyspKhIVZdgmur6+Hj49tFsHX1xcHDhxo9jr19fXQ6/U2X52F23E0z2QSUGVwvJrMx0uDUPMKMl8vDXyd+MWT6KhuyK6IWmq01zu06bnkOfp1s8yQNEU2tFcw+kdbxl5WbcDfvzrfbE8tsvXNuWvyny9eq8YPl0vbfI0a7k3WIygWDBUXF8NoNCI6OtrmeHR0NAoKCpy6xgsvvIDq6mrMnz9fPjZr1iy8+OKLOH/+PEwmE9LS0vDpp58iPz+/2ets2LABwcHB8ld8fHz7XpQTLLkHRkP2qg2N8tL5IJ+mS+eluiFnl8BbZ4Z6hfgCsDRYlFwtE6fJ4sN82zxe6jmsg6HukLWVgqFhvYJtxv5i2jm8kHYOr+y/CEDMhG47lM0C62bsNwdD0u+HP31+Su5y76xq7k3WI7QrGMrJycHVq5b51UOHDmHVqlXYsmVLm69lPzUhCIJT0xXbtm3Dk08+ie3btyMqyrIH0V//+lf0798fKSkp8Pb2xoMPPohf/epX0GiazySsWbMGFRUV8ldOTk6bX4ezmBlqXrE5UNGqVdBpm/7VlIKhUP+WewxJpDeJXiG+lmDIPjNUyswQAckR/lCpxKaf16pa39xXSYIg4LA5gzE6IRSJ4f7QqlWoMRjxxXHxQ9+pPD2+vVCM6zZ8hTU7juO32zJxKq/zMt7dUU5pDS5eq4ZGrcI/7hmFQJ0Wp/P1uPUf3+JCkfNTjtXsM9QjtCsYuueee7B3714AQEFBAWbMmIFDhw7h//2//4f169c7dY2IiAhoNJomWaCioqIm2SJ727dvx5IlS/DBBx9g+vTpNo9FRkbik08+QXV1Na5cuYIzZ84gICAAycnJzV5Pp9MhKCjI5qvTcDuOZv396/MAgJHxIQ4DYml5vbNL4IfEBeFPtw7Bxl+MkLNJ9tMHUmaodygzQ57Mx0sjb+HS1afKrpbVIq+iDlq1CqMTQ+ClUcsd16UVk+cKK/Hfn/LkehgA2H3KuYy7J3jvh2zM/YdYOjEyPgSjEkKxe/UUjIgPgaHRhI9/zHXqOiaTgJoGcZqMfYa6t3YFQydOnMC4ceMAAB988AGGDh2KgwcP4r333sNbb73l1DW8vb2RmpqKtLQ0m+NpaWmYOHFis8/btm0bFi9ejPfeew+33HJLs+f5+PigV69eaGxsxMcff4x58+Y5Na7OxqX1jh3JKsWOH3OhUgHrfjbY4TkxbZwmU6lUuG9iEib0DZebNJY0CYakzBCDIU8nZRLPFXTtQuTvLpUAAIb3DpbfgPubxy4p1NfLGxNPHyRmznef5GozQMysPfPFKZTXNCAhzA9/MP++iQ32xdLrxQ/N/zue79R0aV2jUf5gy8xQ99auYKihoQE6nfgpfc+ePbj11lsBACkpKS3W5thbvXo1Xn/9dfzrX//C6dOn8cgjjyA7OxvLly8HIE5fLVq0SD5/27ZtWLRoEV544QWMHz8eBQUFKCgoQEWFpd/GDz/8gB07duDSpUtIT0/H7NmzYTKZ8Oijj7bnpbqc1HSRoZCtd38Q2yn8IrU3RsaHODznZyPiMCYxFL9IbXtNV5h5as06M1RjaJSDI06TUWqiuEfZPqui2q7oh0viFNn4PuHysX52wRAA5JingFdNHwC1CjiVr0eOedNiT1ZW0yA3Stz9iJgNkkxLiYJOq0ZWSY28KXRLpGX1KpW4sIO6r3YFQ0OGDMErr7yC9PR0pKWlYfbs2QCAvLw8hIeHt/JsiwULFmDTpk1Yv349Ro4cif3792Pnzp1ITEwEAOTn59v0HHr11VfR2NiIlStXIjY2Vv56+OGH5XPq6uqwbt06DB48GLfffjt69eqFAwcOICQkpD0v1eWk7Ti6Q5GmO+lrxf5C0huSIwOiA/HRAxNxff+INl8/zF8M3q0zQ1K34SAfLYJ9natDop5r1hBxev7ghZJ2N+Bzhx8ui5mh65oJhtRWM8wRAd4YEheEsUniilv2IoK8j1tkoA4+dgFMgE6LKQMiAYjZodZYF0+zNUf31q5Jzj//+c+4/fbb8fzzz+O+++7DiBEjAACfffaZPH3mrBUrVmDFihUOH7Ofctu3b1+r15s6dSpOnTrVpjG4k5wZYixko95c26DTds6nKykzdPFaNf79/RUkhPnJu9UzK0QA0DcyAH0i/HGpuBp7z17DrSPilB5SE7nltbhaVguNWmXzwcE6GLoxJQp7ThcBEFebqVQqzBwSgx8ui1PRiycmefQbd645GIoLcTw1PmdoDNJOFeK/P+XjkRkDoFKpUFBRhwtFVUhNDLVp68EeQz1Hu4KhG264AcXFxdDr9QgNtfyDvP/+++HnxzeWlqhYM+SQVOjp7WAVmStImaHT+Xqs++SEzWOsFyIActDwyjcXsftkQZcMhs7kiyvC+kcFIMCqr03/qEAM7RWEAJ0WM4fEyMGQtAntbSPj8Ocvz+B4bgV+zC5vMQPbFew6WYB3f8hGgE6Dv901ClqN634vSJmhXiE+Dh+fOSQGvl4ncKm4Gt9dLMH7h3Pw35/yYBKAUD8v9IsKQG2DEX/++XD2GOpB2vU3rLa2FvX19XIgdOXKFWzatAlnz561WeZOTUmfyBgL2apvFH+pOFpS7wphVivQQv28kBBmCdoHRAd2ys+k7keaKtt39lqXnMrONtf82HdX99aq8d+HJuP9+ydgoNXf5+HmrTrCA3SYZw7u3jqY5Z7BttNHGVex7J0M7D93DTuPF+B0vmsL2qXp8bhgxx+CAnRazBkWAwBY/u8MfHbMEgiV1TTgcFYZTuTq8f6hHHmajJmh7q9d7zzz5s3D1q1bAQDl5eW47rrr8MILL+C2225rdisNEnE1mWP1nZwZ6h3qC2/zp8t3llyHb/7vBvz3oevxzO1D8ZspfTrlZ1L3MzhObKtRVd+I8pqW64bqGox457ssXDbvEeYOV0rEYMhRd3VJv6gAeGvV0KhVcmYIABZPSgIg1sIUVdZ15jA75OMM2z3CpP0D22PboWz8J9P2enkVli14miMt0tCbtwd6+d7ROLx2Ot5cPBZLzCvOMnPKuC9ZD9Kud54ff/wRkydPBgB89NFHiI6OxpUrV7B161b87W9/c+kAexpppp6hkC1DJ9cMhfp745OVk5D+6DQMNddRDO0VjHuvS2TxNMl0Wo3cuqGwhYBBEAT8ZusRPPHpSaz//KS7hidnhuw3Ibbmr9Pi9UVjsGVhqs2efkPigjE4NgiNJgGHmtl24odLJXLvLSVU1DbgcJY4NimrJbW/aKvyGgPW7DiOR7Yfs1lFmlsu/n9trmYIAK5LDpO70t8yPBZzhsVCq1FjWkoUfmUOKs/kV6K0WmzQ6c/MULfXrmCopqYGgYFiKnb37t244447oFarMX78eFy5csWlA+xp1PI0GcMha52dGQLET/3xYaxpo5ZFmQOIQr1tJ+o9pwrx/qFsCIKA7YdzkH5e7OOz96z7luJfKRGzUIlhTTchtjZlQCRuGtS0ee2YJLG0ITO7vMljl65VYcGW77H07SMdH2g77T93DY0mAX0j/TGpn7hqtL3BULHV1juHsizBn6VmqPlgSK1W4c93DMc91yXg6XlDbR7rFeKLyEAdGk2CvJeZHzND3V673nn69euHTz75BDk5Odi1a5e82WpRUVHndm/uAVgz5JhlNZli2+URAbBs+1JYYckMCYKApVuP4PEdx7H7VCGe3Xna5jk15lVFnclkEpBjDgxamiZryaiEEABAZnYZKmoacNaql06WOdA6U1CJYjdvSVJUWYfnd52R65mmD4qWFza0N1NVbrX1zvfmRpX1jUZcqxRfW0uZIQCY2C8Cz94+TG7YKlGpVHIvtG/NjS2ZGer+2vXO84c//AG///3vkZSUhHHjxmHChAkAxCzRqFGjXDrAnsaymkzZcXQ1UgF1Z2aGiJwhdTov1FuCoVKraZbHPv4J+rpGxIf5ylOs7qgbKtDXwdBoglatQmyw45VQrRkVL2aGTuTpsfBfP2D2X/cj40oZAKDEKpPyo/mYu7yRfhn/3HtRHstNg6LllhftzQxZ/z+TGlXmm6fIfL00CPVr//S4FFSWmevKAh1sLE3dS7veee68805kZ2fjyJEj2LVrl3z8pptuwksvveSywfVELKB2zMDMEHUR0h541jVDBVaBkVRYfc+4RLm/T2cGQ1dKqjHlL3vxwLs/AhALf9u71Dwx3A+hfl4wNJrw09UKCALwwWFxY2rr4OFHB9NoneniNcv9mzUkGqmJoVaZodp2lRVYF8CfLtCjoqZBniKLC/HpUK8l6y75ft4a3Jnau93Xoq6h3e88MTExGDVqFPLy8pCbK25qN27cOKSkpLhscD0Rt+NoShAEt9QMETkjSs4MWaaKCipsi6m9NCr8YkxvJJs3SL10rfOCoU17ziO7tAbHcsoBwKYtRFupVCqMSrDtMbTzeD7qGow23dndnRmStgl561dj8erCMdCoVXJNjzMr+xwps5omEwSxbqi1hovOGtE7BMG+XvD10uCtX43DoFiWh3R37XrnMZlMWL9+PYKDg5GYmIiEhASEhITgqaeegslkav0CHozbcTTVYLTci85aTUbkrGgH02TWmSEAuHVEL0QE6NAnUgqG2r7T/T/3XsBLaedaPOfStSp8etR2B/X21gtJpKyGl0aFiAAdKusb8dXpIptpsp9yy9FgdM/vckEQkGOuC7IO9Hy8NPJquPZMlZXZBVBfnS6UC57t+zS1lb9Oi50PT8be39+AcclhHboWdQ3tKoFfu3Yt3njjDTz33HOYNGkSBEHAt99+iyeffBJ1dXV45plnXD3OHoPbcTQl1QsBnCYj5TmqGZKKqe+9LgG3j+qFob3EZd99Ito3TVZR24Dnd52Vryllo+y9vO8iTALkbUIA2wai7XHzsFi8nn4JiyYkwSgIeHnfRXx6NNcm+KlrMOF0vt6mT1FnKak2oMZghErVtPdP71BfXKusx9WyGgwzL7V3lrScfnL/CKSfL8aOzFy5tcnto3t1eNwtrUaj7qdd7zxvv/02Xn/9dTzwwAMYPnw4RowYgRUrVuC1115rsp8Y2eJ2HE1J9UIA5MaIREqRaoauVdbDaF7pIGWGYoJ8MCYpTN7g05IZqkaRvg6NTmZTskssK6SySppfLfW9eVPWP80bIneWvr5/ZFteThP9ogJw7I8z8buZA3BjirhjwMk8vTxNJn0gkYqOO5s0RRYT5NMkM9yRImppmmzmkBiM6B0MQ6MJ9Y0mDO0VhFFWNT9EQDuDodLSUoe1QSkpKSgtdc8/oO6KS+ubkuuFNGqo1Z67gSR1DeEBOqhV4orPEvMS8wJz/VCM3Souacqqsr4R4579Cqu2H3XqZ1wptWSSpN5B9gRBQGGF+HOTI/zx0QMTsGPFRJdMy6hUKqhUKnlaqkBfhyLza5wxWOxPtP+8e/onSY0kHfUA68jyeqnOKNTPy6bL/MLxiR69US051q5gaMSIEfjHP/7R5Pg//vEPDB8+vMOD6sm4mqypzt6klagtNGqVXKsiFVEXmLdwsA+GdFqNTQ3P3jNFTtUDXrHKBl1pJjNUWm2AwWiCSgVEBfog0McLoxNcu8FqZIAO3lo1jCZBzn79fLS4MuqHS6Xt7p+UV16Lg+YePM2prm/E12cK5dfvqDBcmorqSGYozM8bs4fEIDUxFAOjA3HriI5PkVHP066aob/85S+45ZZbsGfPHkyYMAEqlQoHDx5ETk4Odu7c6eox9ij8PNIUGy5SVxMT5INCfT3ePHgZs4fEyKvJYhzU9qyZk4JdJwvxn8xcVBuMuFZVj6jAlvsAWU+TXSl1HAzlm39muL+u0z4oqNUq9A71tVkNNyZJXNZ+tawW310scdjJujWrPziK7y+V4sPlEzA2yXEm689fnsHW7yw7FsSHNg2G4sw7y9sXsDtDCoZC/Lyh1ajx8QMT23wN8hzt+hc2depUnDt3DrfffjvKy8tRWlqKO+64AydPnsSbb77p6jH2KNJ2HMwMWTAzRF1NeICYGdrxYy7ufydD3rAz2kGzw9lDY/HSgpHoY15mL3V1/t/xfPz85YNNluUDlqkhoPlpMul57W2w6KzeVkGIt1aNAJ0WNwwU65K+Oed4qiyruBp3b/keR7KalkUIgoCfrlYAgLxliT2TScDO4wU2xxLCmxYkO1rZ5wxBECzTZP5siEita/e7T1xcHJ555hl8/PHH2LFjB55++mmUlZXh7bffduX4eh5zakiKhWoMjThwvthty1i7Imk1GTND1FXEO9jR3M9bg8AW9qAaYC5wloKhv319ARlXyvDfn/KanGsdDGU1sxItXyra7uRgyPq1Rvh7Q6VS4YYBYmH1181M+32YkYPvLpXgb19faPLYtcp61BjEf9MZVxzXkP6UW9Fkyw9H02SxweLYiqsMNqtOW1NZ34hGc/F7aAdX35Fn4LuPm1kyQ+L3f//6An75xg/4JDO3hWf1bMwMUVezcEISfpHaGwvHJ8rHYoJb7lo8MEYMhs4UVKKipgFnCvQALKulJPWNRuRVWGpg9HWNNvtoSQoVyAyFBYiBw6R+EQjQaXG1rFbuzWMtv6IOapigykpH49EPgMvpgEkMVqzbDGRmlztcYffV6UIAtv/mHU2Thfp5yecU6Z3fL628WswK+Xpp5JV/RC3hu4+bSb9KBXMP6nxzR9S2poF7EkvNEH9pUdfQLyoAz/9iBH43c4B8TF/bcjFxSowlM3Qoq1TO/mbbBUPi9hLi5p5R5kJtR8vrpZqh6GZ6ELlKfJglMxTmL47H11uDuSNiAVi267B5TsEeHND9Fm+r10P7yW+At3+G+o2DgVOf2QRDNQYjTudXNnn+ntNFAIC1Nw9CfJgvUmIC5aJ1ayqVSq7TakvdUKk5uOzI/mPkWRgMuZnabmm9wfypyWD03BoibtJKXVWI1RSLt6bl5Q9SZuhcYSUOXrTUyuTYrYSSiqcTwv3lTsiO6oYK9OLzOjszZJ2RibDaoX3+mHgAwM4T+dDXWXVzPvUZVpU+hRjYZoy8agogfLAIXuf/a3P8iN1UWZG+Dqfz9VCpgLkj4pD2yFR88dvJzWbdpGnCfAe1V82xLp4mckabVpPdcccdLT5eXl7ekbF4BPvtOAyN4n89u2aIq8mo6/r8weux5j8/4f/NGdTieYnh/vDxUqOuwYQdP1qmvXNKa1BR04DvLpXghoGRcuCTEOaLQB8vHMoqdbi8Xnrz7+yaod6h1pkhS/AwMj4E/aMCcL6oCv87no8FYxMAkxHCl49BECxtQiRqiBnvaZdehBovITLIF4X6ehy5UoZfTUqWzzuZL04f9osMsPl5zZGCwcI2BEPStCOLp8lZbQqGgoNbboceHByMRYsWdWhAPZ9tzZCUGXK2c21PxE1aqSsb1jsY/31ocqvnadQqDIwOxLGrFaioFTMpKpX49/v3Hx1D2qlCjEkMxTVz4XDfyAD4mwuyLxTZ7m0mCILVarLO3fYhzN8bft4a1BiMcs2QOHYVbhoUjfNFVTiZJwYwuHIQKn0emiudUgEIM17DOPUZjBo9Fy/vu4iMrDIIgiBnfs4XitNmA8yZtNZI02TNZYbqGozY+l0WUmKCMGWAuAqurFpquMjMEDmnTcEQl813nNpuNVmDORBo8OBpMgMzQ9RD/N+sFKz5z0/IKa1FSkwgKusakVteKxcMHzHvBp8Q5of7JibhlDnIOGXOlkgq6xvlFVmOehu5kkol9ho6V1iFCH/buh3r7UYAAFWFTl0zCuW4bWQvvLb/Egr0dcgtr5ULtc8WiIHfgCgng6FgqWaoaePFsmoD7n/nCA5nlUGrVmHb/eMxNilMniZjMETOalfTRWo/+73JpMwQp8kAbxZQUzd3ff8IfP27G/DthWL0jQzA/310DLnltTAJYuaoV4gv/HVa/GvxGEQH+cgLKi5dq0KtwQhfb/HfgJQVCvb1ko91ppsGRSO3rBajE207XPeVgyFz5irAuQaMJepQ9I30x5BewTiWU44jWWVyMHS+SMwMDYwJcOpa0jSZo35Nf/r8JA5niQFmo0nAA//OwJQBkdh7RizQZgE1OYsfxd3MUkBtWyvkycEQM0PUk3hp1LhhYBTiw/xseucMjQvCvt/fgJ2/vV6e+ooK8kFEgA4mAThdYMkOSRmjzi6eljw2OwVH/zgT/aJsA5TkCPH7vIo6cWuOxImo8YmWp/ntmQDkCeEoDBkFrUaNMebgSiqiNpkEnJOmyaKdzQyJ98pRMHTIvOz/5XtHY0B0AIqrDNjxYy7KahoQF+yDm4fHOvUziPju42aWpfUiKRBoVGiarLzGgKf+ewoncisU+fkAV5NRz2W9UmtcchjUalWTVVND4oIAQK7LefWbi3jkg6Pmx1qu03QlL03Tf39h/t4IMWdXsoprALUGXyf9DoDld5iFCiqo8KzxPlw/IAYALMGQOXuTU1aDugYTvLVqJJpX0rVGmiYsrKyH0SoKq6htQJ45QJrYLwIfLJuA9fOG4PczB+CVX6Zi/6PTkBIT5NTPIOI0mZup7LbjsCytVyYz9M+9F/DGgct448BlZD13iyJjYGaIeirrndivSw53eM6QuCB8c+4aTuVVoMFowkt7zkEQgNtGxmHdLS2vYHOHPhH++DG7HJeKqzA4Lgjfek/E5w2rsDHgPQQaiiwnBsVBNfs5PNtnDgK8xbeW1CQxGDpbWAl9XQPOFYrTbf0iA6CxX47WjMhAHTRqFYwmASVV9YgyB0dShik22AfBvmLAtmhCkiteMnkgBkNuprIroFY6M5RnlXq2XvHhTlxNRj2VFAypVGh2w1Ip+3MiV48z+ZWoazAhyEeLF+ePhNrJgKEzJUcEiMGQuYi6oKIWe03jcNP0xZgfeVUsqg6IBhInAmoNrHMxUYE+SAjzQ3ZpDf746Um5/mmgkyvJALHWKjpQh7yKOpwtrJSDIWnbk7Zci6g5fPdxM/vtOJSuGZI2lwSAwja0uz+SVYqFb/xg0222vQzsQE091NBeQZjULxyLxiciuJli3qG9xPDhbEElfrhcAgAYmRDaJQIhwHpFmZjVkZa4RwX7A8mTgWF3iv9VO/73u+R6scfQfzJz8d4P2QCA/tHOFU9LbkgR90r7KOOqfIzBELkSgyE3s9+OQwoEGpqrSHTTeADgeBvqhjbtOY/088V4Lf1Sh8fAjVqpp9JpNXh36Xj8ad7QZs9JCPNDiJ8XDEYT3vw2CwAwOiHEPQN0gvSBSfrgI20d5Gz/o/smJuGj5RMwqV84BkQHYGLfcNw2slebxnD32AQAwP9OFMgNFaVgKIXBELkA333cTPq0Zz9NJvUbcrd6q4yUoyLqF3efxZin02wyQNX1jfIqjv3nrjnc1bpNY2DNEHkwlUolBwe55r0KRyeEtvQUt+oTKWZxLl2rRmVdA8pqxIaGbemMPSYpDO8uHY/dj0zFe78Zj7iQtjWSHNorCINjg2BoFLt7C4Igb4Tr7Ko0opbw3cfN5MyQvLRe/G+jSaFgqKH5YOhCURX+sfcCiqsM+OKnPPn4dxdL5ILvq2W1uNTBqTLuWk+e7t7rEuQ/q1TAyC6UGUqK8INGrUJlfSPSz4t7rkUH6eSiZXdQqVS4e5y4V9rW77KQW14LfV0jNGpVk3YARO3Bdx83U1nVDAmCoPhGrdar2E7k2QZDL+w+K9c2ZZg75wLAvnNFNuc988VpzPlrOnafLGjXGJgZIk/XPzoQY80rr/pHBSDIp+s0C9RpNXLzxU8yxT3XlMjG3DG6N0L8vJBVUoPVHxwDIDaFZK0huQLffdzMejsO6y04FJsms8oMFerrUVQp1gOcK6zE/05YgpuMK2UwmQQIgoB9Z68BEPumAMDXZ4pwOl+P376fKTeLawtmhoiAB27oC5UKmDO06zUKHBQrFnlL//aVCIb8dVr82rzhqzRNv2xKX7ePg3omvvu4mfV2HNZZGaWmyez7G53OF4sSD2eJv2wm9AmHr5cG+rpGXLxWhfyKOlwtq4WXRoXHZg+Unxfi54W6BhOW/zujzSvjLAXU/IRHnuvGlGhkPjEDD9/UX+mhNCEFQ9LviwFtXA3mKvdNTEKgeXPbW0fE4Y7RbSvEJmoOgyE3U1v18bHOBim1UWt9g9Hme2lH6TPmoGh4fDBGxIt9UI5cKUNptbiSI9xfh9EJobh/Sh/cP6UPvlo9FcG+XsgurWnTqjSATReJJCF+3l1mSb01KRiSKFW0HOzrhed/MRz3XpeAZ24fqkhfNOqZ2HTRzaR/uvaZIaX6DEn1Or1CfJFbXit3dZWWrQ6KCYJWrcL3l0qRcaUMieFiE7kAHy1UKhX+382WDrnjksOQdqoQhy+Xtmk1DJsuEnVtg2Jtg5/+Cq7gmj00FrO74FQidW9893EzlcqytN7QqHwwJI1hWC8x+3OusAqCIMibRg6MCcSYRLE26GhOOarqGgEAgT5N4+hx5g670hSbI4IgwGTXU4lNF4m6tsgAHcL9vQGIH5wCdPwcTT0LgyE3a7ZmSKlpMnO9jtQF93xhJXLLa1FZ1witWoW+kQGIDxN7glyrrEelHAw1Xe0y1lxQfcRcbO3I7z48hhHrdyOntMZqDMwMEXVlKpVKnipjx2fqifju42ZSzZAA22yQUhu1Sj93YEwQvDQqVBuM+PqMuHS+X1QAvLVqeZmvvq4B+jqx4ZqjzNCQuCD4emlQXtOAU/l6OdCSHLxYjB0/5qKyrhG7rJbhc2k9Udc3xrz0f1R8iLIDIeoEzHW6mXXTRetpMsUyQ+al9f46DZIj/HGusAqfHRUbLEpt7oPMzdUEwbIvUaCDNLmXRo3RiSH49kIJ5v7jACICdNjzyFQE+3nBZBKwYecZ+dwjWWWoqj+Htw9myR1tmRki6rqWT+2LAdGBuNG8TxhRT6L4u8/mzZuRnJwMHx8fpKamIj09vdlzd+zYgRkzZiAyMhJBQUGYMGECdu3a1eS8TZs2YeDAgfD19UV8fDweeeQR1NXVObii+0krRUwm28yQYjVDRktWRiqKPGJusDgwRkyL+3hp5KxNbpm4XYCjzBAATOwbAUAMnK5V1uOb89dwtawG9715CMdzK+Q+S4eySvF6+mU5EJLGQERdk4+XBjcPi4WPF2v7qOdR9N1n+/btWLVqFdauXYvMzExMnjwZc+bMQXZ2tsPz9+/fjxkzZmDnzp3IyMjAtGnTMHfuXGRmZsrnvPvuu3j88cfxxz/+EadPn8Ybb7yB7du3Y82aNe56WU4RIMjTQwDQaG5o6G5SZkin1WBAlG0tgLSkHrBkh66WS8GQ4w65S65Pxvp5QzB7SAwAIP3cNSx+8zDSzxfDW6vGhjuGwVurRmm1AVX1jTbPZWaIiIiUoOg02YsvvoglS5Zg6dKlAMSMzq5du/Dyyy9jw4YNTc7ftGmTzffPPvssPv30U3z++ecYNWoUAOC7777DpEmTcM899wAAkpKScPfdd+PQoUOd+2KcpLbajsO+t1CDUYC31r19M6x3jE9NFGsCfL00ePDGfpjQJ1w+L9jXC9cq65FbJhY+N5cZ8vHSYNGEJCSE+eHLkwX49GgeDEYTAnRafPbgJPSJDMDHP+bKHWStcTUZEREpQbGP4gaDARkZGZg5c6bN8ZkzZ+LgwYNOXcNkMqGyshJhYWHyseuvvx4ZGRly8HPp0iXs3LkTt9xyS7PXqa+vh16vt/nqLCqr7TgMdltwKNGF2norjEn9wvHJykk4+PiNWDmtn01DM2lTxuIqselia0trr0sOh7dGLU/DzRsZJ+9+Le3BZI/TZEREpATF3n2Ki4thNBoRHR1tczw6OhoFBc5t+PnCCy+guroa8+fPl4/dddddeOqpp3D99dfDy8sLffv2xbRp0/D44483e50NGzYgODhY/oqPj2/fi3KCZW8yoUmdUEOjAtNkVj1+VCoVRsaHINTcT8RakF0mqLlpMomvt0bONAHA3eMsu3Jf3y8SQNOW/t4aBkNEROR+ir/72LdTFwTBqRbr27Ztw5NPPont27cjKsqyumHfvn145plnsHnzZvz444/YsWMH/vvf/+Kpp55q9lpr1qxBRUWF/JWTk9P+F9QKFSxL6+0zQ+5eXm80CWg09wNqrV5HygxJ7IMjRyYPEIuph/YKwtBelvqjCX3D8cZ9Y/DGfWMRHaSTj3fFbQiIiKjnU6xmKCIiAhqNpkkWqKioqEm2yN727duxZMkSfPjhh5g+fbrNY0888QQWLlwo1yENGzYM1dXVuP/++7F27Vqo1U3f9HU6HXQ6XZPjnaG5posAkFdei+2Hs7FgbAIiAzt/PNbBWGtTVPbBUGuZIQC4b0ISKmobcMeo3k0eu2mQ+P84OcIfhfp6Z4ZLRETUKRTLDHl7eyM1NRVpaWk2x9PS0jBx4sRmn7dt2zYsXrwY7733nsM6oJqamiYBj0ajgSAos1rLnrqZ7TgA4I0Dl7Fx9zm8+8MVt4zF+ue3NTMU4ERmyF+nxZo5g1rsWJsc4d/qdYiIiDqToqvJVq9ejYULF2LMmDGYMGECtmzZguzsbCxfvhyAOH2Vm5uLrVu3AhADoUWLFuGvf/0rxo8fL2eVfH19ERwsTsPMnTsXL774IkaNGoXrrrsOFy5cwBNPPIFbb70VGo3yq5WsM0P2NUNFlWIvpIraBvundQppJZlaBWhbmaIKapIZcs1fnTGJYdh2qPOmJYmIiFqjaDC0YMEClJSUYP369cjPz8fQoUOxc+dOJCYmAgDy8/Nteg69+uqraGxsxMqVK7Fy5Ur5+H333Ye33noLALBu3TqoVCqsW7cOubm5iIyMxNy5c/HMM8+49bU1x3o7DvvMkL5W7LtT1+Ce2iHrPcFaq9PqrGDo9lG9cKZAj+G9Q1xyPSIiorZSfDuOFStWYMWKFQ4fkwIcyb59+1q9nlarxR//+Ef88Y9/dMHoXE/VwmqyynoxI2S/p1dnqW/DbvHW02TeWrXLegKp1SqsvWWwS65FRETUHoqvJvM0qhZqhqQd4esb3ZUZsjRcbE2QVcG0o33JiIiIuisGQ24mTUaJq8lsC7rlYMhN02TWDRdbY50ZctUUGRERUVfAYMjNWlpNZjT3/HH/NJkTwZCfdTDU+rJ6IiKi7oLBkJtZVpM1v1O9+zNDrdf/WDdZZGaIiIh6EgZDbmZZwS40yQxJumJmKECnhcY8eAZDRETUkzAYcjNpOw6T0Pz2G+4qoG5LzZBKpZKzQwE6TpMREVHPwWDIzVrajkPSFVeTAZYiamaGiIioJ2Ew5GYtLa2X1DV0vT5DgKXxojObtBIREXUXDIbcTN3CdhwSd0+TtT0zxGkyIiLqORgMuZnaatuLZguo3ZYZats0Wf8occPVvlHcXJWIiHoOzne4maONWr00KjRYNWDsigXUALDm5hTcPS4e/aICOnNYREREbsXMkJs5qhny87aNSRtNAhqbmUJzhskk4JHtR/HPvRdaPK8tS+sBwEujRv/owFY3dSUiIupOGAy5maPtOAIc7PXVkezQ+aIq/Cczt9VgqK2ZISIiop6I74JuZrsdh1iz4+fddDVXR4Kha5X1AIAag7HZuiTrn+GqHeiJiIi6IwZDbibNMAkC5DohP4eZIdsiakEQmpzTnOKqevnPFbUNzZ7X1gJqIiKinojvgm4mLa0XrLbj8HeQGaqz2p/s+NUKjH4qDe98f8Wpn2EbDBmaPa+e02REREQMhtzPsh2HtJrMvoAasM0M/XC5BGU1DdhzqtCpn1BcZQmAWs4Mta2AmoiIqCfiu6CbyZkhwSozpHNQM2SVGaqsawQAlFTXNznPEWenydqyaz0REVFPxWDIzaRl6dYbtTrODFmCoap6czBU1fyUlzXna4aYGSIiIuK7oJvZZIaMLdUMWabJquoswZAzhdQ2wVCNGAwdvFiM6S9+g+8vlciPSavZWDNERESejO+CbiatJmswCpDiGseryZpmhgxGE/TmwKgl1hmkcnNm6H/HC3ChqApfniho8jOYGSIiIk/Gd0E3k6bJrAukHWWGrB+vrLcEQNZZH0cEQbAJhqRpstJq8VilVTAl1SUxM0RERJ6M74JuJnWgts78OMwMWRVQV9VZ6n5aqxvS1zbK02+AJRiSiq8rra4lncemi0RE5MkYDLmZWs4MWQIWX6+WO1BXWWWGSlrJDF2ze1zfUmZIarroxb8GRETkufgu6GZyMNRgKV720jTd+NRRATXQ+jSZfbBUXmMXDNVbZYakpfUa/jUgIiLPxXdBN5MKqOutAhFHwYh1Zsi2ZqjlaTLpcennVNQ2wGQSmskMiT/Dh5khIiLyYHwXdDMpSJHqdbw0KmitgiEpMJKmsARBsJ0ma6XxopQ5ig/1AyAGQ+W1DTCZV65ZB0O1BmlvMtYMERGR52Iw5GYqWHatBwCN2naaLNjPC4Ala1NjMMK6tVBxZWuZITEY6hvpD0AMhkqtAih9bQMEQUB9o1H+GUG+Xh14RURERN0bgyE3U9uVB2nVKnhZZYZCzIGJVDNknRUCnMkMicFSv6gAAGJQlVdeJz/eaBJQ12CyyRAFOFjNRkRE5CkYDLmZ1GdIorEPhsyZoQtFVfjttkybjtFA6zVDUhYoIcwPGnPkdbm42uacyroGORgK0Gnl84iIiDwRUwJu1iQzpFHZTJOF+HkDANLPFwMATuXrbc5vbTWZFOQE+XohyEeLspqGJsGQvq4R1eaMU5AP/woQEZFnY2bIzewSQ00zQ3b1O1IgExvsA0AMdqy7U9uTgpwAnRbB5mtdaiEzFOjDeiEiIvJsDIbczH6azL5mKNguGDKal4HFhfjKGaSWulBXOgqGrlXZnlPXKHeiDmRmiIiIPByDITezr87RqNXQ2kyTOc7UBPpoEe6vAwBcq2x+qkxq0Oiv08qrxK6W1dqcU1nXCL05GOJKMiIi8nQMhtxMbZcZ8tKobJouSjVD9gJ0WsSGiFNl+RV1Ds8BLNNkgT7aZq9lO03GzBAREXk2BkNu5qhmyNnMUFywLwAgr7zW4TlGk4BqcyPFAJ0W/c3L6yXh/mJwpK9rgJ7BEBEREQAGQ25nnxnSqlXw12mhVonbYgQ1U9AcoNMizpwZai4YqjZYegf567T4xZjeNsvmE8PFrtSVdY3yBq7N/TwiIiJPwbSAmznKDAX5eGHjL0bAX6eFr7fjrTECdF4I8hX/d+VVOA6GpHohL40KOq0ascG+uDElCmmnCgEASRH++DG73FxAzdVkREREADNDbqeCfWZI/F9wx+jemDUkBjqt4/8lAT5axMrTZI5rhqyX1Uur1uaPiZcfTwoXt+gQp8m4moyIiAhgZsjt1Haxjn33Zx8vx5mhQJ0WvUJarhmSltX7W22vcVNKFJZN6YMQP285s2S9tJ6ryYiIyNMpnhnavHkzkpOT4ePjg9TUVKSnpzd77o4dOzBjxgxERkYiKCgIEyZMwK5du2zOueGGG6BSqZp83XLLLZ39UpzSNDNk+711ZqifVQF0gI+lZuhaVT0M5k1WrVVZbbEhUatVWHPzIDxwQ195SoyryYiIiCwUDYa2b9+OVatWYe3atcjMzMTkyZMxZ84cZGdnOzx///79mDFjBnbu3ImMjAxMmzYNc+fORWZmpnzOjh07kJ+fL3+dOHECGo0Gv/jFL9z1slpkvx2HfWZIp7VkhkYnhMh/DtBpEebvDZ1WDUEACvXiVNmJ3AqUVotNGK2X1TsiHbfpM8RgiIiIPJyiwdCLL76IJUuWYOnSpRg0aBA2bdqE+Ph4vPzyyw7P37RpEx599FGMHTsW/fv3x7PPPov+/fvj888/l88JCwtDTEyM/JWWlgY/P78uEwzZF1Bbd58GbDNDI+JD5D8H+Ih1QHHmqbLc8lrsPlmAn/39AH7/4TEAtt2nHQnysZ4mYwE1ERERoGAwZDAYkJGRgZkzZ9ocnzlzJg4ePOjUNUwmEyorKxEWFtbsOW+88Qbuuusu+Pv7N3tOfX099Hq9zVdncbRrvTWdl+V/yaDYIPnPgeYAx3p5/ZOfnQQAfH2mCIBt92lHpMBHbzVNxqX1RETk6RQLhoqLi2E0GhEdHW1zPDo6GgUFBU5d44UXXkB1dTXmz5/v8PFDhw7hxIkTWLp0aYvX2bBhA4KDg+Wv+Pj4Fs/vCPvtOOxrhny0GnkqbVBMEMb3CUN0kA69Q8UeQVLjxXOFVciz60Rd5eQ0WXlNg7znGWuGiIjI0yn+TmifKREEockxR7Zt24Ynn3wSn376KaKiohye88Ybb2Do0KEYN25ci9das2YNVq9eLX+v1+s7LSCyb7ponxlSq1XIWDcDJkGAr7cG7y0djwaTSa4lijVPk737/RX5Od7mqbXqVqbJ7KfENGoV/Jrpa0REROQpFAuGIiIioNFommSBioqKmmSL7G3fvh1LlizBhx9+iOnTpzs8p6amBu+//z7Wr1/f6lh0Oh10Op3zg+8A+zjPeisOSai/ZU8xtVoFndoSsPQyT5NJ9UEAYGg0wdBocri03pq/twbBvl6oqLX0GHIm8CQiIurJFJsm8/b2RmpqKtLS0myOp6WlYeLEic0+b9u2bVi8eDHee++9FpfLf/DBB6ivr8cvf/lLl43ZFVrLDLVm+qBoTO4fgcRwPwztZakpqqxrcLi03ppKpcLEvuHy95wiIyIiUniabPXq1Vi4cCHGjBmDCRMmYMuWLcjOzsby5csBiNNXubm52Lp1KwAxEFq0aBH++te/Yvz48XJWydfXF8HBwTbXfuONN3DbbbchPDwcXZnWvgtjK8IDdHhnyXXy90P+8CWqDUZU1jW2urQeAK7vH4H/nRDvW6COxdNERESKBkMLFixASUkJ1q9fj/z8fAwdOhQ7d+5EYmIiACA/P9+m59Crr76KxsZGrFy5EitXrpSP33fffXjrrbfk78+dO4cDBw5g9+7dbnstzlKrO5YZshfo4yUHQ61NkwHA9f0i5D87mqIjIiLyNIrPk6xYsQIrVqxw+Jh1gAMA+/btc+qaAwYMgCAIHRxZ52htNVlbBfhoAT1QWd/6NBkAJIZbWgyczu+8FgJERETdheLbcXga+5qhjmZnrLtKt7a0XhJs3o8smPuSERERMRhyt6bbcXTsf4FlvzFLzVBL02QAsH3ZeIxJDMXf7hrVoZ9NRETUEyg+TeZx7JfWd7hmSMoMNbS6HYckJSYIHz3Q/Io9IiIiT8LMkJt1dGm9PWmbjtJqg7yTPVeJEREROY/BkJu5uoBaygwVWG3N4a9jV2kiIiJnMRhyM5dnhsw1Q/nmYMjHSw2thv9biYiInMV3TTdrsh2HizJDeRW1AIAATpERERG1CYMhN7PfC0zTwSyOVCx9tVQMhkL9GAwRERG1BYMhN7PPDHm5aJrMYBSLpxPD/Tp0PSIiIk/DYMjNXF0zFGTXYDEhzL+ZM4mIiMgRBkNu1mQ1WYc7UNtOiyVFMDNERETUFgyG3KxpZqiDNUNNMkMMhoiIiNqCwZCbddZqMon1RqxERETUOgZDbmYfDHW8z5AlGNKoVegV4tuh6xEREXkaBkNu1mTX+g4GQzqtBt5a8X9jXIiP/GciIiJyDt853cw+9OloZgiwrChL4hQZERFRmzEYcjP7zJCXC7bOkBovsniaiIio7RgMuZmra4YAy/J6NlwkIiJqOwZDbma/HUdHa4YAoE+kOD02ondIh69FRETkabStn0KuplIBgiD+2RWZoQ13DMNvJvfB0F7BHb4WERGRp2FmSAHWdUPaDjZdBAA/by0DISIionZiMKQA61yQKzJDRERE1H4MhhRgkxnq4N5kRERE1DEMhpRgFf8wM0RERKQsBkMKsI5/vFxQM0RERETtx3diBaisUkMaTpMREREpisGQAqwzQ67oM0RERETtx2BIAdaNF1kzREREpCwGQwpQMTNERETUZTAYUoCamSEiIqIug8GQAmwzQ/xfQEREpCS+EyuAmSEiIqKug8GQAqzDH9YMERERKYvBkAKk1WRqFaBmMERERKQoBkMKkGbJWC9ERESkPL4bK0BKBrFeiIiISHkMhhQgbcfBeiEiIiLlMRhSgJwZ4r5kREREimMwpACpgJqZISIiIuUxGFKAijVDREREXYbiwdDmzZuRnJwMHx8fpKamIj09vdlzd+zYgRkzZiAyMhJBQUGYMGECdu3a1eS88vJyrFy5ErGxsfDx8cGgQYOwc+fOznwZbcLVZERERF2Hou/G27dvx6pVq7B27VpkZmZi8uTJmDNnDrKzsx2ev3//fsyYMQM7d+5ERkYGpk2bhrlz5yIzM1M+x2AwYMaMGcjKysJHH32Es2fP4rXXXkOvXr3c9bJaJXWg1rJmiIiISHEqQRAEpX74ddddh9GjR+Pll1+Wjw0aNAi33XYbNmzY4NQ1hgwZggULFuAPf/gDAOCVV17B888/jzNnzsDLy6td49Lr9QgODkZFRQWCgoLadY2WTNu4D5eLq9En0h9f/+4Gl1+fiIjIE7X3/VuxzJDBYEBGRgZmzpxpc3zmzJk4ePCgU9cwmUyorKxEWFiYfOyzzz7DhAkTsHLlSkRHR2Po0KF49tlnYTQam71OfX099Hq9zVdnkvJBLKAmIiJSnmLBUHFxMYxGI6Kjo22OR0dHo6CgwKlrvPDCC6iursb8+fPlY5cuXcJHH30Eo9GInTt3Yt26dXjhhRfwzDPPNHudDRs2IDg4WP6Kj49v34tykqWAmjVDRERESlP83Vilss2OCILQ5Jgj27Ztw5NPPont27cjKipKPm4ymRAVFYUtW7YgNTUVd911F9auXWszFWdvzZo1qKiokL9ycnLa/4KcwKX1REREXYdWqR8cEREBjUbTJAtUVFTUJFtkb/v27ViyZAk+/PBDTJ8+3eax2NhYeHl5QaPRyMcGDRqEgoICGAwGeHt7N7meTqeDTqfrwKtpG27HQURE1HUolhny9vZGamoq0tLSbI6npaVh4sSJzT5v27ZtWLx4Md577z3ccsstTR6fNGkSLly4AJPJJB87d+4cYmNjHQZCSuB2HERERF2HotNkq1evxuuvv45//etfOH36NB555BFkZ2dj+fLlAMTpq0WLFsnnb9u2DYsWLcILL7yA8ePHo6CgAAUFBaioqJDPeeCBB1BSUoKHH34Y586dwxdffIFnn30WK1eudPvraw6bLhIREXUdik2TAcCCBQtQUlKC9evXIz8/H0OHDsXOnTuRmJgIAMjPz7fpOfTqq6+isbERK1eutAlu7rvvPrz11lsAgPj4eOzevRuPPPIIhg8fjl69euHhhx/GY4895tbX1hKpZshLo3jJFhERkcdTtM9QV9XZfYZu+Vs6TubpMXVAJN7+9TiXX5+IiMgTdbs+Q57Msh0Hp8mIiIiUxmBIAdJ2HKwZIiIiUh6DIQXIHai5NxkREZHiGAwpQCVnhnj7iYiIlMZ3YwWwZoiIiKjrYDCkANYMERERdR0MhhQgxUBerBkiIiJSHIMhBUjbcTAzREREpDwGQwqw1Azx9hMRESmN78YK4N5kREREXQeDIQVIBdRcTUZERKQ8BkMKYGaIiIio62AwpABmhoiIiLoOBkMKYgdqIiIi5fHdWAFyZoh9hoiIiBTHYEgB3I6DiIio62AwpABux0FERNR1MBhSgBQCMTNERESkPAZDClBJmSENbz8REZHS+G6sgKggHQAgOlCn8EiIiIhIq/QAPNGaOSn42fBYXJccrvRQiIiIPB6DIQUE+nhhYt8IpYdBRERE4DQZEREReTgGQ0REROTRGAwRERGRR2MwRERERB6NwRARERF5NAZDRERE5NEYDBEREZFHYzBEREREHo3BEBEREXk0BkNERETk0RgMERERkUdjMEREREQejcEQEREReTTuWu+AIAgAAL1er/BIiIiIyFnS+7b0Pu4sBkMOVFZWAgDi4+MVHgkRERG1VWVlJYKDg50+XyW0NXzyACaTCXl5eQgMDMS4ceNw+PBhh+eNHTu2yWOtHdPr9YiPj0dOTg6CgoI65wU4Mc7Oer4z57Z0Tlsf4/3m/eb95v3m/XbN8zt6v1t63F33WxAEVFZWIi4uDmq185VAzAw5oFar0bt3bwCARqNp9n+Co8ecPRYUFOS2f0wtvQZXP9+Zc9t6T1t6jPeb95v3m/eb99s1z+/o/W7pcXfe77ZkhCQsoG7FypUr2/SYs8fcqaM/vy3Pd+bctt7Tlh7j/eb95v12L95v9+pO97ulx7v6/eY0mZvp9XoEBwejoqLCbZ8sPBnvt3vxfrsX77d78X67lzvvNzNDbqbT6fDHP/4ROp1O6aF4BN5v9+L9di/eb/fi/XYvd95vZoaIiIjIozEzRERERB6NwRARERF5NAZDRERE5NEYDBEREZFHYzBEREREHo3BUBd19uxZjBw5Uv7y9fXFJ598ovSwerTLly9j2rRpGDx4MIYNG4bq6mqlh9SjabVa+e/30qVLlR6OR6ipqUFiYiJ+//vfKz2UHq2yshJjx47FyJEjMWzYMLz22mtKD6lHy8nJwQ033IDBgwdj+PDh+PDDD9t8DS6t7waqqqqQlJSEK1euwN/fX+nh9FhTp07F008/jcmTJ6O0tBRBQUHQarljTWeJiIhAcXGx0sPwKGvXrsX58+eRkJCAjRs3Kj2cHstoNKK+vh5+fn6oqanB0KFDcfjwYYSHhys9tB4pPz8fhYWFGDlyJIqKijB69GicPXu2Te+XzAx1A5999hluuukmBkKd6OTJk/Dy8sLkyZMBAGFhYQyEqEc5f/48zpw5g5tvvlnpofR4Go0Gfn5+AIC6ujoYjUYw79B5YmNjMXLkSABAVFQUwsLCUFpa2qZrMBhqp/3792Pu3LmIi4uDSqVyOIW1efNmJCcnw8fHB6mpqUhPT2/Xz/rggw+wYMGCDo64e+vs+33+/HkEBATg1ltvxejRo/Hss8+6cPTdjzv+fuv1eqSmpuL666/HN99846KRd0/uuN+///3vsWHDBheNuHtzx/0uLy/HiBEj0Lt3bzz66KOIiIhw0ei7H3e+Xx45cgQmkwnx8fFteh4/+rZTdXU1RowYgV/96lf4+c9/3uTx7du3Y9WqVdi8eTMmTZqEV199FXPmzMGpU6eQkJAAAEhNTUV9fX2T5+7evRtxcXEAxDeMb7/9Fu+//37nvqAurrPvd0NDA9LT03H06FFERUVh9uzZGDt2LGbMmNHpr60rcsff76ysLMTFxeHEiRO45ZZbcPz4cY/d76mz7/fhw4cxYMAADBgwAAcPHuz019PVuePvd0hICI4dO4bCwkLccccduPPOOxEdHd3pr60rctf7ZUlJCRYtWoTXX3+97YMUqMMACP/5z39sjo0bN05Yvny5zbGUlBTh8ccfb9O1t27dKtx7770dHWKP0hn3++DBg8KsWbPk7//yl78If/nLXzo81p6gM/9+S2bPni0cPny4vUPsUTrjfj/++ONC7969hcTERCE8PFwICgoS/vSnP7lqyN2aO/5+L1++XPjggw/aO8QepbPud11dnTB58mRh69at7RoXp8k6gcFgQEZGBmbOnGlzfObMmW3+VMYpsta54n6PHTsWhYWFKCsrg8lkwv79+zFo0KDOGG6354r7XVZWJn/Ku3r1Kk6dOoU+ffq4fKw9gSvu94YNG5CTk4OsrCxs3LgRv/nNb/CHP/yhM4bb7bnifhcWFkKv1wMQs/v79+/HwIEDXT7WnsAV91sQBCxevBg33ngjFi5c2K5xcJqsExQXF8NoNDZJiUZHR6OgoMDp61RUVODQoUP4+OOPXT3EHsUV91ur1eLZZ5/FlClTIAgCZs6ciZ/97GedMdxuzxX3+/Tp01i2bBnUajVUKhX++te/IiwsrDOG2+256vcJOccV9/vq1atYsmQJBEGAIAh48MEHMXz48M4Ybrfnivv97bffYvv27Rg+fLhcj/TOO+9g2LBhTo+DwVAnUqlUNt8LgtDkWEuCg4NRWFjo6mH1WB2933PmzMGcOXNcPaweqyP3e+LEiTh+/HhnDKvH6ujfb8nixYtdNKKerSP3OzU1FUePHu2EUfVcHbnf119/PUwmU4d+PqfJOkFERAQ0Gk2TqLaoqMhjC+g6E++3e/F+uxfvt3vxfrtXV7nfDIY6gbe3N1JTU5GWlmZzPC0tDRMnTlRoVD0X77d78X67F++3e/F+u1dXud+cJmunqqoqXLhwQf7+8uXLOHr0KMLCwpCQkIDVq1dj4cKFGDNmDCZMmIAtW7YgOzsby5cvV3DU3Rfvt3vxfrsX77d78X67V7e43+1ag0bC3r17BQBNvu677z75nH/+859CYmKi4O3tLYwePVr45ptvlBtwN8f77V683+7F++1evN/u1R3uN/cmIyIiIo/GmiEiIiLyaAyGiIiIyKMxGCIiIiKPxmCIiIiIPBqDISIiIvJoDIaIiIjIozEYIiIiIo/GYIiIiIg8GoMhIupxkpKSsGnTJqWHQUTdBIMhImqXxYsX47bbblN6GA4dPnwY999/f6f/nKSkJKhUKqhUKvj6+iIlJQXPP/882trYn8EbkbK4USsRdRsNDQ3w8vJq9bzIyEg3jEa0fv16/OY3v0FdXR327NmDBx54AEFBQVi2bJnbxkBEHcPMEBF1ilOnTuHmm29GQEAAoqOjsXDhQhQXF8uPf/nll7j++usREhKC8PBw/OxnP8PFixflx7OysqBSqfDBBx/ghhtugI+PD/7973/LGamNGzciNjYW4eHhWLlyJRoaGuTn2mdaVCoVXn/9ddx+++3w8/ND//798dlnn9mM97PPPkP//v3h6+uLadOm4e2334ZKpUJ5eXmLrzMwMBAxMTFISkrC0qVLMXz4cOzevVt+/OLFi5g3bx6io6MREBCAsWPHYs+ePfLjN9xwA65cuYJHHnlEzjJJDh48iClTpsDX1xfx8fH47W9/i+rqaqf/HxCRcxgMEZHL5efnY+rUqRg5ciSOHDmCL7/8EoWFhZg/f758TnV1NVavXo3Dhw/jq6++glqtxu233w6TyWRzrcceewy//e1vcfr0acyaNQsAsHfvXly8eBF79+7F22+/jbfeegtvvfVWi2P605/+hPnz5+Onn37CzTffjHvvvRelpaUAxMDrzjvvxG233YajR49i2bJlWLt2bZtesyAI2LdvH06fPm2TvaqqqsLNN9+MPXv2IDMzE7NmzcLcuXORnZ0NANixYwd69+6N9evXIz8/H/n5+QCA48ePY9asWbjjjjvw008/Yfv27Thw4AAefPDBNo2LiJzQ8Y3vicgT3XfffcK8efMcPvbEE08IM2fOtDmWk5MjABDOnj3r8DlFRUUCAOH48eOCIAjC5cuXBQDCpk2bmvzcxMREobGxUT72i1/8QliwYIH8fWJiovDSSy/J3wMQ1q1bJ39fVVUlqFQq4X//+58gCILw2GOPCUOHDrX5OWvXrhUACGVlZY5vgPnneHt7C/7+/oKXl5cAQPDx8RG+/fbbZp8jCIIwePBg4e9//3uz4xUEQVi4cKFw//332xxLT08X1Gq1UFtb2+L1iahtmBkiIpfLyMjA3r17ERAQIH+lpKQAgDwVdvHiRdxzzz3o06cPgoKCkJycDAByxkQyZsyYJtcfMmQINBqN/H1sbCyKiopaHNPw4cPlP/v7+yMwMFB+ztmzZzF27Fib88eNG+fUa/2///s/HD16FN988w2mTZuGtWvXYuLEifLj1dXVePTRRzF48GCEhIQgICAAZ86cafI67WVkZOCtt96yuYezZs2CyWTC5cuXnRobETmHBdRE5HImkwlz587Fn//85yaPxcbGAgDmzp2L+Ph4vPbaa4iLi4PJZMLQoUNhMBhszvf3929yDfsiapVK1WR6rS3PEQTBplZHOuaMiIgI9OvXD/369cPHH3+Mfv36Yfz48Zg+fToAMVjatWsXNm7ciH79+sHX1xd33nlnk9dpz2QyYdmyZfjtb3/b5LGEhASnxkZEzmEwREQuN3r0aHz88cdISkqCVtv010xJSQlOnz6NV199FZMnTwYAHDhwwN3DlKWkpGDnzp02x44cOdLm64SGhuKhhx7C73//e2RmZkKlUiE9PR2LFy/G7bffDkCsIcrKyrJ5nre3N4xGo82x0aNH4+TJk+jXr1+bx0FEbcNpMiJqt4qKChw9etTmKzs7GytXrkRpaSnuvvtuHDp0CJcuXcLu3bvx61//GkajEaGhoQgPD8eWLVtw4cIFfP3111i9erVir2PZsmU4c+YMHnvsMZw7dw4ffPCBXJBtnzFqzcqVK3H27Fl8/PHHAIB+/fphx44dOHr0KI4dO4Z77rmnSRYrKSkJ+/fvR25urrzi7rHHHsN3332HlStX4ujRozh//jw+++wzPPTQQx1/wURkg8EQEbXbvn37MGrUKJuvP/zhD4iLi8O3334Lo9GIWbNmYejQoXj44YcRHBwMtVoNtVqN999/HxkZGRg6dCgeeeQRPP/884q9juTkZHz00UfYsWMHhg8fjpdfflleTabT6dp0rcjISCxcuBBPPvkkTCYTXnrpJYSGhmLixImYO3cuZs2ahdGjR9s8Z/369cjKykLfvn3lHknDhw/HN998g/Pnz2Py5MkYNWoUnnjiCXmakYhcRyU4OzFORORBnnnmGbzyyivIyclReihE1MlYM0REBGDz5s0YO3YswsPD8e233+L5559nTx8iD8FgiIgIwPnz5/H000+jtLQUCQkJ+N3vfoc1a9YoPSwicgNOkxEREZFHYwE1EREReTQGQ0REROTRGAwRERGRR2MwRERERB6NwRARERF5NAZDRERE5NEYDBEREZFHYzBEREREHo3BEBEREXm0/w+7GTcY/5d0MAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=300, end_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='5020' class='' max='118750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      4.23% [5020/118750 16:46&lt;6:20:09 0.3498]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:381\u001b[0m, in \u001b[0;36mLookahead.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_weights()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:111\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,pg,state,hyper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_params(with_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: state \u001b[38;5;241m=\u001b[39m _update(state, \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:255\u001b[0m, in \u001b[0;36mradam_step\u001b[0;34m(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, beta, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (sqr_avg\u001b[38;5;241m/\u001b[39mdebias2)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eps: denom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beta: denom \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(denom, beta)\n\u001b[1;32m    256\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(grad_avg, denom, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mv \u001b[38;5;241m/\u001b[39m debias1)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(grad_avg, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m/\u001b[39m debias1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.138223</td>\n",
       "      <td>0.140573</td>\n",
       "      <td>0.188750</td>\n",
       "      <td>0.332497</td>\n",
       "      <td>0.624629</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.131487</td>\n",
       "      <td>0.180172</td>\n",
       "      <td>0.307260</td>\n",
       "      <td>0.653119</td>\n",
       "      <td>23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137762</td>\n",
       "      <td>0.130025</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.298409</td>\n",
       "      <td>0.663113</td>\n",
       "      <td>23:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123283</td>\n",
       "      <td>0.128005</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>0.291168</td>\n",
       "      <td>0.671286</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.125265</td>\n",
       "      <td>0.125230</td>\n",
       "      <td>0.174869</td>\n",
       "      <td>0.283553</td>\n",
       "      <td>0.679884</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.125402</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.283285</td>\n",
       "      <td>0.680187</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.125074</td>\n",
       "      <td>0.122887</td>\n",
       "      <td>0.172365</td>\n",
       "      <td>0.277708</td>\n",
       "      <td>0.686483</td>\n",
       "      <td>23:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.120514</td>\n",
       "      <td>0.123744</td>\n",
       "      <td>0.172361</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>0.681733</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.123021</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>0.171541</td>\n",
       "      <td>0.281699</td>\n",
       "      <td>0.681976</td>\n",
       "      <td>23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.172979</td>\n",
       "      <td>0.278708</td>\n",
       "      <td>0.685353</td>\n",
       "      <td>23:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.117069</td>\n",
       "      <td>0.122796</td>\n",
       "      <td>0.170272</td>\n",
       "      <td>0.276974</td>\n",
       "      <td>0.687311</td>\n",
       "      <td>23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.118279</td>\n",
       "      <td>0.121287</td>\n",
       "      <td>0.169607</td>\n",
       "      <td>0.275288</td>\n",
       "      <td>0.689215</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.111919</td>\n",
       "      <td>0.118910</td>\n",
       "      <td>0.164591</td>\n",
       "      <td>0.266109</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>23:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.099111</td>\n",
       "      <td>0.115342</td>\n",
       "      <td>0.158696</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.709095</td>\n",
       "      <td>23:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.091975</td>\n",
       "      <td>0.115419</td>\n",
       "      <td>0.157185</td>\n",
       "      <td>0.256743</td>\n",
       "      <td>0.710151</td>\n",
       "      <td>23:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6246293783187866.\n",
      "Better model found at epoch 1 with r_squared value: 0.6531193256378174.\n",
      "Better model found at epoch 2 with r_squared value: 0.6631125807762146.\n",
      "Better model found at epoch 3 with r_squared value: 0.6712861657142639.\n",
      "Better model found at epoch 4 with r_squared value: 0.6798838376998901.\n",
      "Better model found at epoch 5 with r_squared value: 0.6801873445510864.\n",
      "Better model found at epoch 6 with r_squared value: 0.6864829063415527.\n",
      "Better model found at epoch 10 with r_squared value: 0.6873109340667725.\n",
      "Better model found at epoch 11 with r_squared value: 0.6892145872116089.\n",
      "Better model found at epoch 12 with r_squared value: 0.6995758414268494.\n",
      "Better model found at epoch 13 with r_squared value: 0.7090945839881897.\n",
      "Better model found at epoch 14 with r_squared value: 0.7101508975028992.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 1e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.113439</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.169136</td>\n",
       "      <td>0.279328</td>\n",
       "      <td>0.684654</td>\n",
       "      <td>1:06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118187</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.168678</td>\n",
       "      <td>0.276340</td>\n",
       "      <td>0.688027</td>\n",
       "      <td>1:06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111758</td>\n",
       "      <td>0.118573</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>1:06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112457</td>\n",
       "      <td>0.118272</td>\n",
       "      <td>0.166766</td>\n",
       "      <td>0.273213</td>\n",
       "      <td>0.691557</td>\n",
       "      <td>1:06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116703</td>\n",
       "      <td>0.118066</td>\n",
       "      <td>0.166885</td>\n",
       "      <td>0.271521</td>\n",
       "      <td>0.693467</td>\n",
       "      <td>1:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.113750</td>\n",
       "      <td>0.118373</td>\n",
       "      <td>0.166376</td>\n",
       "      <td>0.270759</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>1:06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109780</td>\n",
       "      <td>0.118197</td>\n",
       "      <td>0.166344</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>0.694864</td>\n",
       "      <td>1:06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108059</td>\n",
       "      <td>0.118051</td>\n",
       "      <td>0.165657</td>\n",
       "      <td>0.269603</td>\n",
       "      <td>0.695633</td>\n",
       "      <td>1:06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.117424</td>\n",
       "      <td>0.164565</td>\n",
       "      <td>0.267892</td>\n",
       "      <td>0.697564</td>\n",
       "      <td>1:06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.117578</td>\n",
       "      <td>0.164313</td>\n",
       "      <td>0.268119</td>\n",
       "      <td>0.697308</td>\n",
       "      <td>1:06:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6846542954444885.\n",
      "Better model found at epoch 1 with r_squared value: 0.6880269050598145.\n",
      "Better model found at epoch 2 with r_squared value: 0.6905336976051331.\n",
      "Better model found at epoch 3 with r_squared value: 0.6915570497512817.\n",
      "Better model found at epoch 4 with r_squared value: 0.6934674382209778.\n",
      "Better model found at epoch 5 with r_squared value: 0.6943279504776001.\n",
      "Better model found at epoch 6 with r_squared value: 0.6948638558387756.\n",
      "Better model found at epoch 7 with r_squared value: 0.6956329345703125.\n",
      "Better model found at epoch 8 with r_squared value: 0.6975640654563904.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del learn, \n",
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68757"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/model_temp.pth')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('model_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#4) [0.10548990964889526,0.1505843549966812,0.25680434703826904,0.7100826501846313]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      83.33% [5/6 19:54:01&lt;3:58:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.132663</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.170884</td>\n",
       "      <td>0.308618</td>\n",
       "      <td>0.651587</td>\n",
       "      <td>3:58:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116149</td>\n",
       "      <td>0.119035</td>\n",
       "      <td>0.170044</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>0.666922</td>\n",
       "      <td>3:58:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122057</td>\n",
       "      <td>0.117260</td>\n",
       "      <td>0.164114</td>\n",
       "      <td>0.290335</td>\n",
       "      <td>0.672227</td>\n",
       "      <td>3:58:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115437</td>\n",
       "      <td>0.114895</td>\n",
       "      <td>0.169516</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.681435</td>\n",
       "      <td>3:58:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116658</td>\n",
       "      <td>0.111350</td>\n",
       "      <td>0.157978</td>\n",
       "      <td>0.272206</td>\n",
       "      <td>0.692694</td>\n",
       "      <td>3:58:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='16219' class='' max='59375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.32% [16219/59375 1:04:11&lt;2:50:47 0.1096]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6515868902206421.\n",
      "Better model found at epoch 1 with r_squared value: 0.6669217944145203.\n",
      "Better model found at epoch 2 with r_squared value: 0.6722266674041748.\n",
      "Better model found at epoch 3 with r_squared value: 0.6814352869987488.\n",
      "Better model found at epoch 4 with r_squared value: 0.6926941275596619.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(12, 2e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.105264</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>0.160486</td>\n",
       "      <td>0.281360</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>52:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102502</td>\n",
       "      <td>0.111544</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.286838</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>52:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.112530</td>\n",
       "      <td>0.161047</td>\n",
       "      <td>0.277096</td>\n",
       "      <td>0.712049</td>\n",
       "      <td>52:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>0.112151</td>\n",
       "      <td>0.160246</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>52:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102632</td>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.159459</td>\n",
       "      <td>0.279898</td>\n",
       "      <td>0.716126</td>\n",
       "      <td>52:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.108079</td>\n",
       "      <td>0.152991</td>\n",
       "      <td>0.262921</td>\n",
       "      <td>0.725166</td>\n",
       "      <td>52:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7162206768989563.\n",
      "Better model found at epoch 5 with r_squared value: 0.7251663208007812.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145379</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.195003</td>\n",
       "      <td>0.342077</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>1:11:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125336</td>\n",
       "      <td>0.126382</td>\n",
       "      <td>0.184133</td>\n",
       "      <td>0.314350</td>\n",
       "      <td>0.679277</td>\n",
       "      <td>1:11:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126290</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.179871</td>\n",
       "      <td>0.308917</td>\n",
       "      <td>0.686294</td>\n",
       "      <td>1:11:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6503068208694458.\n",
      "Better model found at epoch 1 with r_squared value: 0.6792766451835632.\n",
      "Better model found at epoch 2 with r_squared value: 0.6862936615943909.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(3, 1e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.113239</td>\n",
       "      <td>0.166109</td>\n",
       "      <td>0.277009</td>\n",
       "      <td>0.715268</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118272</td>\n",
       "      <td>0.112384</td>\n",
       "      <td>0.165332</td>\n",
       "      <td>0.274715</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108533</td>\n",
       "      <td>0.111935</td>\n",
       "      <td>0.165136</td>\n",
       "      <td>0.272125</td>\n",
       "      <td>0.718529</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.110852</td>\n",
       "      <td>0.164110</td>\n",
       "      <td>0.270032</td>\n",
       "      <td>0.721469</td>\n",
       "      <td>1:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103124</td>\n",
       "      <td>0.110735</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.266873</td>\n",
       "      <td>0.722130</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.106697</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>0.159765</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>0.726570</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>0.105747</td>\n",
       "      <td>0.154466</td>\n",
       "      <td>0.253241</td>\n",
       "      <td>0.735189</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7152675986289978.\n",
      "Better model found at epoch 1 with r_squared value: 0.7176631093025208.\n",
      "Better model found at epoch 2 with r_squared value: 0.718529224395752.\n",
      "Better model found at epoch 3 with r_squared value: 0.7214685678482056.\n",
      "Better model found at epoch 4 with r_squared value: 0.7221300601959229.\n",
      "Better model found at epoch 5 with r_squared value: 0.7265703678131104.\n",
      "Better model found at epoch 6 with r_squared value: 0.7351890802383423.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(7, 4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/tr_v1.pth')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('tr_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      71.43% [10/14 3:19:23&lt;1:19:45]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.288008</td>\n",
       "      <td>0.174881</td>\n",
       "      <td>0.288008</td>\n",
       "      <td>0.690495</td>\n",
       "      <td>19:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.259229</td>\n",
       "      <td>0.278898</td>\n",
       "      <td>0.169802</td>\n",
       "      <td>0.278898</td>\n",
       "      <td>0.698412</td>\n",
       "      <td>19:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.242027</td>\n",
       "      <td>0.269742</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.269742</td>\n",
       "      <td>0.706795</td>\n",
       "      <td>19:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.267588</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.267588</td>\n",
       "      <td>0.708273</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.242983</td>\n",
       "      <td>0.267330</td>\n",
       "      <td>0.169089</td>\n",
       "      <td>0.267330</td>\n",
       "      <td>0.709385</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232772</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.169060</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.709853</td>\n",
       "      <td>19:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.351062</td>\n",
       "      <td>0.265223</td>\n",
       "      <td>0.170867</td>\n",
       "      <td>0.265223</td>\n",
       "      <td>0.708562</td>\n",
       "      <td>19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.227057</td>\n",
       "      <td>0.265318</td>\n",
       "      <td>0.170030</td>\n",
       "      <td>0.265318</td>\n",
       "      <td>0.709494</td>\n",
       "      <td>19:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.223291</td>\n",
       "      <td>0.269968</td>\n",
       "      <td>0.170160</td>\n",
       "      <td>0.269968</td>\n",
       "      <td>0.708803</td>\n",
       "      <td>19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.219373</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>0.709530</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='23204' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.16% [23204/29687 15:20&lt;04:17 0.2138]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6904946366345599.\n",
      "Better model found at epoch 1 with r_squared value: 0.6984116024016455.\n",
      "Better model found at epoch 2 with r_squared value: 0.7067945812816843.\n",
      "Better model found at epoch 3 with r_squared value: 0.7082729521368367.\n",
      "Better model found at epoch 4 with r_squared value: 0.7093854982283825.\n",
      "Better model found at epoch 5 with r_squared value: 0.7098527482682689.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(14, 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.312845</td>\n",
       "      <td>0.348528</td>\n",
       "      <td>0.186559</td>\n",
       "      <td>0.348528</td>\n",
       "      <td>0.646830</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.295979</td>\n",
       "      <td>0.386613</td>\n",
       "      <td>0.175216</td>\n",
       "      <td>0.386613</td>\n",
       "      <td>0.669071</td>\n",
       "      <td>47:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312540</td>\n",
       "      <td>0.316801</td>\n",
       "      <td>0.172706</td>\n",
       "      <td>0.316801</td>\n",
       "      <td>0.684723</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274675</td>\n",
       "      <td>0.312294</td>\n",
       "      <td>0.167928</td>\n",
       "      <td>0.312294</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.262781</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.165456</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.702070</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253998</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>0.164715</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>0.704825</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229323</td>\n",
       "      <td>0.278633</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.278633</td>\n",
       "      <td>0.702575</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.234593</td>\n",
       "      <td>0.288640</td>\n",
       "      <td>0.165601</td>\n",
       "      <td>0.288640</td>\n",
       "      <td>0.701547</td>\n",
       "      <td>47:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.281531</td>\n",
       "      <td>0.163712</td>\n",
       "      <td>0.281531</td>\n",
       "      <td>0.702765</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.217475</td>\n",
       "      <td>0.370952</td>\n",
       "      <td>0.163606</td>\n",
       "      <td>0.370952</td>\n",
       "      <td>0.695579</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6468300950037681.\n",
      "Better model found at epoch 1 with r_squared value: 0.6690712099070321.\n",
      "Better model found at epoch 2 with r_squared value: 0.6847234115019918.\n",
      "Better model found at epoch 3 with r_squared value: 0.694712214234656.\n",
      "Better model found at epoch 4 with r_squared value: 0.7020703559530641.\n",
      "Better model found at epoch 5 with r_squared value: 0.7048247286794248.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, 2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [2/6 1:40:58&lt;3:21:56]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.240573</td>\n",
       "      <td>0.271668</td>\n",
       "      <td>50:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>50:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='2744' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      9.24% [2744/29687 04:37&lt;45:28 0.2250]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbefore_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  f()\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/training.py:40\u001b[0m, in \u001b[0;36mGradientClip.before_step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.142789</td>\n",
       "      <td>0.137072</td>\n",
       "      <td>0.184968</td>\n",
       "      <td>0.323550</td>\n",
       "      <td>0.663831</td>\n",
       "      <td>1:31:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131440</td>\n",
       "      <td>0.131322</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.304489</td>\n",
       "      <td>0.680972</td>\n",
       "      <td>1:31:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131792</td>\n",
       "      <td>0.125048</td>\n",
       "      <td>0.173002</td>\n",
       "      <td>0.288943</td>\n",
       "      <td>0.696811</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115277</td>\n",
       "      <td>0.122783</td>\n",
       "      <td>0.169563</td>\n",
       "      <td>0.283435</td>\n",
       "      <td>0.702603</td>\n",
       "      <td>1:31:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.120829</td>\n",
       "      <td>0.167641</td>\n",
       "      <td>0.277083</td>\n",
       "      <td>0.707759</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110554</td>\n",
       "      <td>0.121103</td>\n",
       "      <td>0.167036</td>\n",
       "      <td>0.276273</td>\n",
       "      <td>0.707414</td>\n",
       "      <td>1:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.106923</td>\n",
       "      <td>0.121396</td>\n",
       "      <td>0.168461</td>\n",
       "      <td>0.275641</td>\n",
       "      <td>0.706874</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108978</td>\n",
       "      <td>0.121609</td>\n",
       "      <td>0.167722</td>\n",
       "      <td>0.276074</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>1:31:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.105256</td>\n",
       "      <td>0.121704</td>\n",
       "      <td>0.166815</td>\n",
       "      <td>0.274988</td>\n",
       "      <td>0.705985</td>\n",
       "      <td>1:31:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.100636</td>\n",
       "      <td>0.120486</td>\n",
       "      <td>0.164992</td>\n",
       "      <td>0.272843</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>1:31:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6638310887952457.\n",
      "Better model found at epoch 1 with r_squared value: 0.6809722879118543.\n",
      "Better model found at epoch 2 with r_squared value: 0.6968106099706167.\n",
      "Better model found at epoch 3 with r_squared value: 0.7026034652435666.\n",
      "Better model found at epoch 4 with r_squared value: 0.7077585955065709.\n",
      "Better model found at epoch 9 with r_squared value: 0.7093534321797625.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, 1e-3)\n",
    "#learn.fit_flat_cos(5, 4e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/model_okay.pth')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('model_okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145789</td>\n",
       "      <td>0.143075</td>\n",
       "      <td>0.187728</td>\n",
       "      <td>0.403799</td>\n",
       "      <td>0.620909</td>\n",
       "      <td>49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.136473</td>\n",
       "      <td>0.134041</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.355374</td>\n",
       "      <td>0.642884</td>\n",
       "      <td>49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130043</td>\n",
       "      <td>0.127936</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.659469</td>\n",
       "      <td>49:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.121174</td>\n",
       "      <td>0.122204</td>\n",
       "      <td>0.166350</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.673414</td>\n",
       "      <td>49:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.121296</td>\n",
       "      <td>0.166364</td>\n",
       "      <td>0.307960</td>\n",
       "      <td>0.674728</td>\n",
       "      <td>49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116663</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.165333</td>\n",
       "      <td>0.306303</td>\n",
       "      <td>0.674654</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.114686</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.166172</td>\n",
       "      <td>0.302324</td>\n",
       "      <td>0.674308</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.108460</td>\n",
       "      <td>0.122085</td>\n",
       "      <td>0.166011</td>\n",
       "      <td>0.303160</td>\n",
       "      <td>0.672353</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.095856</td>\n",
       "      <td>0.119235</td>\n",
       "      <td>0.161396</td>\n",
       "      <td>0.294277</td>\n",
       "      <td>0.679827</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088201</td>\n",
       "      <td>0.118416</td>\n",
       "      <td>0.158530</td>\n",
       "      <td>0.291398</td>\n",
       "      <td>0.682537</td>\n",
       "      <td>49:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3, 1e-3)\n",
    "learn.fit_flat_cos(7, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.107994</td>\n",
       "      <td>0.106488</td>\n",
       "      <td>0.174495</td>\n",
       "      <td>69.499443</td>\n",
       "      <td>0.993962</td>\n",
       "      <td>1:10:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>0.102991</td>\n",
       "      <td>0.169497</td>\n",
       "      <td>67.631531</td>\n",
       "      <td>0.994168</td>\n",
       "      <td>1:10:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.993962274233683.\n",
      "Better model found at epoch 1 with r_squared value: 0.9941675411417258.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      41.67% [5/12 5:55:20&lt;8:17:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109214</td>\n",
       "      <td>0.099076</td>\n",
       "      <td>0.163365</td>\n",
       "      <td>66.396835</td>\n",
       "      <td>0.994430</td>\n",
       "      <td>1:11:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092406</td>\n",
       "      <td>0.098987</td>\n",
       "      <td>0.163418</td>\n",
       "      <td>64.059464</td>\n",
       "      <td>0.994410</td>\n",
       "      <td>1:11:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092613</td>\n",
       "      <td>0.097975</td>\n",
       "      <td>0.162091</td>\n",
       "      <td>62.233765</td>\n",
       "      <td>0.994467</td>\n",
       "      <td>1:11:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091485</td>\n",
       "      <td>0.098095</td>\n",
       "      <td>0.161914</td>\n",
       "      <td>61.303112</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>1:11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.162724</td>\n",
       "      <td>58.517742</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>1:11:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='433' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.46% [433/29687 01:02&lt;1:10:20 0.0878]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.9944301319218759.\n",
      "Better model found at epoch 2 with r_squared value: 0.994466553097564.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:211\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBackwardException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelStepException)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-3)\n",
    "learn.fit_flat_cos(12, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1562 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_example_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_decomp_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_cut_rematerialization_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2822\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2820\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(args)\n\u001b[0;32m-> 2822\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2515\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2515\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1715\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[0;32m-> 1715\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;66;03m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m#   }\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;66;03m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1328\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context(), track_graph_compiling(aot_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1328\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args_with_views_handled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m create_runtime_wrapper(\n\u001b[1;32m   1331\u001b[0m     compiled_fw,\n\u001b[1;32m   1332\u001b[0m     runtime_metadata\u001b[38;5;241m=\u001b[39mmetadata_,\n\u001b[1;32m   1333\u001b[0m     trace_joint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m     keep_input_mutations\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mkeep_inference_input_mutations\n\u001b[1;32m   1335\u001b[0m )\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:430\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler\u001b[0;34m(model, example_inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m model \u001b[38;5;241m=\u001b[39m convert_outplace_to_inplace(model)\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:595\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/debug.py:239\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:177\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m         graph\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39mexample_inputs)\n\u001b[0;32m--> 177\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cudagraphs:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/graph.py:586\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/graph.py:575\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mprint\u001b[39m(code)\n\u001b[0;32m--> 575\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mPyCodeCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/codecache.py:528\u001b[0m, in \u001b[0;36mPyCodeCache.load\u001b[0;34m(cls, source_code)\u001b[0m\n\u001b[1;32m    527\u001b[0m mod\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m--> 528\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# another thread might set this first\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torchinductor_leroy/63/c63uger4g4aqjhr72kg6zcvh3k4io5bf6y7x5hg7i2s3qiqjfntv.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cuda_getCurrentRawStream \u001b[38;5;28;01mas\u001b[39;00m get_cuda_stream\n\u001b[0;32m---> 20\u001b[0m triton__0 \u001b[38;5;241m=\u001b[39m \u001b[43masync_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43mimport triton\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43mimport triton.language as tl\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.ir import ReductionHint\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.ir import TileHint\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.triton_ops.autotune import pointwise\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.utils import instance_descriptor\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m@pointwise(size_hints=[2048], filename=__file__, meta=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m0: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*fp32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 1: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*fp32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 2: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m}, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: 0, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstants\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmutated_arg_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: [], \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfigs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m@triton.jit\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43mdef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m    xnumel = 2048\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43m    xoffset = tl.program_id(0) * XBLOCK\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43m    xindex = xoffset + tl.arange(0, XBLOCK)[:]\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43m    xmask = xindex < xnumel\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m    x0 = xindex \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m 16\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43m    x1 = (xindex // 16)\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m    x2 = xindex\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;43m    tmp0 = tl.load(in_ptr0 + (360 + x0 + (376*x1)), xmask)\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m    tl.store(out_ptr0 + (x2 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m triton__1 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mtriton(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124mimport triton\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124mimport triton.language as tl\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m    tl.store(out_ptr0 + (x2 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/codecache.py:683\u001b[0m, in \u001b[0;36mAsyncCompile.triton\u001b[0;34m(self, source_code)\u001b[0m\n\u001b[1;32m    682\u001b[0m cc \u001b[38;5;241m=\u001b[39m major \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m minor\n\u001b[0;32m--> 683\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_worker_compile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TritonFuture(source_code, future)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py:720\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit_flat_cos(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7e-4\u001b[39m, pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:248\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_train()\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:216\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orig_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mgraph_module\u001b[38;5;241m.\u001b[39m_forward_from_src \u001b[38;5;241m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[38;5;241m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[38;5;241m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m output \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo start tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m gm\u001b[38;5;241m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-3)\n",
    "learn.fit_flat_cos(3, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.000895364792086184)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.122136</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.181493</td>\n",
       "      <td>0.313605</td>\n",
       "      <td>0.673079</td>\n",
       "      <td>34:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121282</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.176421</td>\n",
       "      <td>0.294398</td>\n",
       "      <td>0.690070</td>\n",
       "      <td>34:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6730790734291077.\n",
      "Better model found at epoch 1 with r_squared value: 0.6900703310966492.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/15 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.109887</td>\n",
       "      <td>0.160886</td>\n",
       "      <td>0.268899</td>\n",
       "      <td>0.718790</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107647</td>\n",
       "      <td>0.109236</td>\n",
       "      <td>0.159493</td>\n",
       "      <td>0.265550</td>\n",
       "      <td>0.721290</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104061</td>\n",
       "      <td>0.108931</td>\n",
       "      <td>0.159251</td>\n",
       "      <td>0.263329</td>\n",
       "      <td>0.722589</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>0.109865</td>\n",
       "      <td>0.159870</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>34:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098991</td>\n",
       "      <td>0.110590</td>\n",
       "      <td>0.160912</td>\n",
       "      <td>0.265195</td>\n",
       "      <td>0.718406</td>\n",
       "      <td>34:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098965</td>\n",
       "      <td>0.111460</td>\n",
       "      <td>0.161282</td>\n",
       "      <td>0.267053</td>\n",
       "      <td>0.716199</td>\n",
       "      <td>34:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.093325</td>\n",
       "      <td>0.111975</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>0.268610</td>\n",
       "      <td>0.714019</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.091725</td>\n",
       "      <td>0.112911</td>\n",
       "      <td>0.161246</td>\n",
       "      <td>0.269824</td>\n",
       "      <td>0.712248</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090569</td>\n",
       "      <td>0.113749</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>0.271304</td>\n",
       "      <td>0.709899</td>\n",
       "      <td>34:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.087785</td>\n",
       "      <td>0.114089</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>0.708751</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.161327</td>\n",
       "      <td>0.273465</td>\n",
       "      <td>0.707173</td>\n",
       "      <td>34:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.078328</td>\n",
       "      <td>0.115303</td>\n",
       "      <td>0.160933</td>\n",
       "      <td>0.274253</td>\n",
       "      <td>0.705873</td>\n",
       "      <td>34:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='1562' class='' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1562/1562 00:26&lt;00:00 0.0751]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7187901735305786.\n",
      "Better model found at epoch 1 with r_squared value: 0.7212895750999451.\n",
      "Better model found at epoch 2 with r_squared value: 0.7225892543792725.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:381\u001b[0m, in \u001b[0;36mLookahead.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_weights()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:111\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,pg,state,hyper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_params(with_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: state \u001b[38;5;241m=\u001b[39m _update(state, \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:255\u001b[0m, in \u001b[0;36mradam_step\u001b[0;34m(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, beta, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (sqr_avg\u001b[38;5;241m/\u001b[39mdebias2)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eps: denom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beta: denom \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(denom, beta)\n\u001b[1;32m    256\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(grad_avg, denom, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mv \u001b[38;5;241m/\u001b[39m debias1)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(grad_avg, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m/\u001b[39m debias1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 3e-3)\n",
    "learn.fit_flat_cos(5, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn.fit(2, 3e-3)\n",
    "learn.fit_flat_cos(15, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:59: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x789db9a75300>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 32:01&lt;2:08:06]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.161324</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>0.715918</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='8182' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.56% [8182/29687 08:44&lt;22:57 0.0807]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7159183621406555.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(5, 1e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#4) [0.11172444373369217,0.16132423281669617,0.26352497935295105,0.7159183621406555]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.129519</td>\n",
       "      <td>0.130109</td>\n",
       "      <td>0.191536</td>\n",
       "      <td>0.325054</td>\n",
       "      <td>0.660586</td>\n",
       "      <td>32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.124522</td>\n",
       "      <td>0.122357</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.302771</td>\n",
       "      <td>0.683851</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116473</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.175865</td>\n",
       "      <td>0.292754</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111453</td>\n",
       "      <td>0.117234</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.289118</td>\n",
       "      <td>0.696406</td>\n",
       "      <td>31:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.112941</td>\n",
       "      <td>0.114254</td>\n",
       "      <td>0.171726</td>\n",
       "      <td>0.278642</td>\n",
       "      <td>0.706919</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108034</td>\n",
       "      <td>0.113473</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>0.274081</td>\n",
       "      <td>0.710151</td>\n",
       "      <td>32:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>0.170169</td>\n",
       "      <td>0.272091</td>\n",
       "      <td>0.710448</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092441</td>\n",
       "      <td>0.111157</td>\n",
       "      <td>0.163690</td>\n",
       "      <td>0.263591</td>\n",
       "      <td>0.717383</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.087232</td>\n",
       "      <td>0.110955</td>\n",
       "      <td>0.160811</td>\n",
       "      <td>0.261027</td>\n",
       "      <td>0.718037</td>\n",
       "      <td>32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.112318</td>\n",
       "      <td>0.160453</td>\n",
       "      <td>0.264056</td>\n",
       "      <td>0.714334</td>\n",
       "      <td>32:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6605860590934753.\n",
      "Better model found at epoch 1 with r_squared value: 0.6838511228561401.\n",
      "Better model found at epoch 2 with r_squared value: 0.6953175067901611.\n",
      "Better model found at epoch 3 with r_squared value: 0.6964060664176941.\n",
      "Better model found at epoch 4 with r_squared value: 0.7069191932678223.\n",
      "Better model found at epoch 5 with r_squared value: 0.7101508975028992.\n",
      "Better model found at epoch 6 with r_squared value: 0.7104479074478149.\n",
      "Better model found at epoch 7 with r_squared value: 0.7173829078674316.\n",
      "Better model found at epoch 8 with r_squared value: 0.7180368900299072.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-14:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-32:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-12:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-11:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-10:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-3, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [3/15 1:31:24&lt;6:05:36]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.131708</td>\n",
       "      <td>0.135052</td>\n",
       "      <td>0.186262</td>\n",
       "      <td>0.365141</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>30:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.126473</td>\n",
       "      <td>0.182601</td>\n",
       "      <td>0.318880</td>\n",
       "      <td>0.669278</td>\n",
       "      <td>30:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>0.120116</td>\n",
       "      <td>0.172366</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.686703</td>\n",
       "      <td>30:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='25362' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.43% [25362/29687 34:39&lt;05:54 0.1160]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6361209154129028.\n",
      "Better model found at epoch 1 with r_squared value: 0.6692783236503601.\n",
      "Better model found at epoch 2 with r_squared value: 0.6867033243179321.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 8e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/mwt_v1.pth')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('mwt_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "165/(198*1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.229272</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.152555</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.727625</td>\n",
       "      <td>27:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210327</td>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.153173</td>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.729453</td>\n",
       "      <td>27:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210848</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.153384</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>27:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369781</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.153907</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.728009</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190333</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.154275</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>27:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.194347</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.723863</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.188494</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.155276</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.719586</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.184154</td>\n",
       "      <td>0.256038</td>\n",
       "      <td>0.155658</td>\n",
       "      <td>0.256038</td>\n",
       "      <td>0.719379</td>\n",
       "      <td>27:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.178666</td>\n",
       "      <td>0.258872</td>\n",
       "      <td>0.156046</td>\n",
       "      <td>0.258872</td>\n",
       "      <td>0.715114</td>\n",
       "      <td>27:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.168267</td>\n",
       "      <td>0.257935</td>\n",
       "      <td>0.155296</td>\n",
       "      <td>0.257935</td>\n",
       "      <td>0.716201</td>\n",
       "      <td>27:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.154889</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.713802</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.148415</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.154951</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.710612</td>\n",
       "      <td>27:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.145642</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.154936</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.708474</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.137470</td>\n",
       "      <td>0.265236</td>\n",
       "      <td>0.155330</td>\n",
       "      <td>0.265236</td>\n",
       "      <td>0.704923</td>\n",
       "      <td>27:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.266994</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>0.266994</td>\n",
       "      <td>0.702860</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.2600410580635071.\n",
      "Better model found at epoch 1 with valid_loss value: 0.25627413392066956.\n",
      "Better model found at epoch 2 with valid_loss value: 0.2527551054954529.\n",
      "Better model found at epoch 3 with valid_loss value: 0.2521514594554901.\n",
      "Better model found at epoch 4 with valid_loss value: 0.2521290183067322.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 4e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.221799</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.161289</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.624436</td>\n",
       "      <td>19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.216447</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.160901</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.621538</td>\n",
       "      <td>19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211245</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.160596</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.617848</td>\n",
       "      <td>19:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221787</td>\n",
       "      <td>0.248711</td>\n",
       "      <td>0.160692</td>\n",
       "      <td>0.248711</td>\n",
       "      <td>0.620983</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.214866</td>\n",
       "      <td>0.249098</td>\n",
       "      <td>0.160391</td>\n",
       "      <td>0.249098</td>\n",
       "      <td>0.619240</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212531</td>\n",
       "      <td>0.249636</td>\n",
       "      <td>0.160608</td>\n",
       "      <td>0.249636</td>\n",
       "      <td>0.621107</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.210268</td>\n",
       "      <td>0.251274</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.251274</td>\n",
       "      <td>0.621262</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.205438</td>\n",
       "      <td>0.249965</td>\n",
       "      <td>0.160234</td>\n",
       "      <td>0.249965</td>\n",
       "      <td>0.612630</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211535</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.160285</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.616812</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.202592</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>0.160388</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>0.620815</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.203163</td>\n",
       "      <td>0.251837</td>\n",
       "      <td>0.160533</td>\n",
       "      <td>0.251837</td>\n",
       "      <td>0.622562</td>\n",
       "      <td>19:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.202210</td>\n",
       "      <td>0.251259</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.251259</td>\n",
       "      <td>0.623116</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.203359</td>\n",
       "      <td>0.254081</td>\n",
       "      <td>0.160537</td>\n",
       "      <td>0.254081</td>\n",
       "      <td>0.626861</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.202191</td>\n",
       "      <td>0.252784</td>\n",
       "      <td>0.160110</td>\n",
       "      <td>0.252784</td>\n",
       "      <td>0.620404</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.253724</td>\n",
       "      <td>0.160042</td>\n",
       "      <td>0.253724</td>\n",
       "      <td>0.620057</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.195313</td>\n",
       "      <td>0.253773</td>\n",
       "      <td>0.160025</td>\n",
       "      <td>0.253773</td>\n",
       "      <td>0.622633</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.201135</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.159798</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.620017</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.200917</td>\n",
       "      <td>0.253857</td>\n",
       "      <td>0.159699</td>\n",
       "      <td>0.253857</td>\n",
       "      <td>0.619852</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.200810</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.159684</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.620227</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.200294</td>\n",
       "      <td>0.254061</td>\n",
       "      <td>0.159705</td>\n",
       "      <td>0.254061</td>\n",
       "      <td>0.620749</td>\n",
       "      <td>19:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.24888284504413605.\n",
      "Better model found at epoch 3 with valid_loss value: 0.24871104955673218.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(20, 8e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/good_v3.pth')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('good_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.232228</td>\n",
       "      <td>0.257023</td>\n",
       "      <td>0.163198</td>\n",
       "      <td>0.257023</td>\n",
       "      <td>0.605320</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>0.162589</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>0.616660</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219839</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.161875</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.611379</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.225076</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.161634</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0.161465</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0.613677</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.248704</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.248704</td>\n",
       "      <td>0.610308</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.300959</td>\n",
       "      <td>0.248664</td>\n",
       "      <td>0.160695</td>\n",
       "      <td>0.248664</td>\n",
       "      <td>0.615366</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.219857</td>\n",
       "      <td>0.248102</td>\n",
       "      <td>0.160497</td>\n",
       "      <td>0.248102</td>\n",
       "      <td>0.617658</td>\n",
       "      <td>19:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.224552</td>\n",
       "      <td>0.248065</td>\n",
       "      <td>0.160396</td>\n",
       "      <td>0.248065</td>\n",
       "      <td>0.619523</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.213481</td>\n",
       "      <td>0.248097</td>\n",
       "      <td>0.160325</td>\n",
       "      <td>0.248097</td>\n",
       "      <td>0.618928</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.2570227384567261.\n",
      "Better model found at epoch 1 with valid_loss value: 0.2529500126838684.\n",
      "Better model found at epoch 2 with valid_loss value: 0.2496650665998459.\n",
      "Better model found at epoch 3 with valid_loss value: 0.24904529750347137.\n",
      "Better model found at epoch 5 with valid_loss value: 0.24870361387729645.\n",
      "Better model found at epoch 6 with valid_loss value: 0.2486642599105835.\n",
      "Better model found at epoch 7 with valid_loss value: 0.248102068901062.\n",
      "Better model found at epoch 8 with valid_loss value: 0.24806547164916992.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_weights_good_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.505111813545227.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [1.6002970933914185,0.505111813545227,1.6571693420410156,0.21333633206727462]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='4' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [4/40 1:47:04&lt;16:03:41]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.326428</td>\n",
       "      <td>0.325009</td>\n",
       "      <td>0.234470</td>\n",
       "      <td>0.355308</td>\n",
       "      <td>0.493184</td>\n",
       "      <td>25:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266216</td>\n",
       "      <td>0.291889</td>\n",
       "      <td>0.213740</td>\n",
       "      <td>0.315320</td>\n",
       "      <td>0.501836</td>\n",
       "      <td>26:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262931</td>\n",
       "      <td>0.283098</td>\n",
       "      <td>0.208533</td>\n",
       "      <td>0.304048</td>\n",
       "      <td>0.549058</td>\n",
       "      <td>27:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256095</td>\n",
       "      <td>0.275646</td>\n",
       "      <td>0.202871</td>\n",
       "      <td>0.295436</td>\n",
       "      <td>0.575205</td>\n",
       "      <td>27:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='3140' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.58% [3140/29687 02:52&lt;24:14 0.2453]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-25 23:31:58,521] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/tmp/ipykernel_23299/3148654658.py:112)\n",
      "   reasons:  ___check_obj_id(self, 140033784157216)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.32500866055488586.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-25 23:32:20,991] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torchvision/ops/misc.py:319)\n",
      "   reasons:  ___check_obj_id(self, 140033784156448)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 1 with valid_loss value: 0.2918890714645386.\n",
      "Better model found at epoch 2 with valid_loss value: 0.28309792280197144.\n",
      "Better model found at epoch 3 with valid_loss value: 0.2756464183330536.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(20, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164413</td>\n",
       "      <td>0.165433</td>\n",
       "      <td>0.177333</td>\n",
       "      <td>0.341697</td>\n",
       "      <td>0.417671</td>\n",
       "      <td>07:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157905</td>\n",
       "      <td>0.160126</td>\n",
       "      <td>0.169911</td>\n",
       "      <td>0.326655</td>\n",
       "      <td>0.454772</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.157170</td>\n",
       "      <td>0.166138</td>\n",
       "      <td>0.316997</td>\n",
       "      <td>0.479607</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157877</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.163412</td>\n",
       "      <td>0.310182</td>\n",
       "      <td>0.490179</td>\n",
       "      <td>07:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153847</td>\n",
       "      <td>0.151915</td>\n",
       "      <td>0.159921</td>\n",
       "      <td>0.296685</td>\n",
       "      <td>0.539397</td>\n",
       "      <td>07:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145465</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.153717</td>\n",
       "      <td>0.277628</td>\n",
       "      <td>0.580840</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      16.67% [1/6 07:29&lt;37:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196049</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>0.172460</td>\n",
       "      <td>0.320301</td>\n",
       "      <td>0.470430</td>\n",
       "      <td>07:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/29687 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 7e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/unet_v3_big.pth')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('unet_v3_big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/29687 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:201\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:560\u001b[0m, in \u001b[0;36mRecorder.after_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    559\u001b[0m mets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_mets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_mets\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m mets: \u001b[43mmet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:509\u001b[0m, in \u001b[0;36mAvgSmoothLoss.accumulate\u001b[0;34m(self, learn)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn):\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlerp(\u001b[43mto_detach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:244\u001b[0m, in \u001b[0;36mto_detach\u001b[0;34m(b, cpu, gather)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gather: x \u001b[38;5;241m=\u001b[39m maybe_gather(x)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m cpu \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:224\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_listy(x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x)([apply(func, o, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,\u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m {k: apply(func, v, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 224\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:243\u001b[0m, in \u001b[0;36mto_detach.<locals>._inner\u001b[0;34m(x, cpu, gather)\u001b[0m\n\u001b[1;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gather: x \u001b[38;5;241m=\u001b[39m maybe_gather(x)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cpu \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pl.read_csv('/mnt/ssd/kaggle/test.csv')\n",
    "test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "x_test = test_df.select(FEAT_COLS).to_numpy()\n",
    "\n",
    "test_ds = Loader({'x' : x_test, 'emb' : np.zeros(x_test.shape[0], dtype=int)}, {'x' : norm_x})\n",
    "test_loader = fv.DataLoader(test_ds, batch_size=bs, drop_last=False, shuffle=False, \n",
    "                            num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (625_000, 557)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_id</th><th>state_t_0</th><th>state_t_1</th><th>state_t_2</th><th>state_t_3</th><th>state_t_4</th><th>state_t_5</th><th>state_t_6</th><th>state_t_7</th><th>state_t_8</th><th>state_t_9</th><th>state_t_10</th><th>state_t_11</th><th>state_t_12</th><th>state_t_13</th><th>state_t_14</th><th>state_t_15</th><th>state_t_16</th><th>state_t_17</th><th>state_t_18</th><th>state_t_19</th><th>state_t_20</th><th>state_t_21</th><th>state_t_22</th><th>state_t_23</th><th>state_t_24</th><th>state_t_25</th><th>state_t_26</th><th>state_t_27</th><th>state_t_28</th><th>state_t_29</th><th>state_t_30</th><th>state_t_31</th><th>state_t_32</th><th>state_t_33</th><th>state_t_34</th><th>state_t_35</th><th>state_t_36</th><th>state_t_37</th><th>state_t_38</th><th>state_t_39</th><th>state_t_40</th><th>state_t_41</th><th>state_t_42</th><th>state_t_43</th><th>state_t_44</th><th>state_t_45</th><th>state_t_46</th><th>state_t_47</th><th>state_t_48</th><th>state_t_49</th><th>state_t_50</th><th>state_t_51</th><th>state_t_52</th><th>state_t_53</th><th>state_t_54</th><th>state_t_55</th><th>state_t_56</th><th>state_t_57</th><th>state_t_58</th><th>state_t_59</th><th>state_q0001_0</th><th>state_q0001_1</th><th>state_q0001_2</th><th>state_q0001_3</th><th>state_q0001_4</th><th>state_q0001_5</th><th>state_q0001_6</th><th>state_q0001_7</th><th>state_q0001_8</th><th>state_q0001_9</th><th>state_q0001_10</th><th>state_q0001_11</th><th>state_q0001_12</th><th>state_q0001_13</th><th>state_q0001_14</th><th>state_q0001_15</th><th>state_q0001_16</th><th>state_q0001_17</th><th>state_q0001_18</th><th>state_q0001_19</th><th>state_q0001_20</th><th>state_q0001_21</th><th>state_q0001_22</th><th>state_q0001_23</th><th>state_q0001_24</th><th>state_q0001_25</th><th>state_q0001_26</th><th>state_q0001_27</th><th>state_q0001_28</th><th>state_q0001_29</th><th>state_q0001_30</th><th>state_q0001_31</th><th>state_q0001_32</th><th>state_q0001_33</th><th>state_q0001_34</th><th>state_q0001_35</th><th>state_q0001_36</th><th>state_q0001_37</th><th>state_q0001_38</th><th>state_q0001_39</th><th>state_q0001_40</th><th>state_q0001_41</th><th>state_q0001_42</th><th>state_q0001_43</th><th>state_q0001_44</th><th>state_q0001_45</th><th>state_q0001_46</th><th>state_q0001_47</th><th>state_q0001_48</th><th>state_q0001_49</th><th>state_q0001_50</th><th>state_q0001_51</th><th>state_q0001_52</th><th>state_q0001_53</th><th>state_q0001_54</th><th>state_q0001_55</th><th>state_q0001_56</th><th>state_q0001_57</th><th>state_q0001_58</th><th>state_q0001_59</th><th>state_q0002_0</th><th>state_q0002_1</th><th>state_q0002_2</th><th>state_q0002_3</th><th>state_q0002_4</th><th>state_q0002_5</th><th>state_q0002_6</th><th>state_q0002_7</th><th>state_q0002_8</th><th>state_q0002_9</th><th>state_q0002_10</th><th>state_q0002_11</th><th>state_q0002_12</th><th>state_q0002_13</th><th>state_q0002_14</th><th>state_q0002_15</th><th>state_q0002_16</th><th>state_q0002_17</th><th>state_q0002_18</th><th>state_q0002_19</th><th>state_q0002_20</th><th>state_q0002_21</th><th>state_q0002_22</th><th>state_q0002_23</th><th>state_q0002_24</th><th>state_q0002_25</th><th>state_q0002_26</th><th>state_q0002_27</th><th>state_q0002_28</th><th>state_q0002_29</th><th>state_q0002_30</th><th>state_q0002_31</th><th>state_q0002_32</th><th>state_q0002_33</th><th>state_q0002_34</th><th>state_q0002_35</th><th>state_q0002_36</th><th>state_q0002_37</th><th>state_q0002_38</th><th>state_q0002_39</th><th>state_q0002_40</th><th>state_q0002_41</th><th>state_q0002_42</th><th>state_q0002_43</th><th>state_q0002_44</th><th>state_q0002_45</th><th>state_q0002_46</th><th>state_q0002_47</th><th>state_q0002_48</th><th>state_q0002_49</th><th>state_q0002_50</th><th>state_q0002_51</th><th>state_q0002_52</th><th>state_q0002_53</th><th>state_q0002_54</th><th>state_q0002_55</th><th>state_q0002_56</th><th>state_q0002_57</th><th>state_q0002_58</th><th>state_q0002_59</th><th>state_q0003_0</th><th>state_q0003_1</th><th>state_q0003_2</th><th>state_q0003_3</th><th>state_q0003_4</th><th>state_q0003_5</th><th>state_q0003_6</th><th>state_q0003_7</th><th>state_q0003_8</th><th>state_q0003_9</th><th>state_q0003_10</th><th>state_q0003_11</th><th>state_q0003_12</th><th>state_q0003_13</th><th>state_q0003_14</th><th>state_q0003_15</th><th>state_q0003_16</th><th>state_q0003_17</th><th>state_q0003_18</th><th>state_q0003_19</th><th>state_q0003_20</th><th>state_q0003_21</th><th>state_q0003_22</th><th>state_q0003_23</th><th>state_q0003_24</th><th>state_q0003_25</th><th>state_q0003_26</th><th>state_q0003_27</th><th>state_q0003_28</th><th>state_q0003_29</th><th>state_q0003_30</th><th>state_q0003_31</th><th>state_q0003_32</th><th>state_q0003_33</th><th>state_q0003_34</th><th>state_q0003_35</th><th>state_q0003_36</th><th>state_q0003_37</th><th>state_q0003_38</th><th>state_q0003_39</th><th>state_q0003_40</th><th>state_q0003_41</th><th>state_q0003_42</th><th>state_q0003_43</th><th>state_q0003_44</th><th>state_q0003_45</th><th>state_q0003_46</th><th>state_q0003_47</th><th>state_q0003_48</th><th>state_q0003_49</th><th>state_q0003_50</th><th>state_q0003_51</th><th>state_q0003_52</th><th>state_q0003_53</th><th>state_q0003_54</th><th>state_q0003_55</th><th>state_q0003_56</th><th>state_q0003_57</th><th>state_q0003_58</th><th>state_q0003_59</th><th>state_u_0</th><th>state_u_1</th><th>state_u_2</th><th>state_u_3</th><th>state_u_4</th><th>state_u_5</th><th>state_u_6</th><th>state_u_7</th><th>state_u_8</th><th>state_u_9</th><th>state_u_10</th><th>state_u_11</th><th>state_u_12</th><th>state_u_13</th><th>state_u_14</th><th>state_u_15</th><th>state_u_16</th><th>state_u_17</th><th>state_u_18</th><th>state_u_19</th><th>state_u_20</th><th>state_u_21</th><th>state_u_22</th><th>state_u_23</th><th>state_u_24</th><th>state_u_25</th><th>state_u_26</th><th>state_u_27</th><th>state_u_28</th><th>state_u_29</th><th>state_u_30</th><th>state_u_31</th><th>state_u_32</th><th>state_u_33</th><th>state_u_34</th><th>state_u_35</th><th>state_u_36</th><th>state_u_37</th><th>state_u_38</th><th>state_u_39</th><th>state_u_40</th><th>state_u_41</th><th>state_u_42</th><th>state_u_43</th><th>state_u_44</th><th>state_u_45</th><th>state_u_46</th><th>state_u_47</th><th>state_u_48</th><th>state_u_49</th><th>state_u_50</th><th>state_u_51</th><th>state_u_52</th><th>state_u_53</th><th>state_u_54</th><th>state_u_55</th><th>state_u_56</th><th>state_u_57</th><th>state_u_58</th><th>state_u_59</th><th>state_v_0</th><th>state_v_1</th><th>state_v_2</th><th>state_v_3</th><th>state_v_4</th><th>state_v_5</th><th>state_v_6</th><th>state_v_7</th><th>state_v_8</th><th>state_v_9</th><th>state_v_10</th><th>state_v_11</th><th>state_v_12</th><th>state_v_13</th><th>state_v_14</th><th>state_v_15</th><th>state_v_16</th><th>state_v_17</th><th>state_v_18</th><th>state_v_19</th><th>state_v_20</th><th>state_v_21</th><th>state_v_22</th><th>state_v_23</th><th>state_v_24</th><th>state_v_25</th><th>state_v_26</th><th>state_v_27</th><th>state_v_28</th><th>state_v_29</th><th>state_v_30</th><th>state_v_31</th><th>state_v_32</th><th>state_v_33</th><th>state_v_34</th><th>state_v_35</th><th>state_v_36</th><th>state_v_37</th><th>state_v_38</th><th>state_v_39</th><th>state_v_40</th><th>state_v_41</th><th>state_v_42</th><th>state_v_43</th><th>state_v_44</th><th>state_v_45</th><th>state_v_46</th><th>state_v_47</th><th>state_v_48</th><th>state_v_49</th><th>state_v_50</th><th>state_v_51</th><th>state_v_52</th><th>state_v_53</th><th>state_v_54</th><th>state_v_55</th><th>state_v_56</th><th>state_v_57</th><th>state_v_58</th><th>state_v_59</th><th>state_ps</th><th>pbuf_SOLIN</th><th>pbuf_LHFLX</th><th>pbuf_SHFLX</th><th>pbuf_TAUX</th><th>pbuf_TAUY</th><th>pbuf_COSZRS</th><th>cam_in_ALDIF</th><th>cam_in_ALDIR</th><th>cam_in_ASDIF</th><th>cam_in_ASDIR</th><th>cam_in_LWUP</th><th>cam_in_ICEFRAC</th><th>cam_in_LANDFRAC</th><th>cam_in_OCNFRAC</th><th>cam_in_SNOWHLAND</th><th>pbuf_ozone_0</th><th>pbuf_ozone_1</th><th>pbuf_ozone_2</th><th>pbuf_ozone_3</th><th>pbuf_ozone_4</th><th>pbuf_ozone_5</th><th>pbuf_ozone_6</th><th>pbuf_ozone_7</th><th>pbuf_ozone_8</th><th>pbuf_ozone_9</th><th>pbuf_ozone_10</th><th>pbuf_ozone_11</th><th>pbuf_ozone_12</th><th>pbuf_ozone_13</th><th>pbuf_ozone_14</th><th>pbuf_ozone_15</th><th>pbuf_ozone_16</th><th>pbuf_ozone_17</th><th>pbuf_ozone_18</th><th>pbuf_ozone_19</th><th>pbuf_ozone_20</th><th>pbuf_ozone_21</th><th>pbuf_ozone_22</th><th>pbuf_ozone_23</th><th>pbuf_ozone_24</th><th>pbuf_ozone_25</th><th>pbuf_ozone_26</th><th>pbuf_ozone_27</th><th>pbuf_ozone_28</th><th>pbuf_ozone_29</th><th>pbuf_ozone_30</th><th>pbuf_ozone_31</th><th>pbuf_ozone_32</th><th>pbuf_ozone_33</th><th>pbuf_ozone_34</th><th>pbuf_ozone_35</th><th>pbuf_ozone_36</th><th>pbuf_ozone_37</th><th>pbuf_ozone_38</th><th>pbuf_ozone_39</th><th>pbuf_ozone_40</th><th>pbuf_ozone_41</th><th>pbuf_ozone_42</th><th>pbuf_ozone_43</th><th>pbuf_ozone_44</th><th>pbuf_ozone_45</th><th>pbuf_ozone_46</th><th>pbuf_ozone_47</th><th>pbuf_ozone_48</th><th>pbuf_ozone_49</th><th>pbuf_ozone_50</th><th>pbuf_ozone_51</th><th>pbuf_ozone_52</th><th>pbuf_ozone_53</th><th>pbuf_ozone_54</th><th>pbuf_ozone_55</th><th>pbuf_ozone_56</th><th>pbuf_ozone_57</th><th>pbuf_ozone_58</th><th>pbuf_ozone_59</th><th>pbuf_CH4_0</th><th>pbuf_CH4_1</th><th>pbuf_CH4_2</th><th>pbuf_CH4_3</th><th>pbuf_CH4_4</th><th>pbuf_CH4_5</th><th>pbuf_CH4_6</th><th>pbuf_CH4_7</th><th>pbuf_CH4_8</th><th>pbuf_CH4_9</th><th>pbuf_CH4_10</th><th>pbuf_CH4_11</th><th>pbuf_CH4_12</th><th>pbuf_CH4_13</th><th>pbuf_CH4_14</th><th>pbuf_CH4_15</th><th>pbuf_CH4_16</th><th>pbuf_CH4_17</th><th>pbuf_CH4_18</th><th>pbuf_CH4_19</th><th>pbuf_CH4_20</th><th>pbuf_CH4_21</th><th>pbuf_CH4_22</th><th>pbuf_CH4_23</th><th>pbuf_CH4_24</th><th>pbuf_CH4_25</th><th>pbuf_CH4_26</th><th>pbuf_CH4_27</th><th>pbuf_CH4_28</th><th>pbuf_CH4_29</th><th>pbuf_CH4_30</th><th>pbuf_CH4_31</th><th>pbuf_CH4_32</th><th>pbuf_CH4_33</th><th>pbuf_CH4_34</th><th>pbuf_CH4_35</th><th>pbuf_CH4_36</th><th>pbuf_CH4_37</th><th>pbuf_CH4_38</th><th>pbuf_CH4_39</th><th>pbuf_CH4_40</th><th>pbuf_CH4_41</th><th>pbuf_CH4_42</th><th>pbuf_CH4_43</th><th>pbuf_CH4_44</th><th>pbuf_CH4_45</th><th>pbuf_CH4_46</th><th>pbuf_CH4_47</th><th>pbuf_CH4_48</th><th>pbuf_CH4_49</th><th>pbuf_CH4_50</th><th>pbuf_CH4_51</th><th>pbuf_CH4_52</th><th>pbuf_CH4_53</th><th>pbuf_CH4_54</th><th>pbuf_CH4_55</th><th>pbuf_CH4_56</th><th>pbuf_CH4_57</th><th>pbuf_CH4_58</th><th>pbuf_CH4_59</th><th>pbuf_N2O_0</th><th>pbuf_N2O_1</th><th>pbuf_N2O_2</th><th>pbuf_N2O_3</th><th>pbuf_N2O_4</th><th>pbuf_N2O_5</th><th>pbuf_N2O_6</th><th>pbuf_N2O_7</th><th>pbuf_N2O_8</th><th>pbuf_N2O_9</th><th>pbuf_N2O_10</th><th>pbuf_N2O_11</th><th>pbuf_N2O_12</th><th>pbuf_N2O_13</th><th>pbuf_N2O_14</th><th>pbuf_N2O_15</th><th>pbuf_N2O_16</th><th>pbuf_N2O_17</th><th>pbuf_N2O_18</th><th>pbuf_N2O_19</th><th>pbuf_N2O_20</th><th>pbuf_N2O_21</th><th>pbuf_N2O_22</th><th>pbuf_N2O_23</th><th>pbuf_N2O_24</th><th>pbuf_N2O_25</th><th>pbuf_N2O_26</th><th>pbuf_N2O_27</th><th>pbuf_N2O_28</th><th>pbuf_N2O_29</th><th>pbuf_N2O_30</th><th>pbuf_N2O_31</th><th>pbuf_N2O_32</th><th>pbuf_N2O_33</th><th>pbuf_N2O_34</th><th>pbuf_N2O_35</th><th>pbuf_N2O_36</th><th>pbuf_N2O_37</th><th>pbuf_N2O_38</th><th>pbuf_N2O_39</th><th>pbuf_N2O_40</th><th>pbuf_N2O_41</th><th>pbuf_N2O_42</th><th>pbuf_N2O_43</th><th>pbuf_N2O_44</th><th>pbuf_N2O_45</th><th>pbuf_N2O_46</th><th>pbuf_N2O_47</th><th>pbuf_N2O_48</th><th>pbuf_N2O_49</th><th>pbuf_N2O_50</th><th>pbuf_N2O_51</th><th>pbuf_N2O_52</th><th>pbuf_N2O_53</th><th>pbuf_N2O_54</th><th>pbuf_N2O_55</th><th>pbuf_N2O_56</th><th>pbuf_N2O_57</th><th>pbuf_N2O_58</th><th>pbuf_N2O_59</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;test_169651&quot;</td><td>209.802593</td><td>220.698213</td><td>227.783289</td><td>241.386812</td><td>254.602962</td><td>262.319063</td><td>261.308368</td><td>254.062035</td><td>243.90442</td><td>236.302401</td><td>229.972123</td><td>225.227444</td><td>221.043785</td><td>215.828104</td><td>208.998899</td><td>203.059782</td><td>199.041293</td><td>198.233715</td><td>196.024335</td><td>198.165421</td><td>201.194742</td><td>205.367505</td><td>209.875699</td><td>214.748964</td><td>219.542131</td><td>224.305865</td><td>228.952891</td><td>233.365752</td><td>237.65552</td><td>241.956551</td><td>246.022055</td><td>249.808255</td><td>253.459608</td><td>256.971278</td><td>260.294397</td><td>263.400258</td><td>266.453995</td><td>269.335235</td><td>271.931611</td><td>274.345917</td><td>276.364867</td><td>278.199151</td><td>279.774969</td><td>281.114457</td><td>282.216242</td><td>283.134861</td><td>284.583856</td><td>285.660463</td><td>286.682694</td><td>287.647368</td><td>288.319392</td><td>288.954874</td><td>289.686522</td><td>290.47467</td><td>291.374321</td><td>292.347968</td><td>293.43973</td><td>294.438724</td><td>295.38002</td><td>296.391446</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000011</td><td>0.00002</td><td>0.000032</td><td>0.000042</td><td>0.000059</td><td>0.000091</td><td>0.000152</td><td>0.000257</td><td>0.000398</td><td>0.000575</td><td>0.000798</td><td>0.001029</td><td>0.001247</td><td>0.001508</td><td>0.001851</td><td>0.002217</td><td>0.002593</td><td>0.003069</td><td>0.003637</td><td>0.004323</td><td>0.004914</td><td>0.005655</td><td>0.006382</td><td>0.007161</td><td>0.00809</td><td>0.008422</td><td>0.009062</td><td>0.009533</td><td>0.010061</td><td>0.010851</td><td>0.011723</td><td>0.01231</td><td>0.012901</td><td>0.013413</td><td>0.013863</td><td>0.014226</td><td>0.014691</td><td>0.01539</td><td>0.015926</td><td>9.7013e-37</td><td>9.7067e-37</td><td>9.5320e-37</td><td>9.0000e-37</td><td>8.8489e-37</td><td>8.7347e-37</td><td>7.4539e-37</td><td>5.7589e-37</td><td>4.1264e-37</td><td>2.5798e-37</td><td>9.0800e-38</td><td>8.8615e-39</td><td>3.9948e-43</td><td>5.2197e-49</td><td>3.8237e-55</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.2248e-49</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.3843e-31</td><td>8.4492e-27</td><td>1.0226e-28</td><td>2.3281e-18</td><td>5.3019e-20</td><td>0.0</td><td>1.7424e-7</td><td>1.9801e-7</td><td>5.1508e-7</td><td>0.0</td><td>3.3095e-7</td><td>5.4720e-7</td><td>5.0558e-7</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.000004</td><td>0.000001</td><td>0.000004</td><td>0.000003</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000003</td><td>0.000007</td><td>0.000005</td><td>0.00001</td><td>0.00001</td><td>0.000004</td><td>2.4976e-7</td><td>7.4434e-7</td><td>2.4470e-7</td><td>7.5538e-9</td><td>3.2430e-11</td><td>3.5474e-11</td><td>3.9380e-11</td><td>4.7086e-11</td><td>5.1482e-11</td><td>5.5466e-11</td><td>5.5631e-11</td><td>5.0086e-11</td><td>4.1181e-11</td><td>3.0849e-11</td><td>1.2191e-11</td><td>1.2796e-12</td><td>5.5664e-17</td><td>8.9162e-23</td><td>2.3687e-24</td><td>0.0</td><td>1.6279e-24</td><td>5.6999e-20</td><td>8.7989e-17</td><td>3.3620e-11</td><td>1.6776e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.000002</td><td>0.000001</td><td>0.000002</td><td>5.0476e-7</td><td>1.7979e-7</td><td>6.1143e-7</td><td>9.7138e-8</td><td>3.2653e-7</td><td>2.4650e-7</td><td>7.7237e-8</td><td>1.6265e-8</td><td>2.6423e-8</td><td>5.0995e-8</td><td>5.2668e-8</td><td>3.7720e-8</td><td>7.3597e-8</td><td>5.2720e-8</td><td>3.7769e-8</td><td>3.7740e-9</td><td>2.3163e-9</td><td>2.7407e-10</td><td>6.0542e-11</td><td>1.4243e-11</td><td>4.6001e-12</td><td>0.0</td><td>3.0952e-13</td><td>1.9876e-13</td><td>2.0088e-13</td><td>1.2196e-14</td><td>1.2907e-12</td><td>-12.621098</td><td>-35.264994</td><td>-39.288286</td><td>-26.101457</td><td>-27.027251</td><td>-26.977124</td><td>-25.681033</td><td>-24.116904</td><td>-21.753854</td><td>-18.174458</td><td>-13.102039</td><td>-7.057726</td><td>-2.783299</td><td>0.48277</td><td>6.097161</td><td>9.94475</td><td>14.382375</td><td>20.15391</td><td>24.696414</td><td>28.10628</td><td>31.409991</td><td>34.470184</td><td>35.623454</td><td>35.262472</td><td>33.688433</td><td>31.477991</td><td>29.394856</td><td>27.362967</td><td>25.418579</td><td>23.586494</td><td>21.805095</td><td>20.05106</td><td>18.329192</td><td>16.628743</td><td>15.029511</td><td>13.527132</td><td>12.198934</td><td>11.006181</td><td>9.946034</td><td>8.986405</td><td>8.046804</td><td>7.155062</td><td>6.298709</td><td>5.470431</td><td>4.767571</td><td>4.192104</td><td>3.654444</td><td>3.092111</td><td>2.546125</td><td>2.053815</td><td>1.582154</td><td>1.125412</td><td>0.672671</td><td>0.210809</td><td>-0.256035</td><td>-0.681038</td><td>-1.069693</td><td>-1.349754</td><td>-1.312553</td><td>-1.116497</td><td>11.938681</td><td>6.037094</td><td>11.908263</td><td>2.385834</td><td>-3.281934</td><td>0.980046</td><td>5.255962</td><td>5.241453</td><td>1.689835</td><td>0.214683</td><td>1.039891</td><td>1.317828</td><td>0.97116</td><td>3.472131</td><td>4.706899</td><td>2.93737</td><td>3.055634</td><td>2.977136</td><td>2.058352</td><td>1.935215</td><td>2.134694</td><td>2.024551</td><td>1.55467</td><td>1.581369</td><td>1.578897</td><td>1.384252</td><td>1.128416</td><td>0.848605</td><td>0.540527</td><td>0.349101</td><td>0.37602</td><td>0.567414</td><td>0.844342</td><td>1.054674</td><td>1.090906</td><td>0.989244</td><td>0.73123</td><td>0.475887</td><td>0.305124</td><td>0.26951</td><td>0.256586</td><td>0.27095</td><td>0.218726</td><td>0.102173</td><td>0.021138</td><td>0.068425</td><td>0.137398</td><td>0.18148</td><td>0.228599</td><td>0.281241</td><td>0.318975</td><td>0.346498</td><td>0.386231</td><td>0.437631</td><td>0.492904</td><td>0.525097</td><td>0.519203</td><td>0.527845</td><td>0.526496</td><td>0.169698</td><td>100600.027964</td><td>0.0</td><td>4.347225</td><td>-0.246807</td><td>0.001545</td><td>-0.0006</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>438.758554</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6356e-7</td><td>4.7469e-7</td><td>8.5083e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000013</td><td>0.000014</td><td>0.000013</td><td>0.000012</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>8.4424e-7</td><td>4.8861e-7</td><td>3.3337e-7</td><td>2.4476e-7</td><td>1.8146e-7</td><td>1.3446e-7</td><td>1.0342e-7</td><td>8.9834e-8</td><td>8.0409e-8</td><td>7.4827e-8</td><td>7.1977e-8</td><td>6.9728e-8</td><td>6.8498e-8</td><td>6.7445e-8</td><td>6.6510e-8</td><td>6.5643e-8</td><td>6.4768e-8</td><td>6.3963e-8</td><td>6.3137e-8</td><td>6.2009e-8</td><td>6.0600e-8</td><td>5.9198e-8</td><td>5.7647e-8</td><td>5.5942e-8</td><td>5.4316e-8</td><td>5.2773e-8</td><td>5.0042e-8</td><td>4.7254e-8</td><td>4.4604e-8</td><td>4.2089e-8</td><td>4.0019e-8</td><td>3.8186e-8</td><td>3.6475e-8</td><td>3.4873e-8</td><td>3.3392e-8</td><td>3.2752e-8</td><td>3.2130e-8</td><td>3.1515e-8</td><td>3.0900e-8</td><td>3.0344e-8</td><td>2.9831e-8</td><td>2.9312e-8</td><td>2.8908e-8</td><td>1.7047e-7</td><td>1.9578e-7</td><td>2.2460e-7</td><td>2.5711e-7</td><td>2.9331e-7</td><td>3.3293e-7</td><td>3.7542e-7</td><td>4.2000e-7</td><td>4.6582e-7</td><td>5.1216e-7</td><td>5.5852e-7</td><td>6.0466e-7</td><td>6.5047e-7</td><td>6.9585e-7</td><td>7.4055e-7</td><td>7.8419e-7</td><td>8.2638e-7</td><td>8.6682e-7</td><td>9.0540e-7</td><td>9.4221e-7</td><td>9.7737e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.7090e-8</td><td>3.3989e-8</td><td>4.2566e-8</td><td>5.3122e-8</td><td>6.5920e-8</td><td>8.1133e-8</td><td>9.8783e-8</td><td>1.1873e-7</td><td>1.4068e-7</td><td>1.6434e-7</td><td>1.8941e-7</td><td>2.1572e-7</td><td>2.4315e-7</td><td>2.7157e-7</td><td>3.0073e-7</td><td>3.3032e-7</td><td>3.5994e-7</td><td>3.8925e-7</td><td>4.1805e-7</td><td>4.4625e-7</td><td>4.7387e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_524862&quot;</td><td>208.366101</td><td>219.238711</td><td>228.265017</td><td>242.099772</td><td>256.16427</td><td>263.783211</td><td>261.835384</td><td>253.636172</td><td>243.783467</td><td>236.961625</td><td>231.254018</td><td>226.821328</td><td>223.089416</td><td>218.781736</td><td>213.044729</td><td>207.428738</td><td>203.258569</td><td>200.945988</td><td>198.008377</td><td>200.708751</td><td>202.260369</td><td>205.759887</td><td>209.204746</td><td>213.188264</td><td>217.391038</td><td>221.709</td><td>226.124449</td><td>230.24439</td><td>234.354927</td><td>238.466732</td><td>242.440843</td><td>246.280318</td><td>249.962632</td><td>253.491784</td><td>256.890607</td><td>260.038062</td><td>263.105183</td><td>265.969345</td><td>268.651067</td><td>271.115895</td><td>273.156304</td><td>275.05413</td><td>276.859299</td><td>278.323683</td><td>279.625526</td><td>280.895406</td><td>281.976503</td><td>283.153419</td><td>284.207563</td><td>285.304527</td><td>286.422891</td><td>287.165373</td><td>287.948619</td><td>288.710265</td><td>289.541786</td><td>290.338582</td><td>291.228917</td><td>292.075051</td><td>292.772405</td><td>293.552796</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000019</td><td>0.000025</td><td>0.000032</td><td>0.000045</td><td>0.000075</td><td>0.000142</td><td>0.000244</td><td>0.000356</td><td>0.000484</td><td>0.000632</td><td>0.000844</td><td>0.001119</td><td>0.001435</td><td>0.001796</td><td>0.002197</td><td>0.002635</td><td>0.003162</td><td>0.003662</td><td>0.004397</td><td>0.004992</td><td>0.005598</td><td>0.006273</td><td>0.006898</td><td>0.007266</td><td>0.007811</td><td>0.008264</td><td>0.008604</td><td>0.008909</td><td>0.009354</td><td>0.009941</td><td>0.010509</td><td>0.010907</td><td>0.011343</td><td>0.011847</td><td>0.0123</td><td>0.012884</td><td>0.013599</td><td>0.014058</td><td>9.7233e-37</td><td>9.6757e-37</td><td>9.5398e-37</td><td>9.3217e-37</td><td>9.4038e-37</td><td>9.2493e-37</td><td>8.1177e-37</td><td>6.6361e-37</td><td>5.3593e-37</td><td>3.9227e-37</td><td>1.4220e-37</td><td>1.5365e-38</td><td>2.9856e-43</td><td>4.8280e-51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.4922e-40</td><td>3.4044e-35</td><td>1.2520e-30</td><td>0.0</td><td>6.3843e-31</td><td>8.4492e-27</td><td>1.0226e-28</td><td>2.3281e-18</td><td>5.3019e-20</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.3582e-7</td><td>7.9312e-8</td><td>1.3835e-7</td><td>0.000002</td><td>0.000012</td><td>0.000037</td><td>0.00002</td><td>0.000014</td><td>0.000033</td><td>0.000027</td><td>0.000029</td><td>0.000005</td><td>0.000007</td><td>0.000022</td><td>0.000028</td><td>0.000015</td><td>0.000013</td><td>0.000007</td><td>0.000002</td><td>0.000001</td><td>0.000003</td><td>1.6578e-8</td><td>3.2606e-11</td><td>3.4582e-11</td><td>3.9890e-11</td><td>4.5706e-11</td><td>5.0204e-11</td><td>5.5300e-11</td><td>5.7235e-11</td><td>5.4381e-11</td><td>4.8958e-11</td><td>4.1832e-11</td><td>1.8482e-11</td><td>2.3615e-12</td><td>5.3417e-17</td><td>2.3347e-25</td><td>7.4363e-25</td><td>3.2001e-20</td><td>1.6279e-24</td><td>2.5878e-14</td><td>2.4028e-12</td><td>1.5160e-10</td><td>3.5728e-9</td><td>1.5177e-8</td><td>6.6076e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.1687e-7</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.6779e-10</td><td>1.9253e-8</td><td>1.0758e-7</td><td>1.4760e-7</td><td>2.1158e-7</td><td>2.1672e-7</td><td>2.4495e-7</td><td>1.4108e-7</td><td>2.6630e-7</td><td>6.8928e-7</td><td>7.9143e-8</td><td>4.7101e-8</td><td>4.8511e-8</td><td>3.8275e-9</td><td>2.0314e-8</td><td>1.9337e-9</td><td>6.0377e-10</td><td>5.5262e-11</td><td>7.8513e-12</td><td>8.6035e-10</td><td>2.1053e-12</td><td>1.1164e-12</td><td>1.0425e-12</td><td>7.0639e-14</td><td>9.8122e-13</td><td>-29.893361</td><td>-39.554844</td><td>-34.522524</td><td>-21.928289</td><td>-18.223787</td><td>-19.48342</td><td>-19.543606</td><td>-18.398179</td><td>-15.392368</td><td>-12.175464</td><td>-8.241326</td><td>-3.019894</td><td>2.268972</td><td>5.34779</td><td>8.920504</td><td>14.489345</td><td>18.426973</td><td>22.032316</td><td>25.706922</td><td>28.965752</td><td>31.16677</td><td>32.408526</td><td>32.771722</td><td>32.556432</td><td>31.808454</td><td>30.882596</td><td>29.846462</td><td>28.652634</td><td>27.30233</td><td>25.846441</td><td>24.259796</td><td>22.670197</td><td>21.183674</td><td>19.844425</td><td>18.700619</td><td>17.666287</td><td>16.707244</td><td>15.681082</td><td>14.583848</td><td>13.460464</td><td>12.467512</td><td>11.563934</td><td>10.787628</td><td>10.096707</td><td>9.47082</td><td>8.897509</td><td>8.334815</td><td>7.766036</td><td>7.249928</td><td>6.811481</td><td>6.43075</td><td>6.074011</td><td>5.748827</td><td>5.444443</td><td>5.16472</td><td>4.916659</td><td>4.68944</td><td>4.454772</td><td>4.332837</td><td>3.568685</td><td>3.407441</td><td>7.927505</td><td>14.541661</td><td>7.870468</td><td>0.851383</td><td>2.700654</td><td>5.313124</td><td>4.424651</td><td>0.94704</td><td>-1.047635</td><td>-0.371438</td><td>0.849279</td><td>0.934926</td><td>1.496484</td><td>4.686353</td><td>5.375017</td><td>5.183725</td><td>6.110477</td><td>7.373743</td><td>8.273774</td><td>8.783596</td><td>9.070392</td><td>8.906493</td><td>8.417356</td><td>7.664339</td><td>6.864443</td><td>6.386836</td><td>5.944948</td><td>5.524617</td><td>5.08944</td><td>4.663317</td><td>4.18785</td><td>3.588459</td><td>2.90988</td><td>2.254511</td><td>1.656692</td><td>1.202701</td><td>0.945096</td><td>0.816315</td><td>0.691156</td><td>0.492947</td><td>0.210738</td><td>-0.034511</td><td>-0.223913</td><td>-0.30456</td><td>-0.306024</td><td>-0.31283</td><td>-0.318147</td><td>-0.329014</td><td>-0.361018</td><td>-0.399833</td><td>-0.439263</td><td>-0.485964</td><td>-0.515237</td><td>-0.548946</td><td>-0.609114</td><td>-0.656643</td><td>-0.768443</td><td>-1.045759</td><td>-1.761068</td><td>101156.338133</td><td>0.0</td><td>4.390058</td><td>-1.169037</td><td>-0.011148</td><td>0.005213</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>420.971275</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6128e-7</td><td>4.7059e-7</td><td>8.4348e-7</td><td>0.000001</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000012</td><td>0.000013</td><td>0.000014</td><td>0.000013</td><td>0.000012</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>6.0761e-7</td><td>4.2808e-7</td><td>3.2513e-7</td><td>2.4502e-7</td><td>1.7848e-7</td><td>1.2772e-7</td><td>1.0575e-7</td><td>9.0547e-8</td><td>8.1658e-8</td><td>7.6913e-8</td><td>7.3041e-8</td><td>7.0865e-8</td><td>6.8947e-8</td><td>6.7412e-8</td><td>6.6084e-8</td><td>6.4747e-8</td><td>6.3495e-8</td><td>6.2212e-8</td><td>6.0667e-8</td><td>5.8916e-8</td><td>5.7175e-8</td><td>5.5445e-8</td><td>5.3750e-8</td><td>5.2134e-8</td><td>5.0600e-8</td><td>4.8114e-8</td><td>4.5698e-8</td><td>4.3400e-8</td><td>4.1220e-8</td><td>3.9122e-8</td><td>3.7149e-8</td><td>3.5305e-8</td><td>3.3580e-8</td><td>3.2174e-8</td><td>3.1576e-8</td><td>3.0994e-8</td><td>3.0419e-8</td><td>2.9844e-8</td><td>2.9475e-8</td><td>2.9160e-8</td><td>2.8842e-8</td><td>2.8615e-8</td><td>1.6823e-7</td><td>1.9321e-7</td><td>2.2165e-7</td><td>2.5373e-7</td><td>2.8945e-7</td><td>3.2855e-7</td><td>3.7049e-7</td><td>4.1448e-7</td><td>4.5970e-7</td><td>5.0543e-7</td><td>5.5118e-7</td><td>5.9671e-7</td><td>6.4193e-7</td><td>6.8671e-7</td><td>7.3082e-7</td><td>7.7389e-7</td><td>8.1552e-7</td><td>8.5543e-7</td><td>8.9351e-7</td><td>9.2983e-7</td><td>9.6453e-7</td><td>9.9771e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.5728e-8</td><td>3.2356e-8</td><td>4.0614e-8</td><td>5.0801e-8</td><td>6.3180e-8</td><td>7.7926e-8</td><td>9.5069e-8</td><td>1.1448e-7</td><td>1.3589e-7</td><td>1.5899e-7</td><td>1.8351e-7</td><td>2.0928e-7</td><td>2.3618e-7</td><td>2.6408e-7</td><td>2.9275e-7</td><td>3.2186e-7</td><td>3.5102e-7</td><td>3.7992e-7</td><td>4.0832e-7</td><td>4.3617e-7</td><td>4.6344e-7</td><td>4.9013e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_634129&quot;</td><td>213.073771</td><td>229.421551</td><td>233.368455</td><td>242.683788</td><td>252.912786</td><td>260.953981</td><td>260.97731</td><td>253.059885</td><td>240.92989</td><td>232.91791</td><td>226.803598</td><td>221.937333</td><td>217.64686</td><td>213.245735</td><td>206.346627</td><td>197.762532</td><td>190.748626</td><td>189.534278</td><td>191.340472</td><td>192.504004</td><td>198.064985</td><td>204.083699</td><td>210.659256</td><td>216.965954</td><td>222.765872</td><td>228.264432</td><td>233.528284</td><td>238.41498</td><td>243.084839</td><td>247.407864</td><td>251.559082</td><td>255.27033</td><td>258.692439</td><td>261.988075</td><td>265.12583</td><td>268.078964</td><td>270.627849</td><td>272.981317</td><td>275.178158</td><td>277.326784</td><td>279.233698</td><td>280.737749</td><td>281.946893</td><td>283.24825</td><td>284.541981</td><td>285.846237</td><td>286.928886</td><td>287.874164</td><td>288.97811</td><td>289.864875</td><td>290.715822</td><td>291.699133</td><td>292.547412</td><td>293.170274</td><td>294.038241</td><td>295.018406</td><td>296.049939</td><td>297.159615</td><td>298.159719</td><td>299.146501</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000004</td><td>0.000009</td><td>0.000023</td><td>0.000049</td><td>0.000079</td><td>0.000129</td><td>0.000197</td><td>0.000301</td><td>0.000421</td><td>0.000615</td><td>0.000799</td><td>0.00101</td><td>0.001195</td><td>0.001334</td><td>0.001478</td><td>0.001718</td><td>0.002179</td><td>0.00276</td><td>0.003317</td><td>0.003839</td><td>0.004386</td><td>0.005077</td><td>0.006277</td><td>0.007519</td><td>0.008327</td><td>0.009056</td><td>0.009829</td><td>0.010764</td><td>0.011435</td><td>0.012143</td><td>0.01285</td><td>0.013119</td><td>0.0135</td><td>0.014371</td><td>0.015046</td><td>0.015551</td><td>0.015951</td><td>0.016206</td><td>0.016689</td><td>0.017309</td><td>9.7196e-37</td><td>9.6819e-37</td><td>9.4737e-37</td><td>8.6130e-37</td><td>7.4553e-37</td><td>7.0430e-37</td><td>5.8079e-37</td><td>3.7533e-37</td><td>1.8347e-37</td><td>2.6609e-38</td><td>1.5455e-38</td><td>4.6986e-39</td><td>2.0639e-42</td><td>1.7214e-48</td><td>6.7973e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0323e-52</td><td>7.5774e-48</td><td>1.6654e-42</td><td>2.8840e-37</td><td>1.5806e-32</td><td>5.5908e-28</td><td>1.2081e-24</td><td>8.0682e-20</td><td>2.1577e-15</td><td>5.8245e-11</td><td>1.3243e-8</td><td>3.2364e-7</td><td>8.0410e-7</td><td>2.9317e-7</td><td>1.7491e-7</td><td>2.0769e-7</td><td>0.000003</td><td>0.000004</td><td>6.1060e-7</td><td>5.5421e-7</td><td>5.5637e-7</td><td>8.9581e-7</td><td>0.000006</td><td>0.000004</td><td>0.000003</td><td>0.000004</td><td>0.000006</td><td>0.000075</td><td>0.000017</td><td>0.000014</td><td>0.000018</td><td>0.000007</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>0.000002</td><td>8.7045e-7</td><td>1.5553e-7</td><td>1.9860e-8</td><td>1.7795e-9</td><td>3.1575e-11</td><td>3.5242e-11</td><td>3.9756e-11</td><td>4.6787e-11</td><td>4.7899e-11</td><td>4.9267e-11</td><td>4.6184e-11</td><td>3.4904e-11</td><td>2.3260e-11</td><td>6.1475e-12</td><td>2.7373e-12</td><td>7.5081e-13</td><td>2.6179e-16</td><td>2.6664e-22</td><td>5.1042e-25</td><td>2.6412e-18</td><td>9.8686e-14</td><td>3.8788e-11</td><td>1.0422e-10</td><td>1.0476e-9</td><td>1.5938e-8</td><td>1.9242e-7</td><td>6.5540e-7</td><td>0.000002</td><td>0.000002</td><td>0.000006</td><td>0.000009</td><td>0.000009</td><td>0.000006</td><td>8.7136e-7</td><td>0.000003</td><td>0.000003</td><td>0.000002</td><td>6.3892e-7</td><td>5.3047e-7</td><td>2.7988e-7</td><td>2.3266e-7</td><td>2.4639e-8</td><td>1.2039e-9</td><td>1.0484e-9</td><td>0.0</td><td>1.2023e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.8992e-12</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.386828</td><td>-11.512458</td><td>-24.817131</td><td>-20.796563</td><td>-37.669454</td><td>-43.577506</td><td>-43.414096</td><td>-41.314129</td><td>-38.498899</td><td>-33.173973</td><td>-25.493209</td><td>-18.301585</td><td>-14.229118</td><td>-9.429682</td><td>-2.728357</td><td>1.08357</td><td>5.246548</td><td>10.596983</td><td>13.751444</td><td>17.168239</td><td>22.897938</td><td>27.350709</td><td>28.289143</td><td>27.581126</td><td>24.350959</td><td>20.201542</td><td>16.367193</td><td>13.093764</td><td>10.395265</td><td>8.339274</td><td>6.783111</td><td>5.510704</td><td>4.329976</td><td>3.196551</td><td>2.071899</td><td>0.949127</td><td>0.071386</td><td>-0.575528</td><td>-0.958757</td><td>-1.161323</td><td>-1.364285</td><td>-1.642227</td><td>-2.055358</td><td>-2.538765</td><td>-3.052665</td><td>-3.529975</td><td>-3.950433</td><td>-4.343366</td><td>-4.695296</td><td>-5.011322</td><td>-5.323996</td><td>-5.662009</td><td>-5.971492</td><td>-6.224755</td><td>-6.444167</td><td>-6.645438</td><td>-6.78385</td><td>-6.794718</td><td>-6.425409</td><td>-5.456625</td><td>20.983392</td><td>5.249665</td><td>4.479647</td><td>-3.157389</td><td>-5.787299</td><td>-0.279474</td><td>5.039368</td><td>5.344292</td><td>1.770346</td><td>0.714379</td><td>2.077056</td><td>1.639421</td><td>0.589909</td><td>2.806933</td><td>3.728732</td><td>3.032948</td><td>2.687794</td><td>1.551657</td><td>-0.080055</td><td>-2.55187</td><td>-4.148039</td><td>-5.943878</td><td>-6.167202</td><td>-5.431529</td><td>-3.50839</td><td>-2.183702</td><td>-1.755617</td><td>-1.703193</td><td>-1.861061</td><td>-2.088447</td><td>-2.089375</td><td>-1.82621</td><td>-1.27117</td><td>-0.607347</td><td>-0.032128</td><td>0.28745</td><td>0.364168</td><td>0.389445</td><td>0.436952</td><td>0.554593</td><td>0.725587</td><td>0.87684</td><td>0.985213</td><td>1.026965</td><td>1.047064</td><td>1.080241</td><td>1.090479</td><td>1.064247</td><td>1.027141</td><td>0.994794</td><td>0.945611</td><td>0.869893</td><td>0.813989</td><td>0.765646</td><td>0.7399</td><td>0.722111</td><td>0.725107</td><td>0.758654</td><td>0.7801</td><td>0.821863</td><td>99186.272487</td><td>0.0</td><td>70.755197</td><td>-0.892743</td><td>0.033205</td><td>-0.00567</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>455.793439</td><td>0.0</td><td>0.298189</td><td>0.701811</td><td>0.0</td><td>2.7551e-7</td><td>4.9620e-7</td><td>8.8940e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000013</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000013</td><td>0.00001</td><td>0.000007</td><td>0.000004</td><td>0.000002</td><td>0.000001</td><td>5.4335e-7</td><td>3.2919e-7</td><td>2.1876e-7</td><td>1.4950e-7</td><td>1.0933e-7</td><td>8.7582e-8</td><td>7.7536e-8</td><td>7.4163e-8</td><td>7.2076e-8</td><td>7.1080e-8</td><td>7.0781e-8</td><td>7.0635e-8</td><td>7.0695e-8</td><td>7.0802e-8</td><td>7.0773e-8</td><td>7.0642e-8</td><td>7.0367e-8</td><td>6.9130e-8</td><td>6.7862e-8</td><td>6.6464e-8</td><td>6.4919e-8</td><td>6.3383e-8</td><td>6.1233e-8</td><td>5.8041e-8</td><td>5.4994e-8</td><td>5.2104e-8</td><td>4.8473e-8</td><td>4.4674e-8</td><td>4.1062e-8</td><td>3.7635e-8</td><td>3.5023e-8</td><td>3.3181e-8</td><td>3.1460e-8</td><td>2.9850e-8</td><td>2.8331e-8</td><td>2.7395e-8</td><td>2.6727e-8</td><td>2.6068e-8</td><td>2.5409e-8</td><td>2.4823e-8</td><td>2.4685e-8</td><td>2.4545e-8</td><td>2.4471e-8</td><td>1.7700e-7</td><td>2.0328e-7</td><td>2.3320e-7</td><td>2.6696e-7</td><td>3.0454e-7</td><td>3.4568e-7</td><td>3.8980e-7</td><td>4.3609e-7</td><td>4.8367e-7</td><td>5.3178e-7</td><td>5.7992e-7</td><td>6.2782e-7</td><td>6.7540e-7</td><td>7.2251e-7</td><td>7.6892e-7</td><td>8.1424e-7</td><td>8.5804e-7</td><td>9.0003e-7</td><td>9.4009e-7</td><td>9.7831e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.1350e-8</td><td>3.9069e-8</td><td>4.8601e-8</td><td>6.0254e-8</td><td>7.4293e-8</td><td>9.0874e-8</td><td>1.1000e-7</td><td>1.3148e-7</td><td>1.5501e-7</td><td>1.8024e-7</td><td>2.0686e-7</td><td>2.3469e-7</td><td>2.6359e-7</td><td>2.9342e-7</td><td>3.2395e-7</td><td>3.5483e-7</td><td>3.8566e-7</td><td>4.1609e-7</td><td>4.4593e-7</td><td>4.7509e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_403572&quot;</td><td>212.048636</td><td>226.003864</td><td>230.980609</td><td>241.056187</td><td>252.829634</td><td>262.18577</td><td>261.900745</td><td>253.187196</td><td>241.186311</td><td>233.427801</td><td>227.551602</td><td>222.870357</td><td>218.355564</td><td>214.293878</td><td>208.696332</td><td>201.119579</td><td>194.082373</td><td>191.574244</td><td>191.439634</td><td>193.614325</td><td>198.680126</td><td>204.361576</td><td>210.181162</td><td>215.804561</td><td>221.317205</td><td>226.572773</td><td>231.736907</td><td>236.627809</td><td>241.202458</td><td>245.491838</td><td>249.651329</td><td>253.511502</td><td>257.175998</td><td>260.61964</td><td>263.84016</td><td>266.715386</td><td>269.360562</td><td>271.653413</td><td>274.017475</td><td>276.072557</td><td>278.181629</td><td>279.566684</td><td>281.092302</td><td>282.396228</td><td>284.098606</td><td>285.416977</td><td>286.76609</td><td>287.861359</td><td>288.827942</td><td>289.783145</td><td>290.64174</td><td>291.331085</td><td>292.200819</td><td>293.041512</td><td>293.902807</td><td>294.538675</td><td>295.251986</td><td>296.308722</td><td>297.493538</td><td>298.723115</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000011</td><td>0.000022</td><td>0.000042</td><td>0.000061</td><td>0.000085</td><td>0.00011</td><td>0.00017</td><td>0.000244</td><td>0.000322</td><td>0.000395</td><td>0.000491</td><td>0.000627</td><td>0.000833</td><td>0.001111</td><td>0.00155</td><td>0.002122</td><td>0.00287</td><td>0.003517</td><td>0.004173</td><td>0.004784</td><td>0.005847</td><td>0.007088</td><td>0.008175</td><td>0.008792</td><td>0.009477</td><td>0.009974</td><td>0.010389</td><td>0.010789</td><td>0.011092</td><td>0.011604</td><td>0.012546</td><td>0.013101</td><td>0.013655</td><td>0.014322</td><td>0.01524</td><td>0.016034</td><td>0.016453</td><td>0.016592</td><td>0.016683</td><td>9.7321e-37</td><td>9.6702e-37</td><td>9.4260e-37</td><td>8.5773e-37</td><td>7.7302e-37</td><td>7.7039e-37</td><td>6.7762e-37</td><td>4.5470e-37</td><td>2.4676e-37</td><td>5.1243e-38</td><td>2.5684e-38</td><td>6.4836e-39</td><td>8.9827e-43</td><td>1.0076e-47</td><td>5.2435e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.6641e-53</td><td>2.2396e-48</td><td>6.9463e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.0829e-26</td><td>6.2168e-21</td><td>1.6750e-16</td><td>4.5542e-12</td><td>1.0455e-9</td><td>1.4923e-8</td><td>7.4443e-8</td><td>7.5741e-7</td><td>3.2564e-7</td><td>9.1671e-7</td><td>0.000005</td><td>0.000003</td><td>0.000007</td><td>8.5192e-7</td><td>0.000008</td><td>0.000007</td><td>0.000073</td><td>0.000054</td><td>0.000011</td><td>0.000016</td><td>0.00002</td><td>0.000012</td><td>0.000005</td><td>0.000004</td><td>0.000001</td><td>0.000021</td><td>0.000026</td><td>0.000023</td><td>0.000017</td><td>0.000011</td><td>0.000003</td><td>7.8136e-7</td><td>2.0159e-7</td><td>1.8606e-8</td><td>3.1498e-11</td><td>3.4882e-11</td><td>3.9500e-11</td><td>4.7076e-11</td><td>4.9022e-11</td><td>5.1808e-11</td><td>5.1092e-11</td><td>4.1047e-11</td><td>2.8809e-11</td><td>1.0113e-11</td><td>4.0288e-12</td><td>1.0157e-12</td><td>7.9655e-17</td><td>1.2778e-21</td><td>1.6981e-24</td><td>0.0</td><td>2.6637e-14</td><td>8.6123e-12</td><td>1.6406e-11</td><td>7.3944e-9</td><td>5.2734e-8</td><td>1.7848e-7</td><td>1.6908e-7</td><td>0.000003</td><td>0.000005</td><td>0.000003</td><td>8.3423e-7</td><td>0.000002</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>2.1885e-7</td><td>6.5565e-8</td><td>0.000001</td><td>3.9603e-7</td><td>2.4611e-7</td><td>6.2521e-7</td><td>1.5304e-7</td><td>4.4849e-8</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.1317e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.895688</td><td>-29.097592</td><td>-34.203822</td><td>-26.292529</td><td>-32.092229</td><td>-37.212649</td><td>-40.736995</td><td>-38.776084</td><td>-34.378608</td><td>-29.958867</td><td>-23.995797</td><td>-16.313667</td><td>-9.717851</td><td>-5.701623</td><td>0.282212</td><td>6.710835</td><td>11.787517</td><td>17.137322</td><td>21.384466</td><td>26.894039</td><td>31.373947</td><td>32.788974</td><td>32.52744</td><td>30.910859</td><td>27.796946</td><td>24.692624</td><td>22.061817</td><td>19.825191</td><td>17.776414</td><td>15.818793</td><td>13.885651</td><td>11.877284</td><td>9.919114</td><td>8.120361</td><td>6.55236</td><td>5.270232</td><td>4.265699</td><td>3.445589</td><td>2.70188</td><td>2.010272</td><td>1.353232</td><td>0.750537</td><td>0.250007</td><td>-0.209561</td><td>-0.618921</td><td>-1.001443</td><td>-1.411117</td><td>-1.795138</td><td>-2.144735</td><td>-2.465519</td><td>-2.706805</td><td>-2.893646</td><td>-3.064147</td><td>-3.239915</td><td>-3.402748</td><td>-3.687885</td><td>-3.894445</td><td>-3.934927</td><td>-3.892879</td><td>-3.790526</td><td>13.161184</td><td>4.86829</td><td>15.49955</td><td>4.304659</td><td>-4.137783</td><td>-1.489619</td><td>4.318581</td><td>5.38695</td><td>1.536853</td><td>-0.516866</td><td>1.375316</td><td>2.55648</td><td>0.584175</td><td>0.286661</td><td>3.324075</td><td>3.994714</td><td>4.213756</td><td>4.146825</td><td>4.086154</td><td>4.275549</td><td>4.685594</td><td>5.83817</td><td>6.443513</td><td>6.691064</td><td>6.057019</td><td>4.967796</td><td>4.069745</td><td>3.403313</td><td>3.053714</td><td>2.873647</td><td>2.858752</td><td>2.763781</td><td>2.548221</td><td>2.133287</td><td>1.660112</td><td>1.154426</td><td>0.808214</td><td>0.620016</td><td>0.561383</td><td>0.517667</td><td>0.425216</td><td>0.257751</td><td>0.012808</td><td>-0.239181</td><td>-0.408705</td><td>-0.510568</td><td>-0.610817</td><td>-0.685114</td><td>-0.777895</td><td>-0.891026</td><td>-0.998847</td><td>-1.148757</td><td>-1.303713</td><td>-1.469805</td><td>-1.610964</td><td>-1.783924</td><td>-1.781503</td><td>-1.651063</td><td>-1.54045</td><td>-1.441145</td><td>101096.425413</td><td>0.0</td><td>80.041768</td><td>5.595857</td><td>0.020411</td><td>0.006781</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>460.810594</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.7210e-7</td><td>4.9007e-7</td><td>8.7841e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000014</td><td>0.000015</td><td>0.000014</td><td>0.000013</td><td>0.00001</td><td>0.000007</td><td>0.000004</td><td>0.000002</td><td>0.000001</td><td>5.7756e-7</td><td>3.5455e-7</td><td>2.5498e-7</td><td>1.8384e-7</td><td>1.3208e-7</td><td>9.8328e-8</td><td>8.0853e-8</td><td>7.5393e-8</td><td>7.2067e-8</td><td>7.0594e-8</td><td>7.0437e-8</td><td>7.0536e-8</td><td>7.1091e-8</td><td>7.1728e-8</td><td>7.2022e-8</td><td>7.2139e-8</td><td>7.1999e-8</td><td>7.1021e-8</td><td>7.0019e-8</td><td>6.8628e-8</td><td>6.6962e-8</td><td>6.5306e-8</td><td>6.2554e-8</td><td>5.9008e-8</td><td>5.5624e-8</td><td>5.2410e-8</td><td>4.7203e-8</td><td>4.2258e-8</td><td>3.7556e-8</td><td>3.3094e-8</td><td>3.1591e-8</td><td>3.0384e-8</td><td>2.9257e-8</td><td>2.8202e-8</td><td>2.7346e-8</td><td>2.6792e-8</td><td>2.6253e-8</td><td>2.5721e-8</td><td>2.5189e-8</td><td>2.4827e-8</td><td>2.4477e-8</td><td>2.4131e-8</td><td>2.4187e-8</td><td>1.7527e-7</td><td>2.0129e-7</td><td>2.3092e-7</td><td>2.6434e-7</td><td>3.0156e-7</td><td>3.4230e-7</td><td>3.8598e-7</td><td>4.3182e-7</td><td>4.7893e-7</td><td>5.2658e-7</td><td>5.7424e-7</td><td>6.2168e-7</td><td>6.6878e-7</td><td>7.1544e-7</td><td>7.6140e-7</td><td>8.0627e-7</td><td>8.4964e-7</td><td>8.9122e-7</td><td>9.3089e-7</td><td>9.6873e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.0161e-8</td><td>3.7656e-8</td><td>4.6929e-8</td><td>5.8286e-8</td><td>7.1990e-8</td><td>8.8206e-8</td><td>1.0694e-7</td><td>1.2801e-7</td><td>1.5113e-7</td><td>1.7595e-7</td><td>2.0217e-7</td><td>2.2960e-7</td><td>2.5812e-7</td><td>2.8760e-7</td><td>3.1778e-7</td><td>3.4833e-7</td><td>3.7886e-7</td><td>4.0902e-7</td><td>4.3860e-7</td><td>4.6753e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_484578&quot;</td><td>207.926097</td><td>216.697348</td><td>227.82159</td><td>243.555011</td><td>257.498525</td><td>265.294257</td><td>262.886892</td><td>253.702132</td><td>243.453695</td><td>236.891841</td><td>231.651554</td><td>227.565292</td><td>224.415847</td><td>221.184967</td><td>216.809066</td><td>211.962639</td><td>207.474773</td><td>204.363333</td><td>200.804282</td><td>202.558145</td><td>202.750097</td><td>205.465023</td><td>208.13513</td><td>211.352072</td><td>214.992253</td><td>218.816602</td><td>222.907968</td><td>226.8796</td><td>230.926906</td><td>235.015689</td><td>238.957999</td><td>242.838951</td><td>246.552663</td><td>250.100233</td><td>253.434653</td><td>256.558746</td><td>259.531588</td><td>262.387539</td><td>265.074654</td><td>267.555766</td><td>269.743913</td><td>271.38381</td><td>273.096603</td><td>275.01915</td><td>276.454907</td><td>277.961399</td><td>278.975628</td><td>280.096285</td><td>281.158367</td><td>282.398872</td><td>283.620941</td><td>284.62805</td><td>285.485231</td><td>286.267462</td><td>287.136152</td><td>287.896636</td><td>288.753091</td><td>289.687569</td><td>290.58775</td><td>291.274499</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000004</td><td>0.000007</td><td>0.00001</td><td>0.000014</td><td>0.000018</td><td>0.000022</td><td>0.00003</td><td>0.000043</td><td>0.000069</td><td>0.000117</td><td>0.000182</td><td>0.000265</td><td>0.000374</td><td>0.000515</td><td>0.000718</td><td>0.00096</td><td>0.001245</td><td>0.001563</td><td>0.001948</td><td>0.002365</td><td>0.002793</td><td>0.003236</td><td>0.003783</td><td>0.004526</td><td>0.005084</td><td>0.005418</td><td>0.005933</td><td>0.006233</td><td>0.006591</td><td>0.007064</td><td>0.007459</td><td>0.007669</td><td>0.008008</td><td>0.008362</td><td>0.008796</td><td>0.00921</td><td>0.009581</td><td>0.010069</td><td>0.010581</td><td>0.011049</td><td>0.011345</td><td>0.01184</td><td>9.7128e-37</td><td>9.6051e-37</td><td>9.5452e-37</td><td>9.5362e-37</td><td>9.6512e-37</td><td>9.5605e-37</td><td>8.6557e-37</td><td>7.1741e-37</td><td>6.0128e-37</td><td>4.8712e-37</td><td>1.8578e-37</td><td>2.0251e-38</td><td>1.0646e-42</td><td>1.2382e-49</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.7811e-40</td><td>3.3669e-33</td><td>3.3602e-29</td><td>1.5452e-23</td><td>5.2629e-18</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.4444e-8</td><td>3.7406e-8</td><td>0.0</td><td>4.2136e-7</td><td>0.000003</td><td>0.000003</td><td>0.000004</td><td>0.000022</td><td>0.000083</td><td>0.000024</td><td>0.000024</td><td>0.000023</td><td>0.000019</td><td>0.000039</td><td>0.00005</td><td>0.000019</td><td>0.000011</td><td>0.000012</td><td>0.00001</td><td>0.000008</td><td>0.000013</td><td>0.000013</td><td>0.000008</td><td>0.000002</td><td>7.8231e-7</td><td>2.7059e-7</td><td>3.2707e-11</td><td>3.4833e-11</td><td>4.0651e-11</td><td>4.4067e-11</td><td>4.8119e-11</td><td>5.4224e-11</td><td>5.7644e-11</td><td>5.6434e-11</td><td>5.2800e-11</td><td>4.8358e-11</td><td>2.4067e-11</td><td>3.2542e-12</td><td>1.6799e-16</td><td>1.1093e-23</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.9935e-13</td><td>1.2625e-11</td><td>4.1767e-9</td><td>5.1563e-8</td><td>6.2808e-8</td><td>4.8014e-8</td><td>8.5056e-8</td><td>1.5604e-7</td><td>2.0746e-7</td><td>3.8882e-7</td><td>8.5910e-7</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>7.1042e-7</td><td>6.5356e-7</td><td>5.6581e-9</td><td>1.6544e-8</td><td>1.6084e-8</td><td>1.3615e-9</td><td>0.0</td><td>0.0</td><td>1.6286e-11</td><td>1.4384e-16</td><td>6.6560e-12</td><td>3.7114e-10</td><td>5.3211e-10</td><td>1.6815e-10</td><td>-40.514671</td><td>-40.032417</td><td>-31.828419</td><td>-18.524967</td><td>-12.830235</td><td>-14.173486</td><td>-15.047244</td><td>-13.743843</td><td>-10.587021</td><td>-7.645994</td><td>-5.238524</td><td>-1.475789</td><td>3.702735</td><td>8.542001</td><td>11.11544</td><td>15.118564</td><td>20.132157</td><td>23.884062</td><td>26.822964</td><td>29.354526</td><td>31.015959</td><td>31.800139</td><td>32.027682</td><td>32.053957</td><td>31.697876</td><td>30.933278</td><td>29.808729</td><td>28.541658</td><td>27.194826</td><td>25.891485</td><td>24.68164</td><td>23.639124</td><td>22.746605</td><td>21.928873</td><td>21.076565</td><td>20.061099</td><td>18.849762</td><td>17.503729</td><td>16.179417</td><td>15.007115</td><td>14.075261</td><td>13.291599</td><td>12.557146</td><td>11.87178</td><td>11.154768</td><td>10.471049</td><td>9.889161</td><td>9.347019</td><td>8.879216</td><td>8.480535</td><td>8.109997</td><td>7.769493</td><td>7.450909</td><td>7.145693</td><td>6.851057</td><td>6.579524</td><td>6.371834</td><td>6.262167</td><td>5.818672</td><td>3.805306</td><td>-0.848759</td><td>5.809736</td><td>13.565142</td><td>11.622131</td><td>5.192177</td><td>3.832022</td><td>4.521555</td><td>3.157626</td><td>0.36445</td><td>-1.681811</td><td>-1.608787</td><td>-0.285473</td><td>0.659845</td><td>0.252871</td><td>1.124726</td><td>4.257013</td><td>5.224426</td><td>5.710101</td><td>6.679622</td><td>7.853395</td><td>8.394954</td><td>8.425032</td><td>8.2186</td><td>7.864857</td><td>7.452511</td><td>7.143194</td><td>6.886608</td><td>6.479343</td><td>5.897673</td><td>5.157122</td><td>4.328224</td><td>3.496279</td><td>2.786117</td><td>2.240791</td><td>1.878776</td><td>1.655745</td><td>1.49622</td><td>1.296232</td><td>0.978217</td><td>0.525321</td><td>0.013448</td><td>-0.525427</td><td>-1.048021</td><td>-1.479973</td><td>-1.86184</td><td>-2.312476</td><td>-2.749788</td><td>-3.113212</td><td>-3.44963</td><td>-3.714691</td><td>-3.954645</td><td>-4.173306</td><td>-4.35114</td><td>-4.488054</td><td>-4.584904</td><td>-4.650776</td><td>-4.742572</td><td>-4.905684</td><td>-5.619688</td><td>-5.524507</td><td>101283.482359</td><td>0.0</td><td>11.273749</td><td>-2.447222</td><td>-0.020809</td><td>0.030842</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>407.406121</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6040e-7</td><td>4.6899e-7</td><td>8.4061e-7</td><td>0.000001</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000012</td><td>0.000013</td><td>0.000013</td><td>0.000013</td><td>0.000011</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>6.6758e-7</td><td>4.6964e-7</td><td>3.5960e-7</td><td>2.7383e-7</td><td>2.0008e-7</td><td>1.4038e-7</td><td>1.1380e-7</td><td>9.5438e-8</td><td>8.4788e-8</td><td>7.9227e-8</td><td>7.4739e-8</td><td>7.2256e-8</td><td>7.0076e-8</td><td>6.8281e-8</td><td>6.6685e-8</td><td>6.5100e-8</td><td>6.3685e-8</td><td>6.2235e-8</td><td>6.0612e-8</td><td>5.8859e-8</td><td>5.7117e-8</td><td>5.5244e-8</td><td>5.3288e-8</td><td>5.1422e-8</td><td>4.9651e-8</td><td>4.7129e-8</td><td>4.4702e-8</td><td>4.2394e-8</td><td>4.0204e-8</td><td>3.8262e-8</td><td>3.6457e-8</td><td>3.4770e-8</td><td>3.3193e-8</td><td>3.1927e-8</td><td>3.1338e-8</td><td>3.0764e-8</td><td>3.0197e-8</td><td>2.9631e-8</td><td>2.9297e-8</td><td>2.9010e-8</td><td>2.8719e-8</td><td>2.8492e-8</td><td>1.6681e-7</td><td>1.9158e-7</td><td>2.1978e-7</td><td>2.5159e-7</td><td>2.8701e-7</td><td>3.2578e-7</td><td>3.6736e-7</td><td>4.1098e-7</td><td>4.5582e-7</td><td>5.0116e-7</td><td>5.4652e-7</td><td>5.9167e-7</td><td>6.3651e-7</td><td>6.8091e-7</td><td>7.2465e-7</td><td>7.6735e-7</td><td>8.0863e-7</td><td>8.4820e-7</td><td>8.8596e-7</td><td>9.2198e-7</td><td>9.5638e-7</td><td>9.8928e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.4879e-8</td><td>3.1335e-8</td><td>3.9392e-8</td><td>4.9344e-8</td><td>6.1457e-8</td><td>7.5905e-8</td><td>9.2726e-8</td><td>1.1179e-7</td><td>1.3285e-7</td><td>1.5559e-7</td><td>1.7976e-7</td><td>2.0519e-7</td><td>2.3175e-7</td><td>2.5931e-7</td><td>2.8766e-7</td><td>3.1646e-7</td><td>3.4533e-7</td><td>3.7395e-7</td><td>4.0210e-7</td><td>4.2971e-7</td><td>4.5676e-7</td><td>4.8324e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;test_578220&quot;</td><td>216.890815</td><td>228.911969</td><td>236.654694</td><td>248.999301</td><td>256.380631</td><td>262.360629</td><td>257.791873</td><td>247.516488</td><td>237.795226</td><td>229.436973</td><td>223.840848</td><td>218.987149</td><td>214.455152</td><td>210.253412</td><td>203.824493</td><td>199.018916</td><td>190.877131</td><td>187.290404</td><td>189.283493</td><td>192.703953</td><td>198.790497</td><td>204.75373</td><td>211.289415</td><td>216.767319</td><td>222.771484</td><td>229.002498</td><td>234.251775</td><td>238.515072</td><td>242.568077</td><td>246.318855</td><td>249.8327</td><td>253.30477</td><td>256.479256</td><td>259.042021</td><td>261.275334</td><td>263.220048</td><td>265.786259</td><td>268.676433</td><td>271.683228</td><td>274.397627</td><td>276.69573</td><td>278.639437</td><td>279.652864</td><td>280.506063</td><td>281.797042</td><td>282.946524</td><td>284.163682</td><td>285.338408</td><td>286.560372</td><td>287.636299</td><td>288.569201</td><td>289.563614</td><td>290.323918</td><td>291.006692</td><td>291.721293</td><td>292.790759</td><td>293.872266</td><td>295.051167</td><td>296.262226</td><td>297.495878</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000011</td><td>0.000033</td><td>0.000083</td><td>0.000159</td><td>0.000292</td><td>0.000407</td><td>0.000431</td><td>0.000365</td><td>0.000301</td><td>0.000298</td><td>0.00038</td><td>0.000684</td><td>0.001178</td><td>0.001932</td><td>0.002519</td><td>0.002905</td><td>0.003077</td><td>0.003199</td><td>0.00336</td><td>0.003508</td><td>0.004223</td><td>0.005404</td><td>0.006593</td><td>0.00726</td><td>0.008064</td><td>0.008769</td><td>0.008992</td><td>0.009276</td><td>0.009807</td><td>0.010232</td><td>0.010999</td><td>0.011914</td><td>0.013087</td><td>0.013428</td><td>0.013801</td><td>0.013912</td><td>0.013956</td><td>0.01404</td><td>1.0255e-37</td><td>9.6831e-38</td><td>8.5105e-38</td><td>7.4848e-38</td><td>6.6743e-38</td><td>5.5362e-38</td><td>5.2533e-38</td><td>3.8248e-38</td><td>3.3658e-38</td><td>1.9240e-38</td><td>1.7839e-38</td><td>9.5738e-39</td><td>5.7609e-44</td><td>1.2903e-47</td><td>5.4957e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9.9242e-53</td><td>2.2527e-48</td><td>3.0741e-44</td><td>3.2172e-40</td><td>7.4653e-37</td><td>3.0357e-32</td><td>1.3435e-27</td><td>1.9898e-23</td><td>5.8084e-19</td><td>1.2265e-14</td><td>6.1973e-11</td><td>1.8722e-8</td><td>0.0</td><td>0.0</td><td>6.4923e-8</td><td>5.2863e-7</td><td>7.0171e-7</td><td>0.000001</td><td>0.000002</td><td>0.000003</td><td>0.000003</td><td>6.5247e-8</td><td>6.5122e-8</td><td>0.000002</td><td>0.000052</td><td>0.000006</td><td>0.000017</td><td>0.000004</td><td>0.000002</td><td>0.00001</td><td>0.000011</td><td>0.000011</td><td>0.000006</td><td>9.2283e-7</td><td>0.000002</td><td>0.000001</td><td>5.3185e-7</td><td>4.9293e-7</td><td>1.4069e-8</td><td>5.0483e-8</td><td>6.1028e-12</td><td>6.5125e-12</td><td>6.8174e-12</td><td>7.7943e-12</td><td>8.1095e-12</td><td>8.3080e-12</td><td>6.3072e-12</td><td>4.2078e-12</td><td>2.4430e-12</td><td>1.3018e-12</td><td>7.4322e-13</td><td>3.4289e-13</td><td>1.9021e-18</td><td>4.1825e-22</td><td>5.6206e-22</td><td>3.4514e-16</td><td>5.5620e-10</td><td>8.8530e-8</td><td>1.0012e-7</td><td>8.7511e-8</td><td>2.6333e-10</td><td>0.0</td><td>0.0</td><td>3.5540e-7</td><td>0.000002</td><td>0.000004</td><td>0.000002</td><td>0.000003</td><td>0.000002</td><td>0.000002</td><td>8.7206e-7</td><td>6.0768e-9</td><td>2.2670e-9</td><td>2.2869e-7</td><td>3.2742e-7</td><td>6.0206e-7</td><td>1.1166e-7</td><td>1.9000e-7</td><td>1.5679e-7</td><td>2.1275e-7</td><td>1.5721e-7</td><td>1.3016e-7</td><td>6.8575e-8</td><td>7.3267e-8</td><td>3.4838e-8</td><td>6.8420e-10</td><td>3.6097e-9</td><td>2.8057e-9</td><td>9.6009e-9</td><td>2.1185e-11</td><td>0.0</td><td>0.0</td><td>4.3523e-12</td><td>8.7768e-10</td><td>2.2667e-11</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>51.209186</td><td>42.419439</td><td>37.281976</td><td>35.648045</td><td>34.451416</td><td>27.809329</td><td>29.607586</td><td>22.240679</td><td>14.340837</td><td>8.470471</td><td>6.683759</td><td>6.85706</td><td>4.737829</td><td>0.168345</td><td>-1.061041</td><td>2.411994</td><td>7.735285</td><td>15.792098</td><td>26.320675</td><td>34.2857</td><td>40.828029</td><td>46.740835</td><td>48.625057</td><td>48.333362</td><td>48.036965</td><td>47.471239</td><td>46.671519</td><td>43.513668</td><td>39.277116</td><td>33.958462</td><td>28.955049</td><td>24.944975</td><td>22.012148</td><td>20.014659</td><td>18.198438</td><td>16.515235</td><td>15.106666</td><td>13.656904</td><td>12.162052</td><td>10.343934</td><td>8.465571</td><td>6.611452</td><td>5.051228</td><td>3.98619</td><td>3.31151</td><td>2.529246</td><td>1.652807</td><td>0.781019</td><td>-0.040192</td><td>-0.972076</td><td>-1.912147</td><td>-2.710149</td><td>-3.392743</td><td>-4.092902</td><td>-4.537605</td><td>-4.577928</td><td>-4.500485</td><td>-4.448912</td><td>-4.368188</td><td>-4.2399</td><td>18.015506</td><td>-22.50621</td><td>-17.31347</td><td>4.918476</td><td>3.765759</td><td>10.192913</td><td>-5.364113</td><td>-4.811059</td><td>-1.97306</td><td>0.720158</td><td>1.511261</td><td>1.277531</td><td>1.426816</td><td>0.514734</td><td>-1.303389</td><td>-1.287716</td><td>-3.50673</td><td>-6.756223</td><td>-9.073841</td><td>-9.815227</td><td>-11.015977</td><td>-5.496088</td><td>5.424886</td><td>8.262681</td><td>6.32565</td><td>2.571159</td><td>-2.623422</td><td>-7.753617</td><td>-11.240746</td><td>-11.856614</td><td>-10.227195</td><td>-6.929758</td><td>-3.456147</td><td>-0.885487</td><td>0.784514</td><td>2.280307</td><td>3.467787</td><td>3.767892</td><td>3.505961</td><td>2.9427</td><td>2.405072</td><td>2.17702</td><td>2.406374</td><td>2.76599</td><td>2.591751</td><td>2.23986</td><td>1.978571</td><td>1.810335</td><td>1.67127</td><td>1.597733</td><td>1.515346</td><td>1.431827</td><td>1.240866</td><td>1.098105</td><td>0.36846</td><td>0.05819</td><td>-0.301357</td><td>-0.440503</td><td>-0.53131</td><td>-0.596656</td><td>100806.042921</td><td>898.336286</td><td>98.588601</td><td>5.033812</td><td>0.023682</td><td>0.003176</td><td>0.638527</td><td>0.06</td><td>0.048138</td><td>0.06</td><td>0.048138</td><td>452.465195</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0087e-7</td><td>5.4188e-7</td><td>9.7127e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000015</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>4.2125e-7</td><td>1.8415e-7</td><td>1.1659e-7</td><td>8.8404e-8</td><td>7.2384e-8</td><td>6.2100e-8</td><td>5.6742e-8</td><td>5.5711e-8</td><td>5.5654e-8</td><td>5.6333e-8</td><td>5.7553e-8</td><td>5.8866e-8</td><td>6.0504e-8</td><td>6.2214e-8</td><td>6.3771e-8</td><td>6.5290e-8</td><td>6.6536e-8</td><td>6.7011e-8</td><td>6.7497e-8</td><td>6.7260e-8</td><td>6.6603e-8</td><td>6.5950e-8</td><td>6.4816e-8</td><td>6.3419e-8</td><td>6.2086e-8</td><td>6.0839e-8</td><td>5.9843e-8</td><td>5.8898e-8</td><td>5.7999e-8</td><td>5.7167e-8</td><td>5.6854e-8</td><td>5.6560e-8</td><td>5.6286e-8</td><td>5.6029e-8</td><td>5.6050e-8</td><td>5.6322e-8</td><td>5.6588e-8</td><td>5.6850e-8</td><td>5.7079e-8</td><td>5.7031e-8</td><td>5.6982e-8</td><td>5.6929e-8</td><td>5.6862e-8</td><td>1.8144e-7</td><td>2.0838e-7</td><td>2.3905e-7</td><td>2.7365e-7</td><td>3.1218e-7</td><td>3.5435e-7</td><td>3.9957e-7</td><td>4.4702e-7</td><td>4.9579e-7</td><td>5.4511e-7</td><td>5.9445e-7</td><td>6.4356e-7</td><td>6.9232e-7</td><td>7.4062e-7</td><td>7.8819e-7</td><td>8.3464e-7</td><td>8.7954e-7</td><td>9.2259e-7</td><td>9.6366e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.4774e-8</td><td>4.3110e-8</td><td>5.3352e-8</td><td>6.5808e-8</td><td>8.0738e-8</td><td>9.8287e-8</td><td>1.1843e-7</td><td>1.4097e-7</td><td>1.6555e-7</td><td>1.9180e-7</td><td>2.1942e-7</td><td>2.4819e-7</td><td>2.7798e-7</td><td>3.0866e-7</td><td>3.3997e-7</td><td>3.7158e-7</td><td>4.0306e-7</td><td>4.3409e-7</td><td>4.6445e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_395695&quot;</td><td>215.417881</td><td>229.137757</td><td>237.973745</td><td>246.218479</td><td>259.189075</td><td>260.83965</td><td>258.333569</td><td>253.231639</td><td>238.788101</td><td>228.37941</td><td>223.069909</td><td>220.787349</td><td>216.499705</td><td>210.730831</td><td>203.953776</td><td>194.718664</td><td>188.666371</td><td>184.449184</td><td>186.865067</td><td>191.698841</td><td>198.851628</td><td>204.828501</td><td>209.950302</td><td>215.456475</td><td>222.533569</td><td>228.906147</td><td>234.612552</td><td>239.638282</td><td>244.021863</td><td>248.023302</td><td>251.402328</td><td>254.368986</td><td>257.646144</td><td>261.002444</td><td>263.9314</td><td>266.801316</td><td>270.141455</td><td>273.535375</td><td>276.593705</td><td>279.327132</td><td>281.5978</td><td>282.528064</td><td>282.734895</td><td>283.418812</td><td>284.320428</td><td>285.386803</td><td>286.568874</td><td>287.784512</td><td>288.781688</td><td>289.612918</td><td>290.449955</td><td>291.255506</td><td>292.038606</td><td>292.838503</td><td>293.709285</td><td>294.597545</td><td>295.368287</td><td>296.335054</td><td>297.440068</td><td>298.650933</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>8.4727e-7</td><td>8.2581e-7</td><td>8.5970e-7</td><td>0.000002</td><td>0.000004</td><td>0.000005</td><td>0.000011</td><td>0.000041</td><td>0.000088</td><td>0.000153</td><td>0.000277</td><td>0.000499</td><td>0.000745</td><td>0.000997</td><td>0.001312</td><td>0.001584</td><td>0.001726</td><td>0.001856</td><td>0.00193</td><td>0.00191</td><td>0.001777</td><td>0.001588</td><td>0.001456</td><td>0.001428</td><td>0.001544</td><td>0.002432</td><td>0.005559</td><td>0.007683</td><td>0.009252</td><td>0.010312</td><td>0.011117</td><td>0.011551</td><td>0.011978</td><td>0.012544</td><td>0.013048</td><td>0.013556</td><td>0.014097</td><td>0.014605</td><td>0.015051</td><td>0.015457</td><td>0.016214</td><td>0.016736</td><td>0.017014</td><td>0.017099</td><td>1.0195e-37</td><td>9.5385e-38</td><td>8.4813e-38</td><td>7.8176e-38</td><td>6.7270e-38</td><td>5.3512e-38</td><td>4.2916e-38</td><td>3.2226e-38</td><td>1.7679e-38</td><td>4.7905e-39</td><td>1.2387e-39</td><td>5.4993e-40</td><td>9.5386e-45</td><td>1.8132e-48</td><td>4.0479e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.2306e-50</td><td>2.4816e-45</td><td>2.7617e-39</td><td>1.6750e-35</td><td>4.6384e-31</td><td>1.5931e-26</td><td>3.3353e-22</td><td>6.6828e-18</td><td>6.7236e-14</td><td>3.6194e-10</td><td>2.8905e-8</td><td>5.0545e-8</td><td>2.7186e-8</td><td>1.3351e-8</td><td>3.3022e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.9953e-7</td><td>0.00002</td><td>0.000009</td><td>0.000016</td><td>0.000008</td><td>0.00002</td><td>0.000013</td><td>0.00001</td><td>0.000008</td><td>0.000011</td><td>0.000008</td><td>0.000011</td><td>0.000017</td><td>0.000014</td><td>0.000006</td><td>0.000002</td><td>5.9203e-8</td><td>2.8390e-9</td><td>8.3071e-9</td><td>6.1567e-12</td><td>6.5463e-12</td><td>7.6386e-12</td><td>9.8801e-12</td><td>8.5007e-12</td><td>9.6855e-12</td><td>7.4884e-12</td><td>6.0782e-12</td><td>3.9052e-12</td><td>1.9818e-12</td><td>9.4869e-13</td><td>1.3513e-13</td><td>6.3659e-19</td><td>1.0206e-22</td><td>1.6182e-27</td><td>1.9016e-17</td><td>1.1009e-11</td><td>2.2732e-9</td><td>7.3264e-9</td><td>1.3369e-8</td><td>6.2237e-10</td><td>1.2517e-9</td><td>3.5348e-8</td><td>0.000007</td><td>0.000016</td><td>0.000018</td><td>0.000017</td><td>0.000019</td><td>0.00002</td><td>0.000016</td><td>0.000004</td><td>0.000002</td><td>6.0101e-7</td><td>8.9458e-8</td><td>1.5849e-8</td><td>4.6950e-9</td><td>2.5531e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.1408e-12</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.4422e-14</td><td>1.3707e-13</td><td>2.2438e-13</td><td>0.0</td><td>20.524008</td><td>11.802658</td><td>-1.470289</td><td>-11.650991</td><td>0.527938</td><td>-13.068308</td><td>-0.530987</td><td>-6.100695</td><td>-11.254077</td><td>-7.793831</td><td>-3.578696</td><td>2.155702</td><td>-0.065155</td><td>-3.212529</td><td>-5.278644</td><td>-2.017981</td><td>9.459058</td><td>16.106525</td><td>20.314203</td><td>24.469862</td><td>28.051703</td><td>29.734808</td><td>29.425833</td><td>30.037842</td><td>30.483579</td><td>30.392413</td><td>29.19196</td><td>26.62855</td><td>24.036171</td><td>21.964985</td><td>20.16926</td><td>18.769084</td><td>17.378007</td><td>15.586056</td><td>12.791198</td><td>9.759307</td><td>7.330399</td><td>5.099918</td><td>2.462769</td><td>-0.475358</td><td>-3.580935</td><td>-6.801777</td><td>-9.821074</td><td>-11.137475</td><td>-11.234591</td><td>-10.841384</td><td>-9.942276</td><td>-8.949552</td><td>-8.185709</td><td>-7.528274</td><td>-6.958012</td><td>-6.406455</td><td>-5.896166</td><td>-5.402691</td><td>-4.908382</td><td>-4.420759</td><td>-3.736797</td><td>-3.195257</td><td>-2.809322</td><td>-2.64099</td><td>-1.339814</td><td>-16.644096</td><td>-2.232021</td><td>11.885605</td><td>1.024442</td><td>0.578848</td><td>-4.366504</td><td>-0.900303</td><td>0.291313</td><td>-0.477247</td><td>1.80855</td><td>0.961809</td><td>0.369959</td><td>-0.326075</td><td>-1.623963</td><td>-0.154821</td><td>3.037685</td><td>-1.834551</td><td>-11.397085</td><td>-17.255997</td><td>-20.447333</td><td>-16.964167</td><td>-11.801245</td><td>-11.611755</td><td>-13.262797</td><td>-14.399235</td><td>-15.262945</td><td>-15.220297</td><td>-14.028617</td><td>-11.76185</td><td>-9.361573</td><td>-6.713737</td><td>-4.097246</td><td>-2.069601</td><td>-0.301056</td><td>1.186739</td><td>1.840707</td><td>1.872513</td><td>1.744303</td><td>1.626873</td><td>1.452098</td><td>1.2803</td><td>1.2355</td><td>1.009737</td><td>0.651134</td><td>0.234168</td><td>-0.27645</td><td>-0.767151</td><td>-1.052228</td><td>-1.148744</td><td>-1.168753</td><td>-1.08572</td><td>-0.953024</td><td>-0.782604</td><td>-0.56262</td><td>-0.399534</td><td>-0.24004</td><td>-0.113426</td><td>-0.029615</td><td>0.005354</td><td>100671.583235</td><td>854.94506</td><td>49.095183</td><td>2.26653</td><td>0.011482</td><td>-0.000041</td><td>0.607685</td><td>0.06</td><td>0.054207</td><td>0.06</td><td>0.054207</td><td>457.907636</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0920e-7</td><td>5.5688e-7</td><td>9.9815e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000016</td><td>0.000017</td><td>0.000016</td><td>0.000013</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>7.3905e-7</td><td>2.5238e-7</td><td>1.2105e-7</td><td>8.6844e-8</td><td>7.2369e-8</td><td>6.3178e-8</td><td>5.6360e-8</td><td>5.1582e-8</td><td>4.9421e-8</td><td>4.8068e-8</td><td>4.7286e-8</td><td>4.7727e-8</td><td>4.8320e-8</td><td>4.9717e-8</td><td>5.1208e-8</td><td>5.2622e-8</td><td>5.4034e-8</td><td>5.5389e-8</td><td>5.6554e-8</td><td>5.7748e-8</td><td>5.8021e-8</td><td>5.7819e-8</td><td>5.7618e-8</td><td>5.6221e-8</td><td>5.4234e-8</td><td>5.2337e-8</td><td>5.0674e-8</td><td>5.0012e-8</td><td>4.9384e-8</td><td>4.8787e-8</td><td>4.8249e-8</td><td>4.7991e-8</td><td>4.7749e-8</td><td>4.7523e-8</td><td>4.7311e-8</td><td>4.7032e-8</td><td>4.6711e-8</td><td>4.6398e-8</td><td>4.6090e-8</td><td>4.5797e-8</td><td>4.5562e-8</td><td>4.5324e-8</td><td>4.5140e-8</td><td>4.5077e-8</td><td>1.8501e-7</td><td>2.1249e-7</td><td>2.4376e-7</td><td>2.7904e-7</td><td>3.1833e-7</td><td>3.6133e-7</td><td>4.0745e-7</td><td>4.5583e-7</td><td>5.0557e-7</td><td>5.5586e-7</td><td>6.0617e-7</td><td>6.5625e-7</td><td>7.0597e-7</td><td>7.5522e-7</td><td>8.0373e-7</td><td>8.5110e-7</td><td>8.9688e-7</td><td>9.4077e-7</td><td>9.8265e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.8830e-8</td><td>4.7824e-8</td><td>5.8800e-8</td><td>7.2065e-8</td><td>8.7863e-8</td><td>1.0632e-7</td><td>1.2738e-7</td><td>1.5081e-7</td><td>1.7625e-7</td><td>2.0328e-7</td><td>2.3159e-7</td><td>2.6097e-7</td><td>2.9129e-7</td><td>3.2240e-7</td><td>3.5406e-7</td><td>3.8592e-7</td><td>4.1759e-7</td><td>4.4871e-7</td><td>4.7911e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_88942&quot;</td><td>215.135772</td><td>232.704767</td><td>240.141912</td><td>245.662113</td><td>259.146969</td><td>262.647895</td><td>257.051904</td><td>250.79876</td><td>238.528778</td><td>229.844499</td><td>224.788552</td><td>220.429468</td><td>214.781601</td><td>209.430195</td><td>203.063896</td><td>195.09745</td><td>190.148431</td><td>186.0906</td><td>188.002738</td><td>191.541135</td><td>197.895704</td><td>203.693431</td><td>209.940932</td><td>216.052365</td><td>222.503102</td><td>228.646343</td><td>234.139038</td><td>238.988475</td><td>243.535863</td><td>247.31566</td><td>250.698097</td><td>253.62759</td><td>256.733883</td><td>260.466638</td><td>263.921188</td><td>267.407461</td><td>270.662386</td><td>273.734718</td><td>276.561968</td><td>278.754379</td><td>280.380761</td><td>281.712617</td><td>282.329215</td><td>283.219744</td><td>284.410044</td><td>285.801689</td><td>286.78303</td><td>287.813578</td><td>288.812922</td><td>289.713165</td><td>290.613272</td><td>291.315381</td><td>291.848483</td><td>292.611633</td><td>293.394563</td><td>294.262868</td><td>295.214247</td><td>296.28421</td><td>297.486999</td><td>298.731429</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>8.8678e-7</td><td>7.9174e-7</td><td>8.0687e-7</td><td>0.000002</td><td>0.000005</td><td>0.000008</td><td>0.000019</td><td>0.000048</td><td>0.000098</td><td>0.000179</td><td>0.000296</td><td>0.000466</td><td>0.000669</td><td>0.000963</td><td>0.001298</td><td>0.001602</td><td>0.001745</td><td>0.001747</td><td>0.001622</td><td>0.001438</td><td>0.001329</td><td>0.001324</td><td>0.001442</td><td>0.002077</td><td>0.003508</td><td>0.00505</td><td>0.007334</td><td>0.00906</td><td>0.010082</td><td>0.010834</td><td>0.011441</td><td>0.011846</td><td>0.012397</td><td>0.012911</td><td>0.013325</td><td>0.013766</td><td>0.014504</td><td>0.015081</td><td>0.015599</td><td>0.016094</td><td>0.016576</td><td>0.016894</td><td>0.017009</td><td>0.01708</td><td>1.0149e-37</td><td>9.5160e-38</td><td>8.6333e-38</td><td>7.6879e-38</td><td>6.6545e-38</td><td>5.3905e-38</td><td>4.4734e-38</td><td>3.1063e-38</td><td>1.7183e-38</td><td>5.1117e-39</td><td>1.2482e-39</td><td>5.1465e-40</td><td>8.1227e-44</td><td>9.3437e-49</td><td>1.5528e-55</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.8628e-57</td><td>7.6737e-54</td><td>7.1301e-49</td><td>2.3488e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.1595e-18</td><td>0.0</td><td>0.0</td><td>1.7669e-8</td><td>1.4851e-7</td><td>1.6483e-7</td><td>3.9592e-8</td><td>1.6248e-9</td><td>0.0</td><td>8.2662e-8</td><td>0.000002</td><td>5.0830e-7</td><td>0.000003</td><td>0.000003</td><td>0.000004</td><td>0.000018</td><td>0.000003</td><td>0.000004</td><td>0.000003</td><td>0.000013</td><td>0.000008</td><td>0.00001</td><td>0.000012</td><td>0.000037</td><td>0.000048</td><td>0.000016</td><td>0.00001</td><td>5.5911e-7</td><td>1.4527e-7</td><td>5.1456e-8</td><td>7.2670e-8</td><td>6.2296e-12</td><td>6.5300e-12</td><td>7.9199e-12</td><td>9.4948e-12</td><td>8.6921e-12</td><td>1.0068e-11</td><td>8.7134e-12</td><td>5.9803e-12</td><td>2.7788e-12</td><td>1.5869e-12</td><td>8.2614e-13</td><td>1.4209e-13</td><td>2.1172e-17</td><td>2.2504e-22</td><td>1.8651e-22</td><td>1.9827e-17</td><td>8.4050e-12</td><td>1.5358e-9</td><td>6.6995e-10</td><td>1.1585e-8</td><td>6.7242e-10</td><td>1.3722e-8</td><td>6.3314e-8</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000004</td><td>0.000005</td><td>0.000006</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>7.2095e-7</td><td>3.2791e-7</td><td>5.2340e-8</td><td>1.9285e-9</td><td>0.0</td><td>2.3510e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.6102e-12</td><td>1.1740e-13</td><td>3.6012e-14</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9.8479e-15</td><td>18.722166</td><td>17.760093</td><td>-1.939848</td><td>-1.779939</td><td>2.664798</td><td>-14.812423</td><td>-7.129902</td><td>-6.119783</td><td>-8.868477</td><td>-3.867754</td><td>-3.596304</td><td>-2.56724</td><td>-3.202521</td><td>-2.468386</td><td>-3.474392</td><td>-1.260273</td><td>9.92765</td><td>14.317554</td><td>14.355944</td><td>17.531598</td><td>23.833913</td><td>28.676099</td><td>30.479286</td><td>31.606482</td><td>31.52993</td><td>29.526812</td><td>26.226882</td><td>22.55532</td><td>20.001379</td><td>18.341267</td><td>17.300985</td><td>16.404362</td><td>15.029361</td><td>13.308423</td><td>11.297323</td><td>9.316709</td><td>6.963697</td><td>4.25882</td><td>0.671901</td><td>-3.633249</td><td>-7.402048</td><td>-9.95146</td><td>-10.981887</td><td>-10.959769</td><td>-10.450645</td><td>-9.384851</td><td>-8.314138</td><td>-7.342307</td><td>-6.588866</td><td>-5.945333</td><td>-5.321458</td><td>-4.750257</td><td>-4.135325</td><td>-3.44858</td><td>-2.792322</td><td>-2.184244</td><td>-1.566916</td><td>-1.170932</td><td>-1.053412</td><td>-0.982516</td><td>-10.326599</td><td>-6.535773</td><td>13.815284</td><td>13.229978</td><td>-0.639377</td><td>-0.859063</td><td>-2.666459</td><td>1.556782</td><td>1.795009</td><td>-1.969189</td><td>-1.053233</td><td>-0.142041</td><td>0.655943</td><td>0.261201</td><td>0.367795</td><td>-1.940656</td><td>3.079126</td><td>6.783854</td><td>-1.096384</td><td>-14.557691</td><td>-22.156552</td><td>-24.851334</td><td>-23.142287</td><td>-21.890604</td><td>-21.159727</td><td>-19.664989</td><td>-16.874067</td><td>-12.766769</td><td>-8.934568</td><td>-5.541738</td><td>-2.916226</td><td>-0.26189</td><td>2.370863</td><td>3.873087</td><td>4.10843</td><td>3.933822</td><td>3.493504</td><td>3.175473</td><td>2.937182</td><td>2.648715</td><td>2.409712</td><td>1.985027</td><td>1.620282</td><td>1.209234</td><td>0.86002</td><td>0.475584</td><td>0.007463</td><td>-0.52146</td><td>-1.020854</td><td>-1.433941</td><td>-1.737137</td><td>-1.9217</td><td>-1.984218</td><td>-1.916981</td><td>-1.68363</td><td>-1.347555</td><td>-1.017434</td><td>-0.675494</td><td>-0.55038</td><td>-0.47987</td><td>100274.203432</td><td>654.966094</td><td>34.293045</td><td>1.692516</td><td>0.003738</td><td>0.002162</td><td>0.465542</td><td>0.06</td><td>0.087262</td><td>0.06</td><td>0.087262</td><td>458.870692</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0892e-7</td><td>5.5639e-7</td><td>9.9727e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000016</td><td>0.000017</td><td>0.000016</td><td>0.000013</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>7.1570e-7</td><td>2.3401e-7</td><td>1.2068e-7</td><td>8.8657e-8</td><td>7.5348e-8</td><td>6.7453e-8</td><td>6.1790e-8</td><td>5.7566e-8</td><td>5.5494e-8</td><td>5.4143e-8</td><td>5.3332e-8</td><td>5.3835e-8</td><td>5.4537e-8</td><td>5.5551e-8</td><td>5.6623e-8</td><td>5.7807e-8</td><td>5.9063e-8</td><td>6.0487e-8</td><td>6.2320e-8</td><td>6.4197e-8</td><td>6.4768e-8</td><td>6.4460e-8</td><td>6.4153e-8</td><td>6.2693e-8</td><td>6.0425e-8</td><td>5.8260e-8</td><td>5.6232e-8</td><td>5.5930e-8</td><td>5.5642e-8</td><td>5.5369e-8</td><td>5.5110e-8</td><td>5.5123e-8</td><td>5.5151e-8</td><td>5.5177e-8</td><td>5.5202e-8</td><td>5.5037e-8</td><td>5.4532e-8</td><td>5.4040e-8</td><td>5.3554e-8</td><td>5.3068e-8</td><td>5.2618e-8</td><td>5.2167e-8</td><td>5.1717e-8</td><td>5.1601e-8</td><td>1.8497e-7</td><td>2.1244e-7</td><td>2.4371e-7</td><td>2.7898e-7</td><td>3.1826e-7</td><td>3.6125e-7</td><td>4.0735e-7</td><td>4.5573e-7</td><td>5.0545e-7</td><td>5.5573e-7</td><td>6.0603e-7</td><td>6.5610e-7</td><td>7.0581e-7</td><td>7.5505e-7</td><td>8.0355e-7</td><td>8.5090e-7</td><td>8.9668e-7</td><td>9.4056e-7</td><td>9.8243e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.8754e-8</td><td>4.7736e-8</td><td>5.8700e-8</td><td>7.1951e-8</td><td>8.7736e-8</td><td>1.0618e-7</td><td>1.2723e-7</td><td>1.5065e-7</td><td>1.7607e-7</td><td>2.0309e-7</td><td>2.3140e-7</td><td>2.6078e-7</td><td>2.9109e-7</td><td>3.2220e-7</td><td>3.5387e-7</td><td>3.8573e-7</td><td>4.1740e-7</td><td>4.4853e-7</td><td>4.7893e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_79382&quot;</td><td>214.901184</td><td>233.680255</td><td>238.15687</td><td>248.790149</td><td>255.438338</td><td>261.873094</td><td>257.734807</td><td>247.84304</td><td>237.012438</td><td>228.727903</td><td>224.292368</td><td>220.079396</td><td>214.653191</td><td>209.639262</td><td>203.138675</td><td>199.302089</td><td>193.921603</td><td>189.000811</td><td>190.088497</td><td>193.27943</td><td>198.858634</td><td>204.43528</td><td>210.750707</td><td>216.076045</td><td>222.250302</td><td>227.990322</td><td>232.826341</td><td>236.837375</td><td>240.402774</td><td>244.051689</td><td>247.401324</td><td>250.766003</td><td>253.878009</td><td>256.356436</td><td>258.638422</td><td>261.275437</td><td>264.396089</td><td>267.521729</td><td>270.586893</td><td>273.461743</td><td>276.047152</td><td>278.104736</td><td>279.692116</td><td>280.596791</td><td>281.034192</td><td>282.102634</td><td>283.295878</td><td>284.581847</td><td>285.760603</td><td>286.929462</td><td>287.77447</td><td>288.764479</td><td>289.704445</td><td>290.891188</td><td>292.129641</td><td>293.35581</td><td>294.575253</td><td>295.81086</td><td>297.055001</td><td>298.3299</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000007</td><td>0.000012</td><td>0.000039</td><td>0.000084</td><td>0.000153</td><td>0.000268</td><td>0.000359</td><td>0.00036</td><td>0.000297</td><td>0.000275</td><td>0.000298</td><td>0.00044</td><td>0.000863</td><td>0.001429</td><td>0.001863</td><td>0.002066</td><td>0.002147</td><td>0.002095</td><td>0.001996</td><td>0.002045</td><td>0.002201</td><td>0.002725</td><td>0.003783</td><td>0.005656</td><td>0.007051</td><td>0.008019</td><td>0.008582</td><td>0.009266</td><td>0.009702</td><td>0.010539</td><td>0.011249</td><td>0.012267</td><td>0.012545</td><td>0.012626</td><td>0.012695</td><td>0.012759</td><td>0.012806</td><td>0.012857</td><td>0.012966</td><td>1.0240e-37</td><td>9.7601e-38</td><td>8.5978e-38</td><td>7.5063e-38</td><td>6.6207e-38</td><td>5.4017e-38</td><td>5.2994e-38</td><td>3.8816e-38</td><td>3.3322e-38</td><td>2.1390e-38</td><td>2.0676e-38</td><td>1.0688e-38</td><td>5.4397e-44</td><td>1.1950e-47</td><td>5.0025e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.2262e-54</td><td>1.8205e-49</td><td>3.9505e-45</td><td>2.5968e-40</td><td>8.6021e-37</td><td>3.1059e-32</td><td>1.3330e-27</td><td>1.8119e-23</td><td>2.9816e-19</td><td>2.1595e-18</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.9384e-8</td><td>7.2420e-8</td><td>1.1810e-7</td><td>1.0490e-7</td><td>1.7525e-7</td><td>1.8097e-7</td><td>0.0</td><td>8.8280e-8</td><td>0.000002</td><td>0.000003</td><td>0.000001</td><td>0.000008</td><td>0.000013</td><td>0.000026</td><td>0.000015</td><td>0.000017</td><td>0.00001</td><td>0.000011</td><td>0.000006</td><td>0.000001</td><td>9.5407e-7</td><td>5.0271e-7</td><td>1.2904e-7</td><td>3.9397e-7</td><td>6.9656e-8</td><td>1.0900e-7</td><td>3.2887e-8</td><td>6.1281e-12</td><td>6.4930e-12</td><td>6.7827e-12</td><td>7.6458e-12</td><td>7.5475e-12</td><td>8.6658e-12</td><td>6.2857e-12</td><td>4.0058e-12</td><td>2.3955e-12</td><td>1.2466e-12</td><td>8.6311e-13</td><td>3.7115e-13</td><td>1.7075e-18</td><td>3.8695e-22</td><td>5.8055e-23</td><td>1.0269e-17</td><td>1.4395e-11</td><td>1.2922e-8</td><td>8.0708e-8</td><td>1.3658e-7</td><td>6.4196e-9</td><td>9.6679e-9</td><td>3.9776e-8</td><td>6.6408e-7</td><td>0.000001</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>3.0622e-7</td><td>2.8218e-7</td><td>1.4132e-7</td><td>1.4059e-8</td><td>1.1835e-8</td><td>0.0</td><td>1.1867e-8</td><td>9.3804e-9</td><td>1.1343e-8</td><td>7.1379e-9</td><td>8.4519e-9</td><td>5.6288e-9</td><td>3.8800e-9</td><td>3.3601e-9</td><td>5.0977e-9</td><td>4.7965e-9</td><td>3.0257e-9</td><td>2.7207e-9</td><td>2.4084e-9</td><td>0.0</td><td>8.1785e-10</td><td>2.1322e-11</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>51.985733</td><td>37.025343</td><td>38.208433</td><td>41.439662</td><td>38.001918</td><td>30.190197</td><td>30.021533</td><td>25.929628</td><td>18.735208</td><td>12.256085</td><td>8.290056</td><td>5.35526</td><td>2.954092</td><td>1.004033</td><td>0.758437</td><td>6.619794</td><td>13.552734</td><td>20.842486</td><td>27.181561</td><td>33.445408</td><td>40.895726</td><td>47.974711</td><td>51.218588</td><td>51.514585</td><td>50.413059</td><td>47.723948</td><td>43.560689</td><td>37.232692</td><td>30.558484</td><td>25.744329</td><td>22.663329</td><td>21.501361</td><td>20.784877</td><td>19.586174</td><td>17.881074</td><td>16.178472</td><td>14.704635</td><td>12.95959</td><td>10.988883</td><td>8.957445</td><td>7.166212</td><td>5.421142</td><td>4.073932</td><td>3.030966</td><td>1.935302</td><td>0.898479</td><td>-0.2374</td><td>-1.384259</td><td>-2.503452</td><td>-3.631182</td><td>-4.770764</td><td>-5.135978</td><td>-5.241799</td><td>-5.185748</td><td>-5.096547</td><td>-5.027384</td><td>-4.974548</td><td>-4.901653</td><td>-4.805298</td><td>-4.648118</td><td>-1.965289</td><td>-13.129979</td><td>-5.666485</td><td>10.940575</td><td>4.035544</td><td>13.363701</td><td>-2.62111</td><td>-1.983662</td><td>-0.392965</td><td>1.379843</td><td>1.769408</td><td>0.686781</td><td>0.276895</td><td>0.326928</td><td>-0.402234</td><td>-0.026033</td><td>0.193482</td><td>-3.025883</td><td>-6.598888</td><td>-11.725105</td><td>-15.12488</td><td>-15.234713</td><td>-10.115342</td><td>-8.825838</td><td>-9.523812</td><td>-11.556454</td><td>-13.714868</td><td>-14.100296</td><td>-12.718612</td><td>-8.868519</td><td>-4.939118</td><td>-1.703198</td><td>0.321915</td><td>0.954065</td><td>1.369331</td><td>1.735942</td><td>1.75698</td><td>1.537375</td><td>1.317913</td><td>1.320769</td><td>1.361174</td><td>1.626889</td><td>2.054247</td><td>2.287533</td><td>2.128686</td><td>1.667322</td><td>1.095501</td><td>0.552384</td><td>0.028008</td><td>-0.460208</td><td>-1.217157</td><td>-1.664914</td><td>-2.227255</td><td>-2.426104</td><td>-2.512243</td><td>-2.51986</td><td>-2.584885</td><td>-2.594903</td><td>-2.590013</td><td>-2.542154</td><td>98762.158347</td><td>724.008071</td><td>185.437869</td><td>25.138019</td><td>0.061392</td><td>0.033524</td><td>0.514617</td><td>0.084467</td><td>0.093485</td><td>0.060476</td><td>0.070729</td><td>462.394743</td><td>0.0</td><td>0.124711</td><td>0.875289</td><td>0.0</td><td>3.0082e-7</td><td>5.4180e-7</td><td>9.7112e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000014</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>4.4315e-7</td><td>2.0092e-7</td><td>1.2922e-7</td><td>9.7813e-8</td><td>7.9838e-8</td><td>6.8105e-8</td><td>6.1687e-8</td><td>6.0120e-8</td><td>5.9512e-8</td><td>5.9809e-8</td><td>6.0668e-8</td><td>6.1699e-8</td><td>6.2575e-8</td><td>6.3422e-8</td><td>6.4365e-8</td><td>6.5408e-8</td><td>6.6470e-8</td><td>6.7327e-8</td><td>6.8205e-8</td><td>6.8819e-8</td><td>6.8903e-8</td><td>6.8986e-8</td><td>6.9011e-8</td><td>6.8856e-8</td><td>6.8708e-8</td><td>6.8568e-8</td><td>6.8128e-8</td><td>6.7445e-8</td><td>6.6796e-8</td><td>6.6180e-8</td><td>6.5696e-8</td><td>6.5551e-8</td><td>6.5416e-8</td><td>6.5290e-8</td><td>6.5170e-8</td><td>6.5273e-8</td><td>6.5925e-8</td><td>6.6569e-8</td><td>6.7213e-8</td><td>6.7861e-8</td><td>6.8092e-8</td><td>6.8143e-8</td><td>6.8196e-8</td><td>1.8082e-7</td><td>2.0767e-7</td><td>2.3824e-7</td><td>2.7272e-7</td><td>3.1112e-7</td><td>3.5314e-7</td><td>3.9821e-7</td><td>4.4550e-7</td><td>4.9411e-7</td><td>5.4326e-7</td><td>5.9243e-7</td><td>6.4137e-7</td><td>6.8997e-7</td><td>7.3811e-7</td><td>7.8552e-7</td><td>8.3181e-7</td><td>8.7656e-7</td><td>9.1945e-7</td><td>9.6038e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.4248e-8</td><td>4.2493e-8</td><td>5.2630e-8</td><td>6.4969e-8</td><td>7.9770e-8</td><td>9.7180e-8</td><td>1.1718e-7</td><td>1.3957e-7</td><td>1.6400e-7</td><td>1.9012e-7</td><td>2.1760e-7</td><td>2.4624e-7</td><td>2.7592e-7</td><td>3.0649e-7</td><td>3.3771e-7</td><td>3.6923e-7</td><td>4.0063e-7</td><td>4.3159e-7</td><td>4.6190e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_601350&quot;</td><td>214.546909</td><td>233.460889</td><td>237.850575</td><td>248.361199</td><td>252.353758</td><td>261.426464</td><td>257.17252</td><td>248.371481</td><td>237.316651</td><td>229.167941</td><td>224.54255</td><td>219.772206</td><td>214.133077</td><td>209.309529</td><td>202.747871</td><td>198.448037</td><td>195.440953</td><td>190.555468</td><td>190.98908</td><td>193.930869</td><td>199.341072</td><td>204.204674</td><td>210.209238</td><td>215.4366</td><td>221.246391</td><td>226.468019</td><td>231.259085</td><td>235.39401</td><td>239.424336</td><td>243.199836</td><td>246.843741</td><td>250.236207</td><td>253.234652</td><td>256.03563</td><td>258.782512</td><td>261.774998</td><td>264.580276</td><td>267.213506</td><td>269.921149</td><td>272.456241</td><td>274.885599</td><td>276.989321</td><td>278.825928</td><td>280.444455</td><td>282.111358</td><td>283.701795</td><td>285.055091</td><td>286.342619</td><td>287.241526</td><td>288.19072</td><td>289.239106</td><td>290.0659</td><td>290.916585</td><td>291.784479</td><td>292.835766</td><td>293.82904</td><td>294.963785</td><td>296.001406</td><td>297.14843</td><td>298.330913</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000008</td><td>0.000018</td><td>0.00004</td><td>0.000078</td><td>0.000127</td><td>0.000179</td><td>0.000305</td><td>0.000457</td><td>0.000608</td><td>0.000761</td><td>0.000909</td><td>0.001088</td><td>0.001383</td><td>0.001672</td><td>0.00179</td><td>0.002011</td><td>0.002433</td><td>0.003017</td><td>0.003747</td><td>0.004449</td><td>0.005163</td><td>0.005965</td><td>0.006654</td><td>0.007281</td><td>0.007852</td><td>0.00856</td><td>0.009093</td><td>0.009912</td><td>0.010605</td><td>0.011021</td><td>0.01168</td><td>0.012362</td><td>0.012985</td><td>0.013253</td><td>0.013796</td><td>0.013865</td><td>0.014251</td><td>0.014363</td><td>0.014507</td><td>1.0053e-37</td><td>9.6761e-38</td><td>8.6073e-38</td><td>7.5284e-38</td><td>6.5289e-38</td><td>5.3307e-38</td><td>5.1958e-38</td><td>3.8087e-38</td><td>3.3045e-38</td><td>2.1781e-38</td><td>1.9838e-38</td><td>9.7569e-39</td><td>5.1639e-45</td><td>1.4710e-51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.6737e-54</td><td>7.1301e-49</td><td>2.3488e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.1064e-20</td><td>2.6918e-16</td><td>1.1465e-12</td><td>1.1626e-9</td><td>1.7548e-7</td><td>0.000002</td><td>0.000002</td><td>6.1742e-7</td><td>6.8795e-8</td><td>9.5800e-8</td><td>0.000002</td><td>0.000007</td><td>0.000006</td><td>0.000025</td><td>0.000022</td><td>0.000015</td><td>0.000032</td><td>0.000031</td><td>0.000022</td><td>0.000012</td><td>0.000031</td><td>0.000042</td><td>0.000029</td><td>0.000016</td><td>0.000014</td><td>0.000008</td><td>0.000002</td><td>0.0</td><td>4.8502e-7</td><td>5.4026e-8</td><td>1.1691e-7</td><td>3.1324e-8</td><td>6.2827e-12</td><td>6.5066e-12</td><td>6.7600e-12</td><td>7.5840e-12</td><td>7.5201e-12</td><td>8.9972e-12</td><td>6.4027e-12</td><td>4.1368e-12</td><td>2.1367e-12</td><td>1.2561e-12</td><td>8.6818e-13</td><td>3.2672e-13</td><td>1.7113e-19</td><td>1.2354e-25</td><td>1.0416e-23</td><td>9.7141e-19</td><td>4.8089e-17</td><td>0.0</td><td>5.4496e-9</td><td>1.2317e-7</td><td>2.4247e-9</td><td>1.4823e-7</td><td>5.7093e-7</td><td>0.000003</td><td>0.000003</td><td>0.000001</td><td>0.000001</td><td>0.000017</td><td>0.000034</td><td>0.000051</td><td>0.000047</td><td>0.000034</td><td>0.000021</td><td>0.000011</td><td>0.000006</td><td>9.4068e-7</td><td>8.3139e-8</td><td>1.2426e-7</td><td>3.8286e-7</td><td>8.5483e-8</td><td>8.1107e-8</td><td>2.4125e-8</td><td>3.4059e-8</td><td>4.9822e-8</td><td>6.3725e-8</td><td>8.6643e-8</td><td>9.9024e-8</td><td>1.0750e-7</td><td>9.7164e-8</td><td>8.5857e-8</td><td>7.7663e-8</td><td>6.7589e-8</td><td>1.3300e-9</td><td>0.0</td><td>0.0</td><td>5.1521e-13</td><td>2.2926e-11</td><td>4.1159e-11</td><td>6.6932e-11</td><td>4.8389e-14</td><td>26.766125</td><td>38.326466</td><td>44.364158</td><td>50.504799</td><td>40.427283</td><td>31.241199</td><td>30.402445</td><td>27.722321</td><td>20.383986</td><td>14.493661</td><td>9.758615</td><td>4.878546</td><td>2.683249</td><td>2.76098</td><td>2.963564</td><td>7.0875</td><td>15.348046</td><td>21.242244</td><td>26.108509</td><td>30.126443</td><td>35.599535</td><td>40.136581</td><td>41.652886</td><td>41.121061</td><td>38.686804</td><td>34.965231</td><td>31.105139</td><td>27.51296</td><td>24.737766</td><td>23.037146</td><td>21.860157</td><td>20.854756</td><td>19.693072</td><td>18.156475</td><td>16.349687</td><td>14.623607</td><td>12.867586</td><td>10.999232</td><td>9.042802</td><td>7.34352</td><td>5.893471</td><td>4.553944</td><td>3.375012</td><td>2.374191</td><td>1.438902</td><td>0.603676</td><td>-0.173665</td><td>-0.930428</td><td>-1.617219</td><td>-2.284518</td><td>-2.797098</td><td>-3.156353</td><td>-3.366056</td><td>-3.481461</td><td>-3.525668</td><td>-3.454358</td><td>-3.384439</td><td>-3.113365</td><td>-2.909109</td><td>-2.618813</td><td>-19.714897</td><td>-7.008516</td><td>5.098422</td><td>16.692305</td><td>6.039838</td><td>14.258133</td><td>0.530596</td><td>0.054057</td><td>-1.110178</td><td>-0.972205</td><td>0.754306</td><td>0.995024</td><td>0.239525</td><td>0.393146</td><td>1.070035</td><td>1.412289</td><td>2.565232</td><td>2.234606</td><td>0.058334</td><td>-5.735292</td><td>-10.932171</td><td>-12.986231</td><td>-12.884899</td><td>-12.573914</td><td>-11.847338</td><td>-10.107544</td><td>-6.941908</td><td>-2.603484</td><td>1.049267</td><td>3.723152</td><td>5.048724</td><td>5.296405</td><td>5.235892</td><td>4.808592</td><td>4.236725</td><td>3.625706</td><td>3.015351</td><td>2.605133</td><td>2.349206</td><td>2.155275</td><td>2.017206</td><td>1.857969</td><td>1.687204</td><td>1.401592</td><td>1.006144</td><td>0.514713</td><td>-0.013485</td><td>-0.566443</td><td>-1.0878</td><td>-1.582165</td><td>-2.035671</td><td>-2.415875</td><td>-2.685827</td><td>-2.897569</td><td>-3.029162</td><td>-3.13676</td><td>-3.149133</td><td>-3.058256</td><td>-2.957907</td><td>-2.749553</td><td>98463.738907</td><td>525.702085</td><td>132.107937</td><td>18.355525</td><td>0.061175</td><td>0.06211</td><td>0.373663</td><td>0.142697</td><td>0.183234</td><td>0.058991</td><td>0.094476</td><td>460.091223</td><td>0.0</td><td>0.401465</td><td>0.598535</td><td>0.0</td><td>3.0046e-7</td><td>5.4114e-7</td><td>9.6993e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000014</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>9.3332e-7</td><td>4.0457e-7</td><td>1.9842e-7</td><td>1.2794e-7</td><td>9.8775e-8</td><td>8.2898e-8</td><td>7.2434e-8</td><td>6.5700e-8</td><td>6.2111e-8</td><td>6.0836e-8</td><td>6.0226e-8</td><td>6.0869e-8</td><td>6.1481e-8</td><td>6.2038e-8</td><td>6.2637e-8</td><td>6.3359e-8</td><td>6.4105e-8</td><td>6.5292e-8</td><td>6.6596e-8</td><td>6.7845e-8</td><td>6.8833e-8</td><td>6.9828e-8</td><td>7.0558e-8</td><td>7.0732e-8</td><td>7.0900e-8</td><td>7.1004e-8</td><td>6.9089e-8</td><td>6.7275e-8</td><td>6.5551e-8</td><td>6.4353e-8</td><td>6.4043e-8</td><td>6.3750e-8</td><td>6.3476e-8</td><td>6.3262e-8</td><td>6.3129e-8</td><td>6.3004e-8</td><td>6.2884e-8</td><td>6.2481e-8</td><td>6.1659e-8</td><td>6.0837e-8</td><td>5.9535e-8</td><td>5.7958e-8</td><td>5.7958e-8</td><td>5.7958e-8</td><td>1.8050e-7</td><td>2.0730e-7</td><td>2.3782e-7</td><td>2.7224e-7</td><td>3.1057e-7</td><td>3.5252e-7</td><td>3.9751e-7</td><td>4.4472e-7</td><td>4.9324e-7</td><td>5.4230e-7</td><td>5.9139e-7</td><td>6.4024e-7</td><td>6.8876e-7</td><td>7.3681e-7</td><td>7.8414e-7</td><td>8.3035e-7</td><td>8.7502e-7</td><td>9.1783e-7</td><td>9.5869e-7</td><td>9.9766e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.3986e-8</td><td>4.2184e-8</td><td>5.2268e-8</td><td>6.4547e-8</td><td>7.9282e-8</td><td>9.6621e-8</td><td>1.1655e-7</td><td>1.3886e-7</td><td>1.6322e-7</td><td>1.8926e-7</td><td>2.1667e-7</td><td>2.4525e-7</td><td>2.7487e-7</td><td>3.0538e-7</td><td>3.3655e-7</td><td>3.6802e-7</td><td>3.9938e-7</td><td>4.3031e-7</td><td>4.6058e-7</td><td>4.9014e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (625_000, 557)\n",
       "┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ sam ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ sta ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ cam ┆ cam ┆ cam ┆ cam ┆ cam ┆ cam ┆ cam ┆ cam ┆ cam ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu ┆ pbu │\n",
       "│ ple ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ te_ ┆ f_S ┆ f_L ┆ f_S ┆ f_T ┆ f_T ┆ f_C ┆ _in ┆ _in ┆ _in ┆ _in ┆ _in ┆ _in ┆ _in ┆ _in ┆ _in ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_o ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_C ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N ┆ f_N │\n",
       "│ _id ┆ t_0 ┆ t_1 ┆ t_2 ┆ t_3 ┆ t_4 ┆ t_5 ┆ t_6 ┆ t_7 ┆ t_8 ┆ t_9 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_1 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_2 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_3 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_4 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ t_5 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ q00 ┆ u_0 ┆ u_1 ┆ u_2 ┆ u_3 ┆ u_4 ┆ u_5 ┆ u_6 ┆ u_7 ┆ u_8 ┆ u_9 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_1 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_2 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_3 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_4 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ u_5 ┆ v_0 ┆ v_1 ┆ v_2 ┆ v_3 ┆ v_4 ┆ v_5 ┆ v_6 ┆ v_7 ┆ v_8 ┆ v_9 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_1 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_2 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_3 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_4 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ v_5 ┆ ps  ┆ OLI ┆ HFL ┆ HFL ┆ AUX ┆ AUY ┆ OSZ ┆ _AL ┆ _AL ┆ _AS ┆ _AS ┆ _LW ┆ _IC ┆ _LA ┆ _OC ┆ _SN ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ zon ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ H4_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ ┆ 2O_ │\n",
       "│ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 01_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 02_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ 03_ ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ --- ┆ N   ┆ X   ┆ X   ┆ --- ┆ --- ┆ RS  ┆ DIF ┆ DIR ┆ DIF ┆ DIR ┆ UP  ┆ EFR ┆ NDF ┆ NFR ┆ OWH ┆ e_0 ┆ e_1 ┆ e_2 ┆ e_3 ┆ e_4 ┆ e_5 ┆ e_6 ┆ e_7 ┆ e_8 ┆ e_9 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 10  ┆ 11  ┆ 12  ┆ 13  ┆ 14  ┆ 15  ┆ 16  ┆ 17  ┆ 18  ┆ 19  ┆ 20  ┆ 21  ┆ 22  ┆ 23  ┆ 24  ┆ 25  ┆ 26  ┆ 27  ┆ 28  ┆ 29  ┆ 30  ┆ 31  ┆ 32  ┆ 33  ┆ 34  ┆ 35  ┆ 36  ┆ 37  ┆ 38  ┆ 39  ┆ 40  ┆ 41  ┆ 42  ┆ 43  ┆ 44  ┆ 45  ┆ 46  ┆ 47  ┆ 48  ┆ 49  ┆ 50  ┆ 51  ┆ 52  ┆ 53  ┆ 54  ┆ 55  ┆ 56  ┆ 57  ┆ 58  ┆ 59  ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 10  ┆ 11  ┆ 12  ┆ 13  ┆ 14  ┆ 15  ┆ 16  ┆ 17  ┆ 18  ┆ 19  ┆ 20  ┆ 21  ┆ 22  ┆ 23  ┆ 24  ┆ 25  ┆ 26  ┆ 27  ┆ 28  ┆ 29  ┆ 30  ┆ 31  ┆ 32  ┆ 33  ┆ 34  ┆ 35  ┆ 36  ┆ 37  ┆ 38  ┆ 39  ┆ 40  ┆ 41  ┆ 42  ┆ 43  ┆ 44  ┆ 45  ┆ 46  ┆ 47  ┆ 48  ┆ 49  ┆ 50  ┆ 51  ┆ 52  ┆ 53  ┆ 54  ┆ 55  ┆ 56  ┆ 57  ┆ 58  ┆ 59  │\n",
       "│ str ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 10  ┆ 11  ┆ 12  ┆ 13  ┆ 14  ┆ 15  ┆ 16  ┆ 17  ┆ 18  ┆ 19  ┆ 20  ┆ 21  ┆ 22  ┆ 23  ┆ 24  ┆ 25  ┆ 26  ┆ 27  ┆ 28  ┆ 29  ┆ 30  ┆ 31  ┆ 32  ┆ 33  ┆ 34  ┆ 35  ┆ 36  ┆ 37  ┆ 38  ┆ 39  ┆ 40  ┆ 41  ┆ 42  ┆ 43  ┆ 44  ┆ 45  ┆ 46  ┆ 47  ┆ 48  ┆ 49  ┆ 50  ┆ 51  ┆ 52  ┆ 53  ┆ 54  ┆ 55  ┆ 56  ┆ 57  ┆ 58  ┆ 59  ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 10  ┆ 11  ┆ 12  ┆ 13  ┆ 14  ┆ 15  ┆ 16  ┆ 17  ┆ 18  ┆ 19  ┆ 20  ┆ 21  ┆ 22  ┆ 23  ┆ 24  ┆ 25  ┆ 26  ┆ 27  ┆ 28  ┆ 29  ┆ 30  ┆ 31  ┆ 32  ┆ 33  ┆ 34  ┆ 35  ┆ 36  ┆ 37  ┆ 38  ┆ 39  ┆ 40  ┆ 41  ┆ 42  ┆ 43  ┆ 44  ┆ 45  ┆ 46  ┆ 47  ┆ 48  ┆ 49  ┆ 50  ┆ 51  ┆ 52  ┆ 53  ┆ 54  ┆ 55  ┆ 56  ┆ 57  ┆ 58  ┆ 59  ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 10  ┆ 11  ┆ 12  ┆ 13  ┆ 14  ┆ 15  ┆ 16  ┆ 17  ┆ 18  ┆ 19  ┆ 20  ┆ 21  ┆ 22  ┆ 23  ┆ 24  ┆ 25  ┆ 26  ┆ 27  ┆ 28  ┆ 29  ┆ 30  ┆ 31  ┆ 32  ┆ 33  ┆ 34  ┆ 35  ┆ 36  ┆ 37  ┆ 38  ┆ 39  ┆ 40  ┆ 41  ┆ 42  ┆ 43  ┆ 44  ┆ 45  ┆ 46  ┆ 47  ┆ 48  ┆ 49  ┆ 50  ┆ 51  ┆ 52  ┆ 53  ┆ 54  ┆ 55  ┆ 56  ┆ 57  ┆ 58  ┆ 59  ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ f64 ┆ --- ┆ --- ┆ --- ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ AC  ┆ RAC ┆ AC  ┆ LAN ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆     ┆ f64 ┆ f64 ┆ f64 ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ D   ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ --- ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆ f64 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f64 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ tes ┆ 209 ┆ 220 ┆ 227 ┆ 241 ┆ 254 ┆ 262 ┆ 261 ┆ 254 ┆ 243 ┆ 236 ┆ 229 ┆ 225 ┆ 221 ┆ 215 ┆ 208 ┆ 203 ┆ 199 ┆ 198 ┆ 196 ┆ 198 ┆ 201 ┆ 205 ┆ 209 ┆ 214 ┆ 219 ┆ 224 ┆ 228 ┆ 233 ┆ 237 ┆ 241 ┆ 246 ┆ 249 ┆ 253 ┆ 256 ┆ 260 ┆ 263 ┆ 266 ┆ 269 ┆ 271 ┆ 274 ┆ 276 ┆ 278 ┆ 279 ┆ 281 ┆ 282 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 292 ┆ 293 ┆ 294 ┆ 295 ┆ 296 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.7 ┆ 9.7 ┆ 9.5 ┆ 9.0 ┆ 8.8 ┆ 8.7 ┆ 7.4 ┆ 5.7 ┆ 4.1 ┆ 2.5 ┆ 9.0 ┆ 8.8 ┆ 3.9 ┆ 5.2 ┆ 3.8 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.3 ┆ 8.4 ┆ 1.0 ┆ 2.3 ┆ 5.3 ┆ 0.0 ┆ 1.7 ┆ 1.9 ┆ 5.1 ┆ 0.0 ┆ 3.3 ┆ 5.4 ┆ 5.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.4 ┆ 7.4 ┆ 2.4 ┆ 7.5 ┆ 3.2 ┆ 3.5 ┆ 3.9 ┆ 4.7 ┆ 5.1 ┆ 5.5 ┆ 5.5 ┆ 5.0 ┆ 4.1 ┆ 3.0 ┆ 1.2 ┆ 1.2 ┆ 5.5 ┆ 8.9 ┆ 2.3 ┆ 0.0 ┆ 1.6 ┆ 5.6 ┆ 8.7 ┆ 3.3 ┆ 1.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 5.0 ┆ 1.7 ┆ 6.1 ┆ 9.7 ┆ 3.2 ┆ 2.4 ┆ 7.7 ┆ 1.6 ┆ 2.6 ┆ 5.0 ┆ 5.2 ┆ 3.7 ┆ 7.3 ┆ 5.2 ┆ 3.7 ┆ 3.7 ┆ 2.3 ┆ 2.7 ┆ 6.0 ┆ 1.4 ┆ 4.6 ┆ 0.0 ┆ 3.0 ┆ 1.9 ┆ 2.0 ┆ 1.2 ┆ 1.2 ┆ -12 ┆ -35 ┆ -39 ┆ -26 ┆ -27 ┆ -26 ┆ -25 ┆ -24 ┆ -21 ┆ -18 ┆ -13 ┆ -7. ┆ -2. ┆ 0.4 ┆ 6.0 ┆ 9.9 ┆ 14. ┆ 20. ┆ 24. ┆ 28. ┆ 31. ┆ 34. ┆ 35. ┆ 35. ┆ 33. ┆ 31. ┆ 29. ┆ 27. ┆ 25. ┆ 23. ┆ 21. ┆ 20. ┆ 18. ┆ 16. ┆ 15. ┆ 13. ┆ 12. ┆ 11. ┆ 9.9 ┆ 8.9 ┆ 8.0 ┆ 7.1 ┆ 6.2 ┆ 5.4 ┆ 4.7 ┆ 4.1 ┆ 3.6 ┆ 3.0 ┆ 2.5 ┆ 2.0 ┆ 1.5 ┆ 1.1 ┆ 0.6 ┆ 0.2 ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ 11. ┆ 6.0 ┆ 11. ┆ 2.3 ┆ -3. ┆ 0.9 ┆ 5.2 ┆ 5.2 ┆ 1.6 ┆ 0.2 ┆ 1.0 ┆ 1.3 ┆ 0.9 ┆ 3.4 ┆ 4.7 ┆ 2.9 ┆ 3.0 ┆ 2.9 ┆ 2.0 ┆ 1.9 ┆ 2.1 ┆ 2.0 ┆ 1.5 ┆ 1.5 ┆ 1.5 ┆ 1.3 ┆ 1.1 ┆ 0.8 ┆ 0.5 ┆ 0.3 ┆ 0.3 ┆ 0.5 ┆ 0.8 ┆ 1.0 ┆ 1.0 ┆ 0.9 ┆ 0.7 ┆ 0.4 ┆ 0.3 ┆ 0.2 ┆ 0.2 ┆ 0.2 ┆ 0.2 ┆ 0.1 ┆ 0.0 ┆ 0.0 ┆ 0.1 ┆ 0.1 ┆ 0.2 ┆ 0.2 ┆ 0.3 ┆ 0.3 ┆ 0.3 ┆ 0.4 ┆ 0.4 ┆ 0.5 ┆ 0.5 ┆ 0.5 ┆ 0.5 ┆ 0.1 ┆ 100 ┆ 0.0 ┆ 4.3 ┆ -0. ┆ 0.0 ┆ -0. ┆ 0.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 438 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 2.6 ┆ 4.7 ┆ 8.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.4 ┆ 4.8 ┆ 3.3 ┆ 2.4 ┆ 1.8 ┆ 1.3 ┆ 1.0 ┆ 8.9 ┆ 8.0 ┆ 7.4 ┆ 7.1 ┆ 6.9 ┆ 6.8 ┆ 6.7 ┆ 6.6 ┆ 6.5 ┆ 6.4 ┆ 6.3 ┆ 6.3 ┆ 6.2 ┆ 6.0 ┆ 5.9 ┆ 5.7 ┆ 5.5 ┆ 5.4 ┆ 5.2 ┆ 5.0 ┆ 4.7 ┆ 4.4 ┆ 4.2 ┆ 4.0 ┆ 3.8 ┆ 3.6 ┆ 3.4 ┆ 3.3 ┆ 3.2 ┆ 3.2 ┆ 3.1 ┆ 3.0 ┆ 3.0 ┆ 2.9 ┆ 2.9 ┆ 2.8 ┆ 1.7 ┆ 1.9 ┆ 2.2 ┆ 2.5 ┆ 2.9 ┆ 3.3 ┆ 3.7 ┆ 4.2 ┆ 4.6 ┆ 5.1 ┆ 5.5 ┆ 6.0 ┆ 6.5 ┆ 6.9 ┆ 7.4 ┆ 7.8 ┆ 8.2 ┆ 8.6 ┆ 9.0 ┆ 9.4 ┆ 9.7 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 2.7 ┆ 3.3 ┆ 4.2 ┆ 5.3 ┆ 6.5 ┆ 8.1 ┆ 9.8 ┆ 1.1 ┆ 1.4 ┆ 1.6 ┆ 1.8 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.0 ┆ 3.3 ┆ 3.5 ┆ 3.8 ┆ 4.1 ┆ 4.4 ┆ 4.7 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_1 ┆ .80 ┆ .69 ┆ .78 ┆ .38 ┆ .60 ┆ .31 ┆ .30 ┆ .06 ┆ .90 ┆ .30 ┆ .97 ┆ .22 ┆ .04 ┆ .82 ┆ .99 ┆ .05 ┆ .04 ┆ .23 ┆ .02 ┆ .16 ┆ .19 ┆ .36 ┆ .87 ┆ .74 ┆ .54 ┆ .30 ┆ .95 ┆ .36 ┆ .65 ┆ .95 ┆ .02 ┆ .80 ┆ .45 ┆ .97 ┆ .29 ┆ .40 ┆ .45 ┆ .33 ┆ .93 ┆ .34 ┆ .36 ┆ .19 ┆ .77 ┆ .11 ┆ .21 ┆ .13 ┆ .58 ┆ .66 ┆ .68 ┆ .64 ┆ .31 ┆ .95 ┆ .68 ┆ .47 ┆ .37 ┆ .34 ┆ .43 ┆ .43 ┆ .38 ┆ .39 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 003 ┆ 005 ┆ 007 ┆ 010 ┆ 012 ┆ 015 ┆ 018 ┆ 022 ┆ 025 ┆ 030 ┆ 036 ┆ 043 ┆ 049 ┆ 056 ┆ 063 ┆ 071 ┆ 080 ┆ 084 ┆ 090 ┆ 095 ┆ 100 ┆ 108 ┆ 117 ┆ 123 ┆ 129 ┆ 134 ┆ 138 ┆ 142 ┆ 146 ┆ 153 ┆ 159 ┆ 013 ┆ 067 ┆ 320 ┆ 000 ┆ 489 ┆ 347 ┆ 539 ┆ 589 ┆ 264 ┆ 798 ┆ 800 ┆ 615 ┆ 948 ┆ 197 ┆ 237 ┆     ┆     ┆     ┆     ┆     ┆     ┆ 248 ┆     ┆     ┆     ┆     ┆     ┆ 843 ┆ 492 ┆ 226 ┆ 281 ┆ 019 ┆     ┆ 424 ┆ 801 ┆ 508 ┆     ┆ 095 ┆ 720 ┆ 558 ┆     ┆     ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 976 ┆ 434 ┆ 470 ┆ 538 ┆ 430 ┆ 474 ┆ 380 ┆ 086 ┆ 482 ┆ 466 ┆ 631 ┆ 086 ┆ 181 ┆ 849 ┆ 191 ┆ 796 ┆ 664 ┆ 162 ┆ 687 ┆     ┆ 279 ┆ 999 ┆ 989 ┆ 620 ┆ 776 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 476 ┆ 979 ┆ 143 ┆ 138 ┆ 653 ┆ 650 ┆ 237 ┆ 265 ┆ 423 ┆ 995 ┆ 668 ┆ 720 ┆ 597 ┆ 720 ┆ 769 ┆ 740 ┆ 163 ┆ 407 ┆ 542 ┆ 243 ┆ 001 ┆     ┆ 952 ┆ 876 ┆ 088 ┆ 196 ┆ 907 ┆ .62 ┆ .26 ┆ .28 ┆ .10 ┆ .02 ┆ .97 ┆ .68 ┆ .11 ┆ .75 ┆ .17 ┆ .10 ┆ 057 ┆ 783 ┆ 827 ┆ 971 ┆ 447 ┆ 382 ┆ 153 ┆ 696 ┆ 106 ┆ 409 ┆ 470 ┆ 623 ┆ 262 ┆ 688 ┆ 477 ┆ 394 ┆ 362 ┆ 418 ┆ 586 ┆ 805 ┆ 051 ┆ 329 ┆ 628 ┆ 029 ┆ 527 ┆ 198 ┆ 006 ┆ 460 ┆ 864 ┆ 468 ┆ 550 ┆ 987 ┆ 704 ┆ 675 ┆ 921 ┆ 544 ┆ 921 ┆ 461 ┆ 538 ┆ 821 ┆ 254 ┆ 726 ┆ 108 ┆ 256 ┆ 681 ┆ 069 ┆ 349 ┆ 312 ┆ 116 ┆ 938 ┆ 370 ┆ 908 ┆ 858 ┆ 281 ┆ 800 ┆ 559 ┆ 414 ┆ 898 ┆ 146 ┆ 398 ┆ 178 ┆ 711 ┆ 721 ┆ 068 ┆ 373 ┆ 556 ┆ 771 ┆ 583 ┆ 352 ┆ 346 ┆ 245 ┆ 546 ┆ 813 ┆ 788 ┆ 842 ┆ 284 ┆ 486 ┆ 405 ┆ 491 ┆ 760 ┆ 674 ┆ 443 ┆ 546 ┆ 909 ┆ 892 ┆ 312 ┆ 758 ┆ 051 ┆ 695 ┆ 565 ┆ 709 ┆ 187 ┆ 021 ┆ 211 ┆ 684 ┆ 373 ┆ 814 ┆ 285 ┆ 812 ┆ 189 ┆ 464 ┆ 862 ┆ 376 ┆ 929 ┆ 250 ┆ 192 ┆ 278 ┆ 264 ┆ 696 ┆ 600 ┆     ┆ 472 ┆ 246 ┆ 015 ┆ 000 ┆     ┆     ┆     ┆     ┆     ┆ .75 ┆     ┆     ┆     ┆     ┆ 356 ┆ 469 ┆ 083 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 424 ┆ 861 ┆ 337 ┆ 476 ┆ 146 ┆ 446 ┆ 342 ┆ 834 ┆ 409 ┆ 827 ┆ 977 ┆ 728 ┆ 498 ┆ 445 ┆ 510 ┆ 643 ┆ 768 ┆ 963 ┆ 137 ┆ 009 ┆ 600 ┆ 198 ┆ 647 ┆ 942 ┆ 316 ┆ 773 ┆ 042 ┆ 254 ┆ 604 ┆ 089 ┆ 019 ┆ 186 ┆ 475 ┆ 873 ┆ 392 ┆ 752 ┆ 130 ┆ 515 ┆ 900 ┆ 344 ┆ 831 ┆ 312 ┆ 908 ┆ 047 ┆ 578 ┆ 460 ┆ 711 ┆ 331 ┆ 293 ┆ 542 ┆ 000 ┆ 582 ┆ 216 ┆ 852 ┆ 466 ┆ 047 ┆ 585 ┆ 055 ┆ 419 ┆ 638 ┆ 682 ┆ 540 ┆ 221 ┆ 737 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 090 ┆ 989 ┆ 566 ┆ 122 ┆ 920 ┆ 133 ┆ 783 ┆ 873 ┆ 068 ┆ 434 ┆ 941 ┆ 572 ┆ 315 ┆ 157 ┆ 073 ┆ 032 ┆ 994 ┆ 925 ┆ 805 ┆ 625 ┆ 387 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 696 ┆ 259 ┆ 821 ┆ 328 ┆ 681 ┆ 296 ┆ 906 ┆ 836 ┆ 203 ┆ 442 ┆ 240 ┆ 212 ┆ 744 ┆ 378 ┆ 810 ┆ 889 ┆ 978 ┆ 129 ┆ 371 ┆ 433 ┆ 542 ┆ 474 ┆ 750 ┆ 569 ┆ 896 ┆ 213 ┆ 586 ┆ 289 ┆ 575 ┆ 552 ┆ 655 ┆ 205 ┆ 825 ┆ 960 ┆ 127 ┆ 439 ┆ 025 ┆ 399 ┆ 523 ┆ 161 ┆ 591 ┆ 486 ┆ 915 ┆ 496 ┆ 445 ┆ 624 ┆ 486 ┆ 385 ┆ 046 ┆ 269 ┆ 736 ┆ 939 ┆ 487 ┆ 652 ┆ 467 ┆ 432 ┆ 796 ┆ 973 ┆ 872 ┆ 002 ┆ 144 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 06  ┆ 11  ┆ 2   ┆ 32  ┆ 42  ┆ 59  ┆ 91  ┆ 52  ┆ 57  ┆ 98  ┆ 75  ┆ 98  ┆ 29  ┆ 47  ┆ 08  ┆ 51  ┆ 17  ┆ 93  ┆ 69  ┆ 37  ┆ 23  ┆ 14  ┆ 55  ┆ 82  ┆ 61  ┆ 9   ┆ 22  ┆ 62  ┆ 33  ┆ 61  ┆ 51  ┆ 23  ┆ 1   ┆ 01  ┆ 13  ┆ 63  ┆ 26  ┆ 91  ┆ 9   ┆ 26  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-4 ┆     ┆     ┆     ┆     ┆     ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-2 ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆     ┆     ┆ 01  ┆ 04  ┆ 01  ┆ 04  ┆ 03  ┆ 06  ┆ 03  ┆ 02  ┆ 03  ┆ 07  ┆ 05  ┆ 1   ┆ 1   ┆ 04  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-9 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆     ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 01  ┆ 02  ┆ 01  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-9 ┆ e-9 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ 109 ┆ 499 ┆ 828 ┆ 145 ┆ 725 ┆ 712 ┆ 103 ┆ 690 ┆ 385 ┆ 445 ┆ 203 ┆ 726 ┆ 299 ┆ 7   ┆ 61  ┆ 5   ┆ 375 ┆ 91  ┆ 414 ┆ 28  ┆ 991 ┆ 184 ┆ 454 ┆ 472 ┆ 433 ┆ 991 ┆ 856 ┆ 967 ┆ 579 ┆ 494 ┆ 095 ┆ 06  ┆ 192 ┆ 743 ┆ 511 ┆ 132 ┆ 934 ┆ 181 ┆ 34  ┆ 05  ┆ 04  ┆ 62  ┆ 09  ┆ 31  ┆ 71  ┆ 04  ┆ 44  ┆ 11  ┆ 25  ┆ 15  ┆ 54  ┆ 12  ┆ 71  ┆ 09  ┆ 035 ┆ 038 ┆ 693 ┆ 754 ┆ 553 ┆ 497 ┆ 681 ┆ 94  ┆ 263 ┆ 34  ┆ 934 ┆ 46  ┆ 62  ┆ 53  ┆ 35  ┆ 83  ┆ 91  ┆ 28  ┆ 6   ┆ 31  ┆ 99  ┆ 7   ┆ 34  ┆ 36  ┆ 52  ┆ 15  ┆ 94  ┆ 51  ┆ 7   ┆ 69  ┆ 97  ┆ 52  ┆ 16  ┆ 05  ┆ 27  ┆ 01  ┆ 2   ┆ 14  ┆ 42  ┆ 74  ┆ 06  ┆ 44  ┆ 3   ┆ 87  ┆ 24  ┆ 1   ┆ 86  ┆ 5   ┆ 26  ┆ 73  ┆ 38  ┆ 25  ┆ 98  ┆ 8   ┆ 99  ┆ 41  ┆ 75  ┆ 98  ┆ 31  ┆ 31  ┆ 04  ┆ 97  ┆ 03  ┆ 45  ┆ 96  ┆ 98  ┆ .02 ┆     ┆ 25  ┆ 807 ┆ 45  ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆ 855 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 08  ┆ 12  ┆ 13  ┆ 14  ┆ 13  ┆ 12  ┆ 1   ┆ 07  ┆ 05  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 51  ┆ 3   ┆ 3   ┆ 9   ┆ 2   ┆ 2   ┆ 3   ┆ 8   ┆ 5   ┆     ┆ 1   ┆ 3   ┆ 4   ┆ 5   ┆ 4   ┆ 9   ┆ 2   ┆ 3   ┆ 5   ┆ 5   ┆ 1   ┆ 2   ┆ 5   ┆ 9   ┆ 4   ┆ 1   ┆ 5   ┆ 1   ┆ 2   ┆     ┆ 1   ┆ 5   ┆ 5   ┆ 8   ┆ 8   ┆ 7   ┆ 8   ┆ 5   ┆ 5   ┆ 1   ┆ 7   ┆ 7   ┆ 1   ┆ 9   ┆ 7   ┆ 2   ┆ 1   ┆ 6   ┆ 3   ┆ 4   ┆ 8   ┆ 2   ┆ 4   ┆ 2   ┆     ┆ 1   ┆ 8   ┆     ┆ 4   ┆     ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 8   ┆ 9   ┆ 3   ┆ 9   ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 7   ┆ 8   ┆ 8   ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 2   ┆ 7   ┆ 3   ┆ 4   ┆     ┆ 4   ┆ 0   ┆ 7   ┆ 1   ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 1   ┆ 1   ┆ 2   ┆     ┆ 3   ┆ 3   ┆ 3   ┆ 4   ┆ 2   ┆ 8   ┆ 4   ┆ 6   ┆ 7   ┆ 1   ┆ 4   ┆ 3   ┆ 4   ┆ 4   ┆ 8   ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 796 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 208 ┆ 219 ┆ 228 ┆ 242 ┆ 256 ┆ 263 ┆ 261 ┆ 253 ┆ 243 ┆ 236 ┆ 231 ┆ 226 ┆ 223 ┆ 218 ┆ 213 ┆ 207 ┆ 203 ┆ 200 ┆ 198 ┆ 200 ┆ 202 ┆ 205 ┆ 209 ┆ 213 ┆ 217 ┆ 221 ┆ 226 ┆ 230 ┆ 234 ┆ 238 ┆ 242 ┆ 246 ┆ 249 ┆ 253 ┆ 256 ┆ 260 ┆ 263 ┆ 265 ┆ 268 ┆ 271 ┆ 273 ┆ 275 ┆ 276 ┆ 278 ┆ 279 ┆ 280 ┆ 281 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 292 ┆ 292 ┆ 293 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.7 ┆ 9.6 ┆ 9.5 ┆ 9.3 ┆ 9.4 ┆ 9.2 ┆ 8.1 ┆ 6.6 ┆ 5.3 ┆ 3.9 ┆ 1.4 ┆ 1.5 ┆ 2.9 ┆ 4.8 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.4 ┆ 3.4 ┆ 1.2 ┆ 0.0 ┆ 6.3 ┆ 8.4 ┆ 1.0 ┆ 2.3 ┆ 5.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.3 ┆ 7.9 ┆ 1.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.6 ┆ 3.2 ┆ 3.4 ┆ 3.9 ┆ 4.5 ┆ 5.0 ┆ 5.5 ┆ 5.7 ┆ 5.4 ┆ 4.8 ┆ 4.1 ┆ 1.8 ┆ 2.3 ┆ 5.3 ┆ 2.3 ┆ 7.4 ┆ 3.2 ┆ 1.6 ┆ 2.5 ┆ 2.4 ┆ 1.5 ┆ 3.5 ┆ 1.5 ┆ 6.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.6 ┆ 1.9 ┆ 1.0 ┆ 1.4 ┆ 2.1 ┆ 2.1 ┆ 2.4 ┆ 1.4 ┆ 2.6 ┆ 6.8 ┆ 7.9 ┆ 4.7 ┆ 4.8 ┆ 3.8 ┆ 2.0 ┆ 1.9 ┆ 6.0 ┆ 5.5 ┆ 7.8 ┆ 8.6 ┆ 2.1 ┆ 1.1 ┆ 1.0 ┆ 7.0 ┆ 9.8 ┆ -29 ┆ -39 ┆ -34 ┆ -21 ┆ -18 ┆ -19 ┆ -19 ┆ -18 ┆ -15 ┆ -12 ┆ -8. ┆ -3. ┆ 2.2 ┆ 5.3 ┆ 8.9 ┆ 14. ┆ 18. ┆ 22. ┆ 25. ┆ 28. ┆ 31. ┆ 32. ┆ 32. ┆ 32. ┆ 31. ┆ 30. ┆ 29. ┆ 28. ┆ 27. ┆ 25. ┆ 24. ┆ 22. ┆ 21. ┆ 19. ┆ 18. ┆ 17. ┆ 16. ┆ 15. ┆ 14. ┆ 13. ┆ 12. ┆ 11. ┆ 10. ┆ 10. ┆ 9.4 ┆ 8.8 ┆ 8.3 ┆ 7.7 ┆ 7.2 ┆ 6.8 ┆ 6.4 ┆ 6.0 ┆ 5.7 ┆ 5.4 ┆ 5.1 ┆ 4.9 ┆ 4.6 ┆ 4.4 ┆ 4.3 ┆ 3.5 ┆ 3.4 ┆ 7.9 ┆ 14. ┆ 7.8 ┆ 0.8 ┆ 2.7 ┆ 5.3 ┆ 4.4 ┆ 0.9 ┆ -1. ┆ -0. ┆ 0.8 ┆ 0.9 ┆ 1.4 ┆ 4.6 ┆ 5.3 ┆ 5.1 ┆ 6.1 ┆ 7.3 ┆ 8.2 ┆ 8.7 ┆ 9.0 ┆ 8.9 ┆ 8.4 ┆ 7.6 ┆ 6.8 ┆ 6.3 ┆ 5.9 ┆ 5.5 ┆ 5.0 ┆ 4.6 ┆ 4.1 ┆ 3.5 ┆ 2.9 ┆ 2.2 ┆ 1.6 ┆ 1.2 ┆ 0.9 ┆ 0.8 ┆ 0.6 ┆ 0.4 ┆ 0.2 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ 101 ┆ 0.0 ┆ 4.3 ┆ -1. ┆ -0. ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 420 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 2.6 ┆ 4.7 ┆ 8.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.0 ┆ 4.2 ┆ 3.2 ┆ 2.4 ┆ 1.7 ┆ 1.2 ┆ 1.0 ┆ 9.0 ┆ 8.1 ┆ 7.6 ┆ 7.3 ┆ 7.0 ┆ 6.8 ┆ 6.7 ┆ 6.6 ┆ 6.4 ┆ 6.3 ┆ 6.2 ┆ 6.0 ┆ 5.8 ┆ 5.7 ┆ 5.5 ┆ 5.3 ┆ 5.2 ┆ 5.0 ┆ 4.8 ┆ 4.5 ┆ 4.3 ┆ 4.1 ┆ 3.9 ┆ 3.7 ┆ 3.5 ┆ 3.3 ┆ 3.2 ┆ 3.1 ┆ 3.0 ┆ 3.0 ┆ 2.9 ┆ 2.9 ┆ 2.9 ┆ 2.8 ┆ 2.8 ┆ 1.6 ┆ 1.9 ┆ 2.2 ┆ 2.5 ┆ 2.8 ┆ 3.2 ┆ 3.7 ┆ 4.1 ┆ 4.5 ┆ 5.0 ┆ 5.5 ┆ 5.9 ┆ 6.4 ┆ 6.8 ┆ 7.3 ┆ 7.7 ┆ 8.1 ┆ 8.5 ┆ 8.9 ┆ 9.2 ┆ 9.6 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 2.5 ┆ 3.2 ┆ 4.0 ┆ 5.0 ┆ 6.3 ┆ 7.7 ┆ 9.5 ┆ 1.1 ┆ 1.3 ┆ 1.5 ┆ 1.8 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 2.9 ┆ 3.2 ┆ 3.5 ┆ 3.7 ┆ 4.0 ┆ 4.3 ┆ 4.6 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_5 ┆ .36 ┆ .23 ┆ .26 ┆ .09 ┆ .16 ┆ .78 ┆ .83 ┆ .63 ┆ .78 ┆ .96 ┆ .25 ┆ .82 ┆ .08 ┆ .78 ┆ .04 ┆ .42 ┆ .25 ┆ .94 ┆ .00 ┆ .70 ┆ .26 ┆ .75 ┆ .20 ┆ .18 ┆ .39 ┆ .70 ┆ .12 ┆ .24 ┆ .35 ┆ .46 ┆ .44 ┆ .28 ┆ .96 ┆ .49 ┆ .89 ┆ .03 ┆ .10 ┆ .96 ┆ .65 ┆ .11 ┆ .15 ┆ .05 ┆ .85 ┆ .32 ┆ .62 ┆ .89 ┆ .97 ┆ .15 ┆ .20 ┆ .30 ┆ .42 ┆ .16 ┆ .94 ┆ .71 ┆ .54 ┆ .33 ┆ .22 ┆ .07 ┆ .77 ┆ .55 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 003 ┆ 004 ┆ 006 ┆ 008 ┆ 011 ┆ 014 ┆ 017 ┆ 021 ┆ 026 ┆ 031 ┆ 036 ┆ 043 ┆ 049 ┆ 055 ┆ 062 ┆ 068 ┆ 072 ┆ 078 ┆ 082 ┆ 086 ┆ 089 ┆ 093 ┆ 099 ┆ 105 ┆ 109 ┆ 113 ┆ 118 ┆ 123 ┆ 128 ┆ 135 ┆ 140 ┆ 233 ┆ 757 ┆ 398 ┆ 217 ┆ 038 ┆ 493 ┆ 177 ┆ 361 ┆ 593 ┆ 227 ┆ 220 ┆ 365 ┆ 856 ┆ 280 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 922 ┆ 044 ┆ 520 ┆     ┆ 843 ┆ 492 ┆ 226 ┆ 281 ┆ 019 ┆     ┆     ┆     ┆     ┆     ┆     ┆ 582 ┆ 312 ┆ 835 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 578 ┆ 606 ┆ 582 ┆ 890 ┆ 706 ┆ 204 ┆ 300 ┆ 235 ┆ 381 ┆ 958 ┆ 832 ┆ 482 ┆ 615 ┆ 417 ┆ 347 ┆ 363 ┆ 001 ┆ 279 ┆ 878 ┆ 028 ┆ 160 ┆ 728 ┆ 177 ┆ 076 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 687 ┆     ┆     ┆     ┆ 779 ┆ 253 ┆ 758 ┆ 760 ┆ 158 ┆ 672 ┆ 495 ┆ 108 ┆ 630 ┆ 928 ┆ 143 ┆ 101 ┆ 511 ┆ 275 ┆ 314 ┆ 337 ┆ 377 ┆ 262 ┆ 513 ┆ 035 ┆ 053 ┆ 164 ┆ 425 ┆ 639 ┆ 122 ┆ .89 ┆ .55 ┆ .52 ┆ .92 ┆ .22 ┆ .48 ┆ .54 ┆ .39 ┆ .39 ┆ .17 ┆ 241 ┆ 019 ┆ 689 ┆ 477 ┆ 205 ┆ 489 ┆ 426 ┆ 032 ┆ 706 ┆ 965 ┆ 166 ┆ 408 ┆ 771 ┆ 556 ┆ 808 ┆ 882 ┆ 846 ┆ 652 ┆ 302 ┆ 846 ┆ 259 ┆ 670 ┆ 183 ┆ 844 ┆ 700 ┆ 666 ┆ 707 ┆ 681 ┆ 583 ┆ 460 ┆ 467 ┆ 563 ┆ 787 ┆ 096 ┆ 708 ┆ 975 ┆ 348 ┆ 660 ┆ 499 ┆ 114 ┆ 307 ┆ 740 ┆ 488 ┆ 444 ┆ 647 ┆ 166 ┆ 894 ┆ 547 ┆ 328 ┆ 686 ┆ 074 ┆ 275 ┆ 541 ┆ 704 ┆ 513 ┆ 006 ┆ 131 ┆ 246 ┆ 470 ┆ 047 ┆ 371 ┆ 492 ┆ 349 ┆ 964 ┆ 863 ┆ 750 ┆ 837 ┆ 104 ┆ 737 ┆ 737 ┆ 835 ┆ 703 ┆ 064 ┆ 173 ┆ 643 ┆ 644 ┆ 868 ┆ 449 ┆ 246 ┆ 894 ┆ 633 ┆ 878 ┆ 884 ┆ 098 ┆ 545 ┆ 566 ┆ 027 ┆ 450 ┆ 163 ┆ 911 ┆ 929 ┆ 107 ┆ 034 ┆ 223 ┆ 304 ┆ 306 ┆ 312 ┆ 318 ┆ 329 ┆ 361 ┆ 399 ┆ 439 ┆ 485 ┆ 515 ┆ 548 ┆ 609 ┆ 656 ┆ 768 ┆ 045 ┆ 761 ┆ 156 ┆     ┆ 900 ┆ 169 ┆ 011 ┆ 052 ┆     ┆     ┆     ┆     ┆     ┆ .97 ┆     ┆     ┆     ┆     ┆ 128 ┆ 059 ┆ 348 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 761 ┆ 808 ┆ 513 ┆ 502 ┆ 848 ┆ 772 ┆ 575 ┆ 547 ┆ 658 ┆ 913 ┆ 041 ┆ 865 ┆ 947 ┆ 412 ┆ 084 ┆ 747 ┆ 495 ┆ 212 ┆ 667 ┆ 916 ┆ 175 ┆ 445 ┆ 750 ┆ 134 ┆ 600 ┆ 114 ┆ 698 ┆ 400 ┆ 220 ┆ 122 ┆ 149 ┆ 305 ┆ 580 ┆ 174 ┆ 576 ┆ 994 ┆ 419 ┆ 844 ┆ 475 ┆ 160 ┆ 842 ┆ 615 ┆ 823 ┆ 321 ┆ 165 ┆ 373 ┆ 945 ┆ 855 ┆ 049 ┆ 448 ┆ 970 ┆ 543 ┆ 118 ┆ 671 ┆ 193 ┆ 671 ┆ 082 ┆ 389 ┆ 552 ┆ 543 ┆ 351 ┆ 983 ┆ 453 ┆ 771 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 728 ┆ 356 ┆ 614 ┆ 801 ┆ 180 ┆ 926 ┆ 069 ┆ 448 ┆ 589 ┆ 899 ┆ 351 ┆ 928 ┆ 618 ┆ 408 ┆ 275 ┆ 186 ┆ 102 ┆ 992 ┆ 832 ┆ 617 ┆ 344 ┆ 013 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 248 ┆ 610 ┆ 871 ┆ 501 ┆ 977 ┆ 427 ┆ 321 ┆ 538 ┆ 617 ┆ 346 ┆ 162 ┆ 401 ┆ 132 ┆ 941 ┆ 173 ┆ 472 ┆ 873 ┆ 856 ┆ 598 ┆ 837 ┆ 875 ┆ 036 ┆ 988 ┆ 474 ┆ 826 ┆ 103 ┆ 9   ┆ 444 ┆ 439 ┆ 492 ┆ 673 ┆ 084 ┆ 031 ┆ 263 ┆ 178 ┆ 060 ┆ 806 ┆ 518 ┆ 934 ┆ 106 ┆ 589 ┆ 630 ┆ 413 ┆ 929 ┆ 368 ┆ 552 ┆ 540 ┆ 650 ┆ 341 ┆ 756 ┆ 452 ┆ 289 ┆ 537 ┆ 861 ┆ 026 ┆ 178 ┆ 858 ┆ 891 ┆ 505 ┆ 240 ┆ 279 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 05  ┆ 08  ┆ 12  ┆ 19  ┆ 25  ┆ 32  ┆ 45  ┆ 75  ┆ 42  ┆ 44  ┆ 56  ┆ 84  ┆ 32  ┆ 44  ┆ 19  ┆ 35  ┆ 96  ┆ 97  ┆ 35  ┆ 62  ┆ 62  ┆ 97  ┆ 92  ┆ 98  ┆ 73  ┆ 98  ┆ 66  ┆ 11  ┆ 64  ┆ 04  ┆ 09  ┆ 54  ┆ 41  ┆ 09  ┆ 07  ┆ 43  ┆ 47  ┆     ┆ 84  ┆ 99  ┆ 58  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-4 ┆ e-3 ┆ e-3 ┆     ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-2 ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-8 ┆ e-7 ┆ 02  ┆ 12  ┆ 37  ┆ 2   ┆ 14  ┆ 33  ┆ 27  ┆ 29  ┆ 05  ┆ 07  ┆ 22  ┆ 28  ┆ 15  ┆ 13  ┆ 07  ┆ 02  ┆ 01  ┆ 03  ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-8 ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-7 ┆     ┆     ┆     ┆ e-1 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-9 ┆ e-8 ┆ e-9 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ 336 ┆ 484 ┆ 252 ┆ 828 ┆ 378 ┆ 342 ┆ 360 ┆ 817 ┆ 236 ┆ 546 ┆ 326 ┆ 894 ┆ 72  ┆ 9   ┆ 04  ┆ 345 ┆ 973 ┆ 316 ┆ 922 ┆ 752 ┆ 77  ┆ 526 ┆ 722 ┆ 432 ┆ 454 ┆ 596 ┆ 462 ┆ 634 ┆ 33  ┆ 441 ┆ 796 ┆ 197 ┆ 674 ┆ 425 ┆ 619 ┆ 287 ┆ 244 ┆ 082 ┆ 848 ┆ 464 ┆ 512 ┆ 934 ┆ 628 ┆ 707 ┆ 2   ┆ 09  ┆ 15  ┆ 36  ┆ 28  ┆ 81  ┆ 5   ┆ 11  ┆ 27  ┆ 43  ┆ 2   ┆ 59  ┆ 4   ┆ 72  ┆ 37  ┆ 85  ┆ 41  ┆ 05  ┆ 661 ┆ 68  ┆ 83  ┆ 54  ┆ 24  ┆ 51  ┆ 4   ┆ 635 ┆ 438 ┆ 79  ┆ 26  ┆ 84  ┆ 53  ┆ 17  ┆ 25  ┆ 77  ┆ 43  ┆ 74  ┆ 96  ┆ 92  ┆ 93  ┆ 56  ┆ 39  ┆ 43  ┆ 36  ┆ 48  ┆ 17  ┆ 4   ┆ 17  ┆ 5   ┆ 59  ┆ 8   ┆ 11  ┆ 92  ┆ 01  ┆ 96  ┆ 15  ┆ 56  ┆ 47  ┆ 38  ┆ 511 ┆ 913 ┆ 56  ┆ 024 ┆ 83  ┆ 147 ┆ 014 ┆ 018 ┆ 833 ┆ 263 ┆ 964 ┆ 237 ┆ 946 ┆ 114 ┆ 643 ┆ 443 ┆ 759 ┆ 068 ┆ .33 ┆     ┆ 58  ┆ 037 ┆ 148 ┆ 13  ┆     ┆     ┆     ┆     ┆     ┆ 127 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 01  ┆ 03  ┆ 04  ┆ 07  ┆ 12  ┆ 13  ┆ 14  ┆ 13  ┆ 12  ┆ 1   ┆ 07  ┆ 05  ┆ 03  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 62  ┆ 1   ┆ 1   ┆ 7   ┆ 2   ┆     ┆ 1   ┆ 4   ┆ 2   ┆ 7   ┆ 5   ┆ 8   ┆ 8   ┆ 6   ┆ 6   ┆ 9   ┆ 8   ┆ 9   ┆ 8   ┆ 7   ┆ 1   ┆ 9   ┆ 7   ┆ 6   ┆ 4   ┆ 8   ┆     ┆ 9   ┆     ┆ 7   ┆ 2   ┆ 3   ┆ 8   ┆ 2   ┆ 4   ┆ 7   ┆ 2   ┆ 3   ┆ 5   ┆ 7   ┆ 5   ┆ 4   ┆     ┆ 9   ┆ 3   ┆ 6   ┆ 6   ┆ 3   ┆ 9   ┆ 3   ┆ 7   ┆ 1   ┆ 3   ┆ 9   ┆ 5   ┆ 6   ┆ 2   ┆ 7   ┆ 1   ┆ 5   ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 8   ┆ 3   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 5   ┆ 0   ┆     ┆ 1   ┆ 7   ┆ 8   ┆ 8   ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 2   ┆ 7   ┆ 5   ┆ 5   ┆ 0   ┆ 4   ┆ 4   ┆ 2   ┆ 0   ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 1   ┆ 2   ┆ 0   ┆ 2   ┆ 2   ┆ 2   ┆ 4   ┆ 3   ┆ 1   ┆ 4   ┆ 4   ┆ 9   ┆ 7   ┆     ┆ 6   ┆ 9   ┆ 8   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 813 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 213 ┆ 229 ┆ 233 ┆ 242 ┆ 252 ┆ 260 ┆ 260 ┆ 253 ┆ 240 ┆ 232 ┆ 226 ┆ 221 ┆ 217 ┆ 213 ┆ 206 ┆ 197 ┆ 190 ┆ 189 ┆ 191 ┆ 192 ┆ 198 ┆ 204 ┆ 210 ┆ 216 ┆ 222 ┆ 228 ┆ 233 ┆ 238 ┆ 243 ┆ 247 ┆ 251 ┆ 255 ┆ 258 ┆ 261 ┆ 265 ┆ 268 ┆ 270 ┆ 272 ┆ 275 ┆ 277 ┆ 279 ┆ 280 ┆ 281 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 292 ┆ 293 ┆ 294 ┆ 295 ┆ 296 ┆ 297 ┆ 298 ┆ 299 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.7 ┆ 9.6 ┆ 9.4 ┆ 8.6 ┆ 7.4 ┆ 7.0 ┆ 5.8 ┆ 3.7 ┆ 1.8 ┆ 2.6 ┆ 1.5 ┆ 4.6 ┆ 2.0 ┆ 1.7 ┆ 6.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 7.5 ┆ 1.6 ┆ 2.8 ┆ 1.5 ┆ 5.5 ┆ 1.2 ┆ 8.0 ┆ 2.1 ┆ 5.8 ┆ 1.3 ┆ 3.2 ┆ 8.0 ┆ 2.9 ┆ 1.7 ┆ 2.0 ┆ 0.0 ┆ 0.0 ┆ 6.1 ┆ 5.5 ┆ 5.5 ┆ 8.9 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.7 ┆ 1.5 ┆ 1.9 ┆ 1.7 ┆ 3.1 ┆ 3.5 ┆ 3.9 ┆ 4.6 ┆ 4.7 ┆ 4.9 ┆ 4.6 ┆ 3.4 ┆ 2.3 ┆ 6.1 ┆ 2.7 ┆ 7.5 ┆ 2.6 ┆ 2.6 ┆ 5.1 ┆ 2.6 ┆ 9.8 ┆ 3.8 ┆ 1.0 ┆ 1.0 ┆ 1.5 ┆ 1.9 ┆ 6.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.3 ┆ 5.3 ┆ 2.7 ┆ 2.3 ┆ 2.4 ┆ 1.2 ┆ 1.0 ┆ 0.0 ┆ 1.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.8 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 16. ┆ -11 ┆ -24 ┆ -20 ┆ -37 ┆ -43 ┆ -43 ┆ -41 ┆ -38 ┆ -33 ┆ -25 ┆ -18 ┆ -14 ┆ -9. ┆ -2. ┆ 1.0 ┆ 5.2 ┆ 10. ┆ 13. ┆ 17. ┆ 22. ┆ 27. ┆ 28. ┆ 27. ┆ 24. ┆ 20. ┆ 16. ┆ 13. ┆ 10. ┆ 8.3 ┆ 6.7 ┆ 5.5 ┆ 4.3 ┆ 3.1 ┆ 2.0 ┆ 0.9 ┆ 0.0 ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -3. ┆ -3. ┆ -3. ┆ -4. ┆ -4. ┆ -5. ┆ -5. ┆ -5. ┆ -5. ┆ -6. ┆ -6. ┆ -6. ┆ -6. ┆ -6. ┆ -6. ┆ -5. ┆ 20. ┆ 5.2 ┆ 4.4 ┆ -3. ┆ -5. ┆ -0. ┆ 5.0 ┆ 5.3 ┆ 1.7 ┆ 0.7 ┆ 2.0 ┆ 1.6 ┆ 0.5 ┆ 2.8 ┆ 3.7 ┆ 3.0 ┆ 2.6 ┆ 1.5 ┆ -0. ┆ -2. ┆ -4. ┆ -5. ┆ -6. ┆ -5. ┆ -3. ┆ -2. ┆ -1. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -1. ┆ -1. ┆ -0. ┆ -0. ┆ 0.2 ┆ 0.3 ┆ 0.3 ┆ 0.4 ┆ 0.5 ┆ 0.7 ┆ 0.8 ┆ 0.9 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 0.9 ┆ 0.9 ┆ 0.8 ┆ 0.8 ┆ 0.7 ┆ 0.7 ┆ 0.7 ┆ 0.7 ┆ 0.7 ┆ 0.7 ┆ 0.8 ┆ 991 ┆ 0.0 ┆ 70. ┆ -0. ┆ 0.0 ┆ -0. ┆ 0.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 455 ┆ 0.0 ┆ 0.2 ┆ 0.7 ┆ 0.0 ┆ 2.7 ┆ 4.9 ┆ 8.8 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 5.4 ┆ 3.2 ┆ 2.1 ┆ 1.4 ┆ 1.0 ┆ 8.7 ┆ 7.7 ┆ 7.4 ┆ 7.2 ┆ 7.1 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 6.9 ┆ 6.7 ┆ 6.6 ┆ 6.4 ┆ 6.3 ┆ 6.1 ┆ 5.8 ┆ 5.4 ┆ 5.2 ┆ 4.8 ┆ 4.4 ┆ 4.1 ┆ 3.7 ┆ 3.5 ┆ 3.3 ┆ 3.1 ┆ 2.9 ┆ 2.8 ┆ 2.7 ┆ 2.6 ┆ 2.6 ┆ 2.5 ┆ 2.4 ┆ 2.4 ┆ 2.4 ┆ 2.4 ┆ 1.7 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 3.0 ┆ 3.4 ┆ 3.8 ┆ 4.3 ┆ 4.8 ┆ 5.3 ┆ 5.7 ┆ 6.2 ┆ 6.7 ┆ 7.2 ┆ 7.6 ┆ 8.1 ┆ 8.5 ┆ 9.0 ┆ 9.4 ┆ 9.7 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.1 ┆ 3.9 ┆ 4.8 ┆ 6.0 ┆ 7.4 ┆ 9.0 ┆ 1.1 ┆ 1.3 ┆ 1.5 ┆ 1.8 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 2.9 ┆ 3.2 ┆ 3.5 ┆ 3.8 ┆ 4.1 ┆ 4.4 ┆ 4.7 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_6 ┆ .07 ┆ .42 ┆ .36 ┆ .68 ┆ .91 ┆ .95 ┆ .97 ┆ .05 ┆ .92 ┆ .91 ┆ .80 ┆ .93 ┆ .64 ┆ .24 ┆ .34 ┆ .76 ┆ .74 ┆ .53 ┆ .34 ┆ .50 ┆ .06 ┆ .08 ┆ .65 ┆ .96 ┆ .76 ┆ .26 ┆ .52 ┆ .41 ┆ .08 ┆ .40 ┆ .55 ┆ .27 ┆ .69 ┆ .98 ┆ .12 ┆ .07 ┆ .62 ┆ .98 ┆ .17 ┆ .32 ┆ .23 ┆ .73 ┆ .94 ┆ .24 ┆ .54 ┆ .84 ┆ .92 ┆ .87 ┆ .97 ┆ .86 ┆ .71 ┆ .69 ┆ .54 ┆ .17 ┆ .03 ┆ .01 ┆ .04 ┆ .15 ┆ .15 ┆ .14 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 001 ┆ 003 ┆ 004 ┆ 006 ┆ 007 ┆ 010 ┆ 011 ┆ 013 ┆ 014 ┆ 017 ┆ 021 ┆ 027 ┆ 033 ┆ 038 ┆ 043 ┆ 050 ┆ 062 ┆ 075 ┆ 083 ┆ 090 ┆ 098 ┆ 107 ┆ 114 ┆ 121 ┆ 128 ┆ 131 ┆ 135 ┆ 143 ┆ 150 ┆ 155 ┆ 159 ┆ 162 ┆ 166 ┆ 173 ┆ 196 ┆ 819 ┆ 737 ┆ 130 ┆ 553 ┆ 430 ┆ 079 ┆ 533 ┆ 347 ┆ 609 ┆ 455 ┆ 986 ┆ 639 ┆ 214 ┆ 973 ┆     ┆     ┆     ┆     ┆     ┆ 323 ┆ 774 ┆ 654 ┆ 840 ┆ 806 ┆ 908 ┆ 081 ┆ 682 ┆ 577 ┆ 245 ┆ 243 ┆ 364 ┆ 410 ┆ 317 ┆ 491 ┆ 769 ┆ 000 ┆ 000 ┆ 060 ┆ 421 ┆ 637 ┆ 581 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 045 ┆ 553 ┆ 860 ┆ 795 ┆ 575 ┆ 242 ┆ 756 ┆ 787 ┆ 899 ┆ 267 ┆ 184 ┆ 904 ┆ 260 ┆ 475 ┆ 373 ┆ 081 ┆ 179 ┆ 664 ┆ 042 ┆ 412 ┆ 686 ┆ 788 ┆ 422 ┆ 476 ┆ 938 ┆ 242 ┆ 540 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 136 ┆ 000 ┆ 000 ┆ 000 ┆ 892 ┆ 047 ┆ 988 ┆ 266 ┆ 639 ┆ 039 ┆ 484 ┆     ┆ 023 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 992 ┆     ┆     ┆     ┆     ┆     ┆ 386 ┆ .51 ┆ .81 ┆ .79 ┆ .66 ┆ .57 ┆ .41 ┆ .31 ┆ .49 ┆ .17 ┆ .49 ┆ .30 ┆ .22 ┆ 429 ┆ 728 ┆ 835 ┆ 465 ┆ 596 ┆ 751 ┆ 168 ┆ 897 ┆ 350 ┆ 289 ┆ 581 ┆ 350 ┆ 201 ┆ 367 ┆ 093 ┆ 395 ┆ 392 ┆ 831 ┆ 107 ┆ 299 ┆ 965 ┆ 718 ┆ 491 ┆ 713 ┆ 575 ┆ 958 ┆ 161 ┆ 364 ┆ 642 ┆ 055 ┆ 538 ┆ 052 ┆ 529 ┆ 950 ┆ 343 ┆ 695 ┆ 011 ┆ 323 ┆ 662 ┆ 971 ┆ 224 ┆ 444 ┆ 645 ┆ 783 ┆ 794 ┆ 425 ┆ 456 ┆ 983 ┆ 496 ┆ 796 ┆ 157 ┆ 787 ┆ 279 ┆ 393 ┆ 442 ┆ 703 ┆ 143 ┆ 770 ┆ 394 ┆ 899 ┆ 069 ┆ 287 ┆ 329 ┆ 877 ┆ 516 ┆ 080 ┆ 551 ┆ 148 ┆ 943 ┆ 167 ┆ 431 ┆ 508 ┆ 183 ┆ 755 ┆ 703 ┆ 861 ┆ 088 ┆ 089 ┆ 826 ┆ 271 ┆ 607 ┆ 032 ┆ 874 ┆ 641 ┆ 894 ┆ 369 ┆ 545 ┆ 255 ┆ 768 ┆ 852 ┆ 269 ┆ 470 ┆ 802 ┆ 904 ┆ 642 ┆ 271 ┆ 947 ┆ 456 ┆ 698 ┆ 139 ┆ 656 ┆ 399 ┆ 221 ┆ 251 ┆ 586 ┆ 801 ┆ 218 ┆ 86. ┆     ┆ 755 ┆ 892 ┆ 332 ┆ 005 ┆     ┆     ┆     ┆     ┆     ┆ .79 ┆     ┆ 981 ┆ 018 ┆     ┆ 551 ┆ 620 ┆ 940 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 335 ┆ 919 ┆ 876 ┆ 950 ┆ 933 ┆ 582 ┆ 536 ┆ 163 ┆ 076 ┆ 080 ┆ 781 ┆ 635 ┆ 695 ┆ 802 ┆ 773 ┆ 642 ┆ 367 ┆ 130 ┆ 862 ┆ 464 ┆ 919 ┆ 383 ┆ 233 ┆ 041 ┆ 994 ┆ 104 ┆ 473 ┆ 674 ┆ 062 ┆ 635 ┆ 023 ┆ 181 ┆ 460 ┆ 850 ┆ 331 ┆ 395 ┆ 727 ┆ 068 ┆ 409 ┆ 823 ┆ 685 ┆ 545 ┆ 471 ┆ 700 ┆ 328 ┆ 320 ┆ 696 ┆ 454 ┆ 568 ┆ 980 ┆ 609 ┆ 367 ┆ 178 ┆ 992 ┆ 782 ┆ 540 ┆ 251 ┆ 892 ┆ 424 ┆ 804 ┆ 003 ┆ 009 ┆ 831 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 350 ┆ 069 ┆ 601 ┆ 254 ┆ 293 ┆ 874 ┆ 000 ┆ 148 ┆ 501 ┆ 024 ┆ 686 ┆ 469 ┆ 359 ┆ 342 ┆ 395 ┆ 483 ┆ 566 ┆ 609 ┆ 593 ┆ 509 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 341 ┆ 377 ┆ 155 ┆ 845 ┆ 378 ┆ 278 ┆ 398 ┆ 731 ┆ 988 ┆ 989 ┆ 791 ┆ 359 ┆ 733 ┆ 686 ┆ 573 ┆ 662 ┆ 253 ┆ 862 ┆ 427 ┆ 047 ┆ 400 ┆ 498 ┆ 369 ┆ 925 ┆ 595 ┆ 587 ┆ 443 ┆ 828 ┆ 498 ┆ 483 ┆ 786 ┆ 908 ┆ 033 ┆ 243 ┆ 807 ┆ 583 ┆ 896 ┆ 784 ┆ 131 ┆ 815 ┆ 678 ┆ 369 ┆ 774 ┆ 689 ┆ 825 ┆ 198 ┆ 623 ┆ 888 ┆ 416 ┆ 811 ┆ 487 ┆ 582 ┆ 913 ┆ 741 ┆ 027 ┆ 824 ┆ 840 ┆ 993 ┆ 961 ┆ 971 ┆ 650 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 01  ┆ 01  ┆ 01  ┆ 02  ┆ 04  ┆ 09  ┆ 23  ┆ 49  ┆ 79  ┆ 29  ┆ 97  ┆ 01  ┆ 21  ┆ 15  ┆ 99  ┆ 1   ┆ 95  ┆ 34  ┆ 78  ┆ 18  ┆ 79  ┆ 6   ┆ 17  ┆ 39  ┆ 86  ┆ 77  ┆ 77  ┆ 19  ┆ 27  ┆ 56  ┆ 29  ┆ 64  ┆ 35  ┆ 43  ┆ 5   ┆ 19  ┆     ┆ 71  ┆ 46  ┆ 51  ┆ 51  ┆ 06  ┆ 89  ┆ 09  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-4 ┆ e-3 ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ 03  ┆ 04  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ 06  ┆ 04  ┆ 03  ┆ 04  ┆ 06  ┆ 75  ┆ 17  ┆ 14  ┆ 18  ┆ 07  ┆ 04  ┆ 03  ┆ 02  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-9 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-8 ┆ e-7 ┆ e-7 ┆ 02  ┆ 02  ┆ 06  ┆ 09  ┆ 09  ┆ 06  ┆ e-7 ┆ 03  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-9 ┆ e-9 ┆     ┆ e-9 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆ 828 ┆ 245 ┆ 713 ┆ 656 ┆ 945 ┆ 750 ┆ 409 ┆ 412 ┆ 889 ┆ 397 ┆ 320 ┆ 158 ┆ 911 ┆ 682 ┆ 357 ┆ 7   ┆ 48  ┆ 983 ┆ 444 ┆ 239 ┆ 938 ┆ 709 ┆ 143 ┆ 126 ┆ 959 ┆ 542 ┆ 193 ┆ 764 ┆ 265 ┆ 74  ┆ 11  ┆ 04  ┆ 76  ┆ 51  ┆ 99  ┆ 27  ┆ 86  ┆ 528 ┆ 757 ┆ 323 ┆ 285 ┆ 227 ┆ 358 ┆ 765 ┆ 665 ┆ 975 ┆ 433 ┆ 366 ┆ 296 ┆ 322 ┆ 996 ┆ 009 ┆ 492 ┆ 755 ┆ 167 ┆ 438 ┆ 85  ┆ 718 ┆ 409 ┆ 625 ┆ 392 ┆ 65  ┆ 47  ┆ 389 ┆ 299 ┆ 474 ┆ 68  ┆ 92  ┆ 46  ┆ 79  ┆ 56  ┆ 21  ┆ 09  ┆ 33  ┆ 32  ┆ 48  ┆ 94  ┆ 57  ┆ 055 ┆ 87  ┆ 039 ┆ 878 ┆ 202 ┆ 529 ┆ 39  ┆ 702 ┆ 617 ┆ 193 ┆ 061 ┆ 447 ┆ 375 ┆ 21  ┆ 17  ┆ 347 ┆ 128 ┆ 5   ┆ 68  ┆ 45  ┆ 52  ┆ 93  ┆ 87  ┆ 4   ┆ 13  ┆ 65  ┆ 64  ┆ 41  ┆ 79  ┆ 47  ┆ 41  ┆ 94  ┆ 11  ┆ 93  ┆ 89  ┆ 46  ┆     ┆ 11  ┆ 07  ┆ 54  ┆     ┆ 63  ┆ 272 ┆     ┆ 197 ┆ 743 ┆ 05  ┆ 67  ┆     ┆     ┆     ┆     ┆     ┆ 343 ┆     ┆ 89  ┆ 11  ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 08  ┆ 13  ┆ 14  ┆ 15  ┆ 15  ┆ 13  ┆ 1   ┆ 07  ┆ 04  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 29  ┆ 1   ┆ 1   ┆ 5   ┆ 8   ┆ 6   ┆ 1   ┆     ┆ 5   ┆     ┆     ┆ 8   ┆ 3   ┆     ┆ 5   ┆ 7   ┆ 2   ┆ 6   ┆ 8   ┆ 2   ┆ 4   ┆ 5   ┆ 9   ┆ 6   ┆ 4   ┆ 2   ┆ 2   ┆ 4   ┆     ┆ 9   ┆ 4   ┆ 2   ┆     ┆ 9   ┆ 5   ┆     ┆ 4   ┆ 9   ┆ 7   ┆ 8   ┆ 4   ┆ 8   ┆ 9   ┆ 3   ┆     ┆ 1   ┆ 7   ┆ 6   ┆ 4   ┆     ┆ 5   ┆ 2   ┆ 3   ┆ 2   ┆ 4   ┆ 1   ┆ 6   ┆ 9   ┆ 5   ┆ 9   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 8   ┆ 8   ┆ 9   ┆ 2   ┆ 8   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 8   ┆ 2   ┆ 7   ┆ 2   ┆ 8   ┆ 4   ┆ 0   ┆ 5   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 2   ┆ 2   ┆ 3   ┆ 6   ┆ 2   ┆ 5   ┆ 8   ┆ 4   ┆ 1   ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆ 8   ┆ 1   ┆ 3   ┆ 4   ┆ 6   ┆ 6   ┆ 9   ┆ 9   ┆ 3   ┆ 9   ┆ 5   ┆ 8   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 487 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 212 ┆ 226 ┆ 230 ┆ 241 ┆ 252 ┆ 262 ┆ 261 ┆ 253 ┆ 241 ┆ 233 ┆ 227 ┆ 222 ┆ 218 ┆ 214 ┆ 208 ┆ 201 ┆ 194 ┆ 191 ┆ 191 ┆ 193 ┆ 198 ┆ 204 ┆ 210 ┆ 215 ┆ 221 ┆ 226 ┆ 231 ┆ 236 ┆ 241 ┆ 245 ┆ 249 ┆ 253 ┆ 257 ┆ 260 ┆ 263 ┆ 266 ┆ 269 ┆ 271 ┆ 274 ┆ 276 ┆ 278 ┆ 279 ┆ 281 ┆ 282 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 292 ┆ 293 ┆ 293 ┆ 294 ┆ 295 ┆ 296 ┆ 297 ┆ 298 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.7 ┆ 9.6 ┆ 9.4 ┆ 8.5 ┆ 7.7 ┆ 7.7 ┆ 6.7 ┆ 4.5 ┆ 2.4 ┆ 5.1 ┆ 2.5 ┆ 6.4 ┆ 8.9 ┆ 1.0 ┆ 5.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.6 ┆ 2.2 ┆ 6.9 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.0 ┆ 6.2 ┆ 1.6 ┆ 4.5 ┆ 1.0 ┆ 1.4 ┆ 7.4 ┆ 7.5 ┆ 3.2 ┆ 9.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.8 ┆ 2.0 ┆ 1.8 ┆ 3.1 ┆ 3.4 ┆ 3.9 ┆ 4.7 ┆ 4.9 ┆ 5.1 ┆ 5.1 ┆ 4.1 ┆ 2.8 ┆ 1.0 ┆ 4.0 ┆ 1.0 ┆ 7.9 ┆ 1.2 ┆ 1.6 ┆ 0.0 ┆ 2.6 ┆ 8.6 ┆ 1.6 ┆ 7.3 ┆ 5.2 ┆ 1.7 ┆ 1.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.1 ┆ 6.5 ┆ 0.0 ┆ 3.9 ┆ 2.4 ┆ 6.2 ┆ 1.5 ┆ 4.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ -0. ┆ -29 ┆ -34 ┆ -26 ┆ -32 ┆ -37 ┆ -40 ┆ -38 ┆ -34 ┆ -29 ┆ -23 ┆ -16 ┆ -9. ┆ -5. ┆ 0.2 ┆ 6.7 ┆ 11. ┆ 17. ┆ 21. ┆ 26. ┆ 31. ┆ 32. ┆ 32. ┆ 30. ┆ 27. ┆ 24. ┆ 22. ┆ 19. ┆ 17. ┆ 15. ┆ 13. ┆ 11. ┆ 9.9 ┆ 8.1 ┆ 6.5 ┆ 5.2 ┆ 4.2 ┆ 3.4 ┆ 2.7 ┆ 2.0 ┆ 1.3 ┆ 0.7 ┆ 0.2 ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ 13. ┆ 4.8 ┆ 15. ┆ 4.3 ┆ -4. ┆ -1. ┆ 4.3 ┆ 5.3 ┆ 1.5 ┆ -0. ┆ 1.3 ┆ 2.5 ┆ 0.5 ┆ 0.2 ┆ 3.3 ┆ 3.9 ┆ 4.2 ┆ 4.1 ┆ 4.0 ┆ 4.2 ┆ 4.6 ┆ 5.8 ┆ 6.4 ┆ 6.6 ┆ 6.0 ┆ 4.9 ┆ 4.0 ┆ 3.4 ┆ 3.0 ┆ 2.8 ┆ 2.8 ┆ 2.7 ┆ 2.5 ┆ 2.1 ┆ 1.6 ┆ 1.1 ┆ 0.8 ┆ 0.6 ┆ 0.5 ┆ 0.5 ┆ 0.4 ┆ 0.2 ┆ 0.0 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ 101 ┆ 0.0 ┆ 80. ┆ 5.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 460 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 2.7 ┆ 4.9 ┆ 8.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 5.7 ┆ 3.5 ┆ 2.5 ┆ 1.8 ┆ 1.3 ┆ 9.8 ┆ 8.0 ┆ 7.5 ┆ 7.2 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.1 ┆ 7.1 ┆ 7.2 ┆ 7.2 ┆ 7.1 ┆ 7.1 ┆ 7.0 ┆ 6.8 ┆ 6.6 ┆ 6.5 ┆ 6.2 ┆ 5.9 ┆ 5.5 ┆ 5.2 ┆ 4.7 ┆ 4.2 ┆ 3.7 ┆ 3.3 ┆ 3.1 ┆ 3.0 ┆ 2.9 ┆ 2.8 ┆ 2.7 ┆ 2.6 ┆ 2.6 ┆ 2.5 ┆ 2.5 ┆ 2.4 ┆ 2.4 ┆ 2.4 ┆ 2.4 ┆ 1.7 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 3.0 ┆ 3.4 ┆ 3.8 ┆ 4.3 ┆ 4.7 ┆ 5.2 ┆ 5.7 ┆ 6.2 ┆ 6.6 ┆ 7.1 ┆ 7.6 ┆ 8.0 ┆ 8.4 ┆ 8.9 ┆ 9.3 ┆ 9.6 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.0 ┆ 3.7 ┆ 4.6 ┆ 5.8 ┆ 7.1 ┆ 8.8 ┆ 1.0 ┆ 1.2 ┆ 1.5 ┆ 1.7 ┆ 2.0 ┆ 2.2 ┆ 2.5 ┆ 2.8 ┆ 3.1 ┆ 3.4 ┆ 3.7 ┆ 4.0 ┆ 4.3 ┆ 4.6 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_4 ┆ .04 ┆ .00 ┆ .98 ┆ .05 ┆ .82 ┆ .18 ┆ .90 ┆ .18 ┆ .18 ┆ .42 ┆ .55 ┆ .87 ┆ .35 ┆ .29 ┆ .69 ┆ .11 ┆ .08 ┆ .57 ┆ .43 ┆ .61 ┆ .68 ┆ .36 ┆ .18 ┆ .80 ┆ .31 ┆ .57 ┆ .73 ┆ .62 ┆ .20 ┆ .49 ┆ .65 ┆ .51 ┆ .17 ┆ .61 ┆ .84 ┆ .71 ┆ .36 ┆ .65 ┆ .01 ┆ .07 ┆ .18 ┆ .56 ┆ .09 ┆ .39 ┆ .09 ┆ .41 ┆ .76 ┆ .86 ┆ .82 ┆ .78 ┆ .64 ┆ .33 ┆ .20 ┆ .04 ┆ .90 ┆ .53 ┆ .25 ┆ .30 ┆ .49 ┆ .72 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 001 ┆ 002 ┆ 003 ┆ 003 ┆ 004 ┆ 006 ┆ 008 ┆ 011 ┆ 015 ┆ 021 ┆ 028 ┆ 035 ┆ 041 ┆ 047 ┆ 058 ┆ 070 ┆ 081 ┆ 087 ┆ 094 ┆ 099 ┆ 103 ┆ 107 ┆ 110 ┆ 116 ┆ 125 ┆ 131 ┆ 136 ┆ 143 ┆ 152 ┆ 160 ┆ 164 ┆ 165 ┆ 166 ┆ 321 ┆ 702 ┆ 260 ┆ 773 ┆ 302 ┆ 039 ┆ 762 ┆ 470 ┆ 676 ┆ 243 ┆ 684 ┆ 836 ┆ 827 ┆ 076 ┆ 435 ┆     ┆     ┆     ┆     ┆     ┆ 641 ┆ 396 ┆ 463 ┆     ┆     ┆     ┆ 829 ┆ 168 ┆ 750 ┆ 542 ┆ 455 ┆ 923 ┆ 443 ┆ 741 ┆ 564 ┆ 671 ┆ 000 ┆ 000 ┆ 000 ┆ 192 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 136 ┆ 159 ┆ 606 ┆ 498 ┆ 882 ┆ 500 ┆ 076 ┆ 022 ┆ 808 ┆ 092 ┆ 047 ┆ 809 ┆ 113 ┆ 288 ┆ 157 ┆ 655 ┆ 778 ┆ 981 ┆     ┆ 637 ┆ 123 ┆ 406 ┆ 944 ┆ 734 ┆ 848 ┆ 908 ┆ 000 ┆ 000 ┆ 000 ┆ 423 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 885 ┆ 565 ┆ 000 ┆ 603 ┆ 611 ┆ 521 ┆ 304 ┆ 849 ┆     ┆     ┆     ┆ 317 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 895 ┆ .09 ┆ .20 ┆ .29 ┆ .09 ┆ .21 ┆ .73 ┆ .77 ┆ .37 ┆ .95 ┆ .99 ┆ .31 ┆ 717 ┆ 701 ┆ 822 ┆ 108 ┆ 787 ┆ 137 ┆ 384 ┆ 894 ┆ 373 ┆ 788 ┆ 527 ┆ 910 ┆ 796 ┆ 692 ┆ 061 ┆ 825 ┆ 776 ┆ 818 ┆ 885 ┆ 877 ┆ 191 ┆ 203 ┆ 523 ┆ 702 ┆ 656 ┆ 455 ┆ 018 ┆ 102 ┆ 532 ┆ 505 ┆ 500 ┆ 209 ┆ 618 ┆ 001 ┆ 411 ┆ 795 ┆ 144 ┆ 465 ┆ 706 ┆ 893 ┆ 064 ┆ 239 ┆ 402 ┆ 687 ┆ 894 ┆ 934 ┆ 892 ┆ 790 ┆ 161 ┆ 682 ┆ 499 ┆ 046 ┆ 137 ┆ 489 ┆ 185 ┆ 869 ┆ 368 ┆ 516 ┆ 753 ┆ 564 ┆ 841 ┆ 866 ┆ 240 ┆ 947 ┆ 137 ┆ 468 ┆ 861 ┆ 755 ┆ 855 ┆ 381 ┆ 435 ┆ 910 ┆ 570 ┆ 677 ┆ 697 ┆ 033 ┆ 537 ┆ 736 ┆ 587 ┆ 637 ┆ 482 ┆ 332 ┆ 601 ┆ 544 ┆ 082 ┆ 200 ┆ 613 ┆ 176 ┆ 252 ┆ 577 ┆ 128 ┆ 239 ┆ 408 ┆ 510 ┆ 610 ┆ 685 ┆ 777 ┆ 891 ┆ 998 ┆ 148 ┆ 303 ┆ 469 ┆ 610 ┆ 783 ┆ 781 ┆ 651 ┆ 540 ┆ 441 ┆ 096 ┆     ┆ 041 ┆ 958 ┆ 204 ┆ 067 ┆     ┆     ┆     ┆     ┆     ┆ .81 ┆     ┆     ┆     ┆     ┆ 210 ┆ 007 ┆ 841 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 756 ┆ 455 ┆ 498 ┆ 384 ┆ 208 ┆ 328 ┆ 853 ┆ 393 ┆ 067 ┆ 594 ┆ 437 ┆ 536 ┆ 091 ┆ 728 ┆ 022 ┆ 139 ┆ 999 ┆ 021 ┆ 019 ┆ 628 ┆ 962 ┆ 306 ┆ 554 ┆ 008 ┆ 624 ┆ 410 ┆ 203 ┆ 258 ┆ 556 ┆ 094 ┆ 591 ┆ 384 ┆ 257 ┆ 202 ┆ 346 ┆ 792 ┆ 253 ┆ 721 ┆ 189 ┆ 827 ┆ 477 ┆ 131 ┆ 187 ┆ 527 ┆ 129 ┆ 092 ┆ 434 ┆ 156 ┆ 230 ┆ 598 ┆ 182 ┆ 893 ┆ 658 ┆ 424 ┆ 168 ┆ 878 ┆ 544 ┆ 140 ┆ 627 ┆ 964 ┆ 122 ┆ 089 ┆ 873 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 161 ┆ 656 ┆ 929 ┆ 286 ┆ 990 ┆ 206 ┆ 694 ┆ 801 ┆ 113 ┆ 595 ┆ 217 ┆ 960 ┆ 812 ┆ 760 ┆ 778 ┆ 833 ┆ 886 ┆ 902 ┆ 860 ┆ 753 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 035 ┆ 863 ┆ 386 ┆ 060 ┆ 618 ┆ 963 ┆ 577 ┆ 074 ┆ 719 ┆ 631 ┆ 780 ┆ 160 ┆ 035 ┆ 556 ┆ 387 ┆ 633 ┆ 957 ┆ 237 ┆ 424 ┆ 963 ┆ 432 ┆ 012 ┆ 157 ┆ 116 ┆ 456 ┆ 720 ┆ 277 ┆ 690 ┆ 780 ┆ 245 ┆ 183 ┆ 132 ┆ 150 ┆ 599 ┆ 964 ┆ 016 ┆ 538 ┆ 056 ┆ 341 ┆ 747 ┆ 255 ┆ 162 ┆ 668 ┆ 230 ┆ 622 ┆ 860 ┆ 697 ┆ 609 ┆ 135 ┆ 794 ┆ 314 ┆ 174 ┆ 108 ┆ 081 ┆ 151 ┆ 280 ┆ 867 ┆ 198 ┆ 872 ┆ 353 ┆ 311 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 06  ┆ 11  ┆ 22  ┆ 42  ┆ 61  ┆ 85  ┆ 1   ┆ 7   ┆ 44  ┆ 22  ┆ 95  ┆ 91  ┆ 27  ┆ 33  ┆ 11  ┆ 5   ┆ 22  ┆ 7   ┆ 17  ┆ 73  ┆ 84  ┆ 47  ┆ 88  ┆ 75  ┆ 92  ┆ 77  ┆ 74  ┆ 89  ┆ 89  ┆ 92  ┆ 04  ┆ 46  ┆ 01  ┆ 55  ┆ 22  ┆ 4   ┆ 34  ┆ 53  ┆ 92  ┆ 83  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-4 ┆     ┆     ┆     ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ 05  ┆ 03  ┆ 07  ┆ e-7 ┆ 08  ┆ 07  ┆ 73  ┆ 54  ┆ 11  ┆ 16  ┆ 2   ┆ 12  ┆ 05  ┆ 04  ┆ 01  ┆ 21  ┆ 26  ┆ 23  ┆ 17  ┆ 11  ┆ 03  ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-8 ┆ e-7 ┆ e-7 ┆ 03  ┆ 05  ┆ 03  ┆ e-7 ┆ 02  ┆ 05  ┆ 03  ┆ 02  ┆ e-7 ┆ e-8 ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆     ┆     ┆     ┆ e-9 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 688 ┆ 759 ┆ 382 ┆ 252 ┆ 222 ┆ 264 ┆ 699 ┆ 608 ┆ 860 ┆ 886 ┆ 579 ┆ 366 ┆ 851 ┆ 623 ┆ 12  ┆ 35  ┆ 517 ┆ 322 ┆ 466 ┆ 039 ┆ 947 ┆ 974 ┆ 44  ┆ 859 ┆ 946 ┆ 624 ┆ 817 ┆ 191 ┆ 414 ┆ 793 ┆ 651 ┆ 284 ┆ 14  ┆ 61  ┆ 6   ┆ 32  ┆ 99  ┆ 89  ┆ 8   ┆ 72  ┆ 32  ┆ 37  ┆ 07  ┆ 561 ┆ 921 ┆ 443 ┆ 117 ┆ 138 ┆ 735 ┆ 519 ┆ 805 ┆ 646 ┆ 147 ┆ 915 ┆ 748 ┆ 885 ┆ 445 ┆ 927 ┆ 879 ┆ 526 ┆ 184 ┆ 9   ┆ 55  ┆ 59  ┆ 783 ┆ 619 ┆ 81  ┆ 5   ┆ 53  ┆ 866 ┆ 16  ┆ 8   ┆ 75  ┆ 61  ┆ 75  ┆ 14  ┆ 56  ┆ 25  ┆ 54  ┆ 49  ┆ 94  ┆ 7   ┆ 13  ┆ 64  ┆ 19  ┆ 96  ┆ 45  ┆ 13  ┆ 14  ┆ 47  ┆ 52  ┆ 81  ┆ 21  ┆ 87  ┆ 12  ┆ 26  ┆ 14  ┆ 16  ┆ 83  ┆ 67  ┆ 16  ┆ 51  ┆ 08  ┆ 181 ┆ 705 ┆ 568 ┆ 817 ┆ 114 ┆ 895 ┆ 026 ┆ 847 ┆ 757 ┆ 713 ┆ 805 ┆ 964 ┆ 924 ┆ 503 ┆ 063 ┆ 45  ┆ 145 ┆ .42 ┆     ┆ 768 ┆ 57  ┆ 11  ┆ 81  ┆     ┆     ┆     ┆     ┆     ┆ 059 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 08  ┆ 12  ┆ 14  ┆ 15  ┆ 14  ┆ 13  ┆ 1   ┆ 07  ┆ 04  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 72  ┆ 6   ┆ 4   ┆ 9   ┆ 7   ┆ 4   ┆     ┆ 5   ┆ 6   ┆ 1   ┆ 1   ┆ 2   ┆ 7   ┆ 4   ┆ 8   ┆ 2   ┆ 9   ┆ 3   ┆ 4   ┆ 4   ┆ 5   ┆ 6   ┆ 6   ┆ 2   ┆ 1   ┆ 5   ┆ 3   ┆ 7   ┆ 9   ┆ 8   ┆ 8   ┆ 9   ┆ 2   ┆ 8   ┆     ┆     ┆ 6   ┆ 2   ┆ 3   ┆ 5   ┆ 7   ┆ 9   ┆ 4   ┆ 2   ┆ 8   ┆ 6   ┆ 7   ┆     ┆ 9   ┆ 2   ┆ 5   ┆     ┆ 5   ┆ 9   ┆ 2   ┆ 7   ┆ 5   ┆ 6   ┆ 2   ┆ 8   ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 8   ┆ 8   ┆ 9   ┆ 3   ┆ 7   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆ 8   ┆ 4   ┆     ┆     ┆     ┆ 6   ┆ 1   ┆ 6   ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 2   ┆ 2   ┆ 7   ┆ 1   ┆ 4   ┆     ┆ 4   ┆ 2   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 9   ┆ 9   ┆ 9   ┆ 5   ┆ 4   ┆ 8   ┆ 7   ┆ 7   ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 541 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 207 ┆ 216 ┆ 227 ┆ 243 ┆ 257 ┆ 265 ┆ 262 ┆ 253 ┆ 243 ┆ 236 ┆ 231 ┆ 227 ┆ 224 ┆ 221 ┆ 216 ┆ 211 ┆ 207 ┆ 204 ┆ 200 ┆ 202 ┆ 202 ┆ 205 ┆ 208 ┆ 211 ┆ 214 ┆ 218 ┆ 222 ┆ 226 ┆ 230 ┆ 235 ┆ 238 ┆ 242 ┆ 246 ┆ 250 ┆ 253 ┆ 256 ┆ 259 ┆ 262 ┆ 265 ┆ 267 ┆ 269 ┆ 271 ┆ 273 ┆ 275 ┆ 276 ┆ 277 ┆ 278 ┆ 280 ┆ 281 ┆ 282 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.7 ┆ 9.6 ┆ 9.5 ┆ 9.5 ┆ 9.6 ┆ 9.5 ┆ 8.6 ┆ 7.1 ┆ 6.0 ┆ 4.8 ┆ 1.8 ┆ 2.0 ┆ 1.0 ┆ 1.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.7 ┆ 3.3 ┆ 3.3 ┆ 1.5 ┆ 5.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.4 ┆ 3.7 ┆ 0.0 ┆ 4.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.8 ┆ 2.7 ┆ 3.2 ┆ 3.4 ┆ 4.0 ┆ 4.4 ┆ 4.8 ┆ 5.4 ┆ 5.7 ┆ 5.6 ┆ 5.2 ┆ 4.8 ┆ 2.4 ┆ 3.2 ┆ 1.6 ┆ 1.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.9 ┆ 1.2 ┆ 4.1 ┆ 5.1 ┆ 6.2 ┆ 4.8 ┆ 8.5 ┆ 1.5 ┆ 2.0 ┆ 3.8 ┆ 8.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.1 ┆ 6.5 ┆ 5.6 ┆ 1.6 ┆ 1.6 ┆ 1.3 ┆ 0.0 ┆ 0.0 ┆ 1.6 ┆ 1.4 ┆ 6.6 ┆ 3.7 ┆ 5.3 ┆ 1.6 ┆ -40 ┆ -40 ┆ -31 ┆ -18 ┆ -12 ┆ -14 ┆ -15 ┆ -13 ┆ -10 ┆ -7. ┆ -5. ┆ -1. ┆ 3.7 ┆ 8.5 ┆ 11. ┆ 15. ┆ 20. ┆ 23. ┆ 26. ┆ 29. ┆ 31. ┆ 31. ┆ 32. ┆ 32. ┆ 31. ┆ 30. ┆ 29. ┆ 28. ┆ 27. ┆ 25. ┆ 24. ┆ 23. ┆ 22. ┆ 21. ┆ 21. ┆ 20. ┆ 18. ┆ 17. ┆ 16. ┆ 15. ┆ 14. ┆ 13. ┆ 12. ┆ 11. ┆ 11. ┆ 10. ┆ 9.8 ┆ 9.3 ┆ 8.8 ┆ 8.4 ┆ 8.1 ┆ 7.7 ┆ 7.4 ┆ 7.1 ┆ 6.8 ┆ 6.5 ┆ 6.3 ┆ 6.2 ┆ 5.8 ┆ 3.8 ┆ -0. ┆ 5.8 ┆ 13. ┆ 11. ┆ 5.1 ┆ 3.8 ┆ 4.5 ┆ 3.1 ┆ 0.3 ┆ -1. ┆ -1. ┆ -0. ┆ 0.6 ┆ 0.2 ┆ 1.1 ┆ 4.2 ┆ 5.2 ┆ 5.7 ┆ 6.6 ┆ 7.8 ┆ 8.3 ┆ 8.4 ┆ 8.2 ┆ 7.8 ┆ 7.4 ┆ 7.1 ┆ 6.8 ┆ 6.4 ┆ 5.8 ┆ 5.1 ┆ 4.3 ┆ 3.4 ┆ 2.7 ┆ 2.2 ┆ 1.8 ┆ 1.6 ┆ 1.4 ┆ 1.2 ┆ 0.9 ┆ 0.5 ┆ 0.0 ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -5. ┆ -5. ┆ 101 ┆ 0.0 ┆ 11. ┆ -2. ┆ -0. ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 1.0 ┆ 407 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 2.6 ┆ 4.6 ┆ 8.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.6 ┆ 4.6 ┆ 3.5 ┆ 2.7 ┆ 2.0 ┆ 1.4 ┆ 1.1 ┆ 9.5 ┆ 8.4 ┆ 7.9 ┆ 7.4 ┆ 7.2 ┆ 7.0 ┆ 6.8 ┆ 6.6 ┆ 6.5 ┆ 6.3 ┆ 6.2 ┆ 6.0 ┆ 5.8 ┆ 5.7 ┆ 5.5 ┆ 5.3 ┆ 5.1 ┆ 4.9 ┆ 4.7 ┆ 4.4 ┆ 4.2 ┆ 4.0 ┆ 3.8 ┆ 3.6 ┆ 3.4 ┆ 3.3 ┆ 3.1 ┆ 3.1 ┆ 3.0 ┆ 3.0 ┆ 2.9 ┆ 2.9 ┆ 2.9 ┆ 2.8 ┆ 2.8 ┆ 1.6 ┆ 1.9 ┆ 2.1 ┆ 2.5 ┆ 2.8 ┆ 3.2 ┆ 3.6 ┆ 4.1 ┆ 4.5 ┆ 5.0 ┆ 5.4 ┆ 5.9 ┆ 6.3 ┆ 6.8 ┆ 7.2 ┆ 7.6 ┆ 8.0 ┆ 8.4 ┆ 8.8 ┆ 9.2 ┆ 9.5 ┆ 9.8 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 2.4 ┆ 3.1 ┆ 3.9 ┆ 4.9 ┆ 6.1 ┆ 7.5 ┆ 9.2 ┆ 1.1 ┆ 1.3 ┆ 1.5 ┆ 1.7 ┆ 2.0 ┆ 2.3 ┆ 2.5 ┆ 2.8 ┆ 3.1 ┆ 3.4 ┆ 3.7 ┆ 4.0 ┆ 4.2 ┆ 4.5 ┆ 4.8 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_4 ┆ .92 ┆ .69 ┆ .82 ┆ .55 ┆ .49 ┆ .29 ┆ .88 ┆ .70 ┆ .45 ┆ .89 ┆ .65 ┆ .56 ┆ .41 ┆ .18 ┆ .80 ┆ .96 ┆ .47 ┆ .36 ┆ .80 ┆ .55 ┆ .75 ┆ .46 ┆ .13 ┆ .35 ┆ .99 ┆ .81 ┆ .90 ┆ .87 ┆ .92 ┆ .01 ┆ .95 ┆ .83 ┆ .55 ┆ .10 ┆ .43 ┆ .55 ┆ .53 ┆ .38 ┆ .07 ┆ .55 ┆ .74 ┆ .38 ┆ .09 ┆ .01 ┆ .45 ┆ .96 ┆ .97 ┆ .09 ┆ .15 ┆ .39 ┆ .62 ┆ .62 ┆ .48 ┆ .26 ┆ .13 ┆ .89 ┆ .75 ┆ .68 ┆ .58 ┆ .27 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 001 ┆ 002 ┆ 003 ┆ 005 ┆ 007 ┆ 009 ┆ 012 ┆ 015 ┆ 019 ┆ 023 ┆ 027 ┆ 032 ┆ 037 ┆ 045 ┆ 050 ┆ 054 ┆ 059 ┆ 062 ┆ 065 ┆ 070 ┆ 074 ┆ 076 ┆ 080 ┆ 083 ┆ 087 ┆ 092 ┆ 095 ┆ 100 ┆ 105 ┆ 110 ┆ 113 ┆ 118 ┆ 128 ┆ 051 ┆ 452 ┆ 362 ┆ 512 ┆ 605 ┆ 557 ┆ 741 ┆ 128 ┆ 712 ┆ 578 ┆ 251 ┆ 646 ┆ 382 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 811 ┆ 669 ┆ 602 ┆ 452 ┆ 629 ┆     ┆     ┆     ┆     ┆ 444 ┆ 406 ┆     ┆ 136 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 231 ┆ 059 ┆ 707 ┆ 833 ┆ 651 ┆ 067 ┆ 119 ┆ 224 ┆ 644 ┆ 434 ┆ 800 ┆ 358 ┆ 067 ┆ 542 ┆ 799 ┆ 093 ┆     ┆     ┆     ┆     ┆ 935 ┆ 625 ┆ 767 ┆ 563 ┆ 808 ┆ 014 ┆ 056 ┆ 604 ┆ 746 ┆ 882 ┆ 910 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 042 ┆ 356 ┆ 581 ┆ 544 ┆ 084 ┆ 615 ┆     ┆     ┆ 286 ┆ 384 ┆ 560 ┆ 114 ┆ 211 ┆ 815 ┆ .51 ┆ .03 ┆ .82 ┆ .52 ┆ .83 ┆ .17 ┆ .04 ┆ .74 ┆ .58 ┆ 645 ┆ 238 ┆ 475 ┆ 027 ┆ 420 ┆ 115 ┆ 118 ┆ 132 ┆ 884 ┆ 822 ┆ 354 ┆ 015 ┆ 800 ┆ 027 ┆ 053 ┆ 697 ┆ 933 ┆ 808 ┆ 541 ┆ 194 ┆ 891 ┆ 681 ┆ 639 ┆ 746 ┆ 928 ┆ 076 ┆ 061 ┆ 849 ┆ 503 ┆ 179 ┆ 007 ┆ 075 ┆ 291 ┆ 557 ┆ 871 ┆ 154 ┆ 471 ┆ 891 ┆ 470 ┆ 792 ┆ 805 ┆ 099 ┆ 694 ┆ 509 ┆ 456 ┆ 510 ┆ 795 ┆ 718 ┆ 621 ┆ 186 ┆ 053 ┆ 848 ┆ 097 ┆ 565 ┆ 622 ┆ 921 ┆ 320 ┆ 215 ┆ 576 ┆ 644 ┆ 681 ┆ 608 ┆ 285 ┆ 598 ┆ 528 ┆ 247 ┆ 570 ┆ 244 ┆ 101 ┆ 796 ┆ 533 ┆ 949 ┆ 250 ┆ 186 ┆ 648 ┆ 525 ┆ 431 ┆ 866 ┆ 793 ┆ 976 ┆ 571 ┆ 282 ┆ 962 ┆ 861 ┆ 407 ┆ 787 ┆ 557 ┆ 962 ┆ 962 ┆ 782 ┆ 253 ┆ 134 ┆ 525 ┆ 048 ┆ 479 ┆ 861 ┆ 312 ┆ 749 ┆ 113 ┆ 449 ┆ 714 ┆ 954 ┆ 173 ┆ 351 ┆ 488 ┆ 584 ┆ 650 ┆ 742 ┆ 905 ┆ 619 ┆ 524 ┆ 283 ┆     ┆ 273 ┆ 447 ┆ 020 ┆ 308 ┆     ┆     ┆     ┆     ┆     ┆ .40 ┆     ┆     ┆     ┆     ┆ 040 ┆ 899 ┆ 061 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 758 ┆ 964 ┆ 960 ┆ 383 ┆ 008 ┆ 038 ┆ 380 ┆ 438 ┆ 788 ┆ 227 ┆ 739 ┆ 256 ┆ 076 ┆ 281 ┆ 685 ┆ 100 ┆ 685 ┆ 235 ┆ 612 ┆ 859 ┆ 117 ┆ 244 ┆ 288 ┆ 422 ┆ 651 ┆ 129 ┆ 702 ┆ 394 ┆ 204 ┆ 262 ┆ 457 ┆ 770 ┆ 193 ┆ 927 ┆ 338 ┆ 764 ┆ 197 ┆ 631 ┆ 297 ┆ 010 ┆ 719 ┆ 492 ┆ 681 ┆ 158 ┆ 978 ┆ 159 ┆ 701 ┆ 578 ┆ 736 ┆ 098 ┆ 582 ┆ 116 ┆ 652 ┆ 167 ┆ 651 ┆ 091 ┆ 465 ┆ 735 ┆ 863 ┆ 820 ┆ 596 ┆ 198 ┆ 638 ┆ 928 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 879 ┆ 335 ┆ 392 ┆ 344 ┆ 457 ┆ 905 ┆ 726 ┆ 179 ┆ 285 ┆ 559 ┆ 976 ┆ 519 ┆ 175 ┆ 931 ┆ 766 ┆ 646 ┆ 533 ┆ 395 ┆ 210 ┆ 971 ┆ 676 ┆ 324 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 845 ┆ 609 ┆ 734 ┆ 159 ┆ 501 ┆ 852 ┆ 425 ┆ 689 ┆ 213 ┆ 369 ┆ 184 ┆ 155 ┆ 529 ┆ 584 ┆ 496 ┆ 906 ┆ 263 ┆ 477 ┆ 333 ┆ 428 ┆ 814 ┆ 009 ┆ 502 ┆ 513 ┆ 207 ┆ 225 ┆ 660 ┆ 796 ┆ 96  ┆ 690 ┆ 568 ┆ 799 ┆ 895 ┆ 266 ┆ 023 ┆ 465 ┆ 874 ┆ 158 ┆ 753 ┆ 465 ┆ 576 ┆ 391 ┆ 381 ┆ 660 ┆ 915 ┆ 490 ┆ 139 ┆ 562 ┆ 628 ┆ 836 ┆ 887 ┆ 094 ┆ 805 ┆ 523 ┆ 746 ┆ 615 ┆ 663 ┆ 309 ┆ 756 ┆ 775 ┆ 449 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 04  ┆ 07  ┆ 1   ┆ 14  ┆ 18  ┆ 22  ┆ 3   ┆ 43  ┆ 69  ┆ 17  ┆ 82  ┆ 65  ┆ 74  ┆ 15  ┆ 18  ┆ 6   ┆ 45  ┆ 63  ┆ 48  ┆ 65  ┆ 93  ┆ 36  ┆ 83  ┆ 26  ┆ 84  ┆ 18  ┆ 33  ┆ 33  ┆ 91  ┆ 64  ┆ 59  ┆ 69  ┆ 08  ┆ 62  ┆ 96  ┆ 1   ┆ 81  ┆ 69  ┆ 81  ┆ 49  ┆ 45  ┆ 4   ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-4 ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆     ┆     ┆     ┆     ┆ e-8 ┆ e-8 ┆     ┆ e-7 ┆ 03  ┆ 03  ┆ 04  ┆ 22  ┆ 83  ┆ 24  ┆ 24  ┆ 23  ┆ 19  ┆ 39  ┆ 5   ┆ 19  ┆ 11  ┆ 12  ┆ 1   ┆ 08  ┆ 13  ┆ 13  ┆ 08  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆     ┆     ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ 01  ┆ 01  ┆ 02  ┆ 01  ┆ 01  ┆ 01  ┆ 01  ┆ 02  ┆ 02  ┆ 03  ┆ 03  ┆ 03  ┆ 03  ┆ 03  ┆ 02  ┆ 01  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-9 ┆ e-8 ┆ e-8 ┆ e-9 ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ 467 ┆ 241 ┆ 841 ┆ 496 ┆ 023 ┆ 348 ┆ 724 ┆ 384 ┆ 702 ┆ 994 ┆ 524 ┆ 789 ┆ 35  ┆ 01  ┆ 44  ┆ 564 ┆ 157 ┆ 062 ┆ 964 ┆ 526 ┆ 959 ┆ 139 ┆ 682 ┆ 957 ┆ 876 ┆ 278 ┆ 729 ┆ 658 ┆ 826 ┆ 485 ┆ 64  ┆ 124 ┆ 605 ┆ 873 ┆ 565 ┆ 099 ┆ 762 ┆ 729 ┆ 417 ┆ 115 ┆ 261 ┆ 599 ┆ 146 ┆ 78  ┆ 768 ┆ 049 ┆ 61  ┆ 19  ┆ 16  ┆ 35  ┆ 97  ┆ 93  ┆ 09  ┆ 93  ┆ 57  ┆ 24  ┆ 34  ┆ 67  ┆ 72  ┆ 06  ┆ 759 ┆ 36  ┆ 142 ┆ 131 ┆ 77  ┆ 22  ┆ 55  ┆ 26  ┆ 5   ┆ 811 ┆ 787 ┆ 473 ┆ 45  ┆ 71  ┆ 26  ┆ 13  ┆ 26  ┆ 01  ┆ 22  ┆ 95  ┆ 54  ┆ 32  ┆     ┆ 57  ┆ 11  ┆ 94  ┆ 08  ┆ 43  ┆ 73  ┆ 22  ┆ 24  ┆ 79  ┆ 17  ┆ 91  ┆ 76  ┆ 45  ┆ 2   ┆ 32  ┆ 17  ┆ 21  ┆ 48  ┆ 427 ┆ 021 ┆ 973 ┆ 84  ┆ 476 ┆ 788 ┆ 212 ┆ 63  ┆ 691 ┆ 645 ┆ 306 ┆ 14  ┆ 054 ┆ 904 ┆ 776 ┆ 572 ┆ 684 ┆ 688 ┆ 507 ┆ .48 ┆     ┆ 749 ┆ 222 ┆ 809 ┆ 42  ┆     ┆     ┆     ┆     ┆     ┆ 612 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 01  ┆ 03  ┆ 04  ┆ 07  ┆ 12  ┆ 13  ┆ 13  ┆ 13  ┆ 11  ┆ 1   ┆ 07  ┆ 05  ┆ 03  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 78  ┆ 7   ┆ 8   ┆     ┆ 1   ┆ 5   ┆ 7   ┆ 2   ┆ 2   ┆ 5   ┆ 1   ┆ 4   ┆ 2   ┆ 7   ┆ 7   ┆ 6   ┆ 9   ┆ 3   ┆ 3   ┆ 2   ┆ 5   ┆ 7   ┆ 3   ┆     ┆ 2   ┆ 3   ┆ 2   ┆ 8   ┆     ┆ 6   ┆ 9   ┆ 9   ┆ 1   ┆ 3   ┆ 3   ┆ 3   ┆ 6   ┆ 8   ┆ 9   ┆ 4   ┆ 6   ┆ 3   ┆     ┆ 3   ┆     ┆ 7   ┆ 9   ┆ 8   ┆ 5   ┆ 7   ┆ 2   ┆ 1   ┆     ┆ 1   ┆ 2   ┆ 2   ┆ 6   ┆ 1   ┆ 9   ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 7   ┆ 8   ┆ 2   ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 3   ┆ 9   ┆ 3   ┆ 8   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 1   ┆ 2   ┆ 6   ┆ 3   ┆     ┆     ┆     ┆     ┆ 3   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 6   ┆ 2   ┆ 0   ┆ 0   ┆ 0   ┆ 1   ┆ 7   ┆ 9   ┆ 7   ┆ 5   ┆ 6   ┆ 4   ┆ 3   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 235 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   │\n",
       "│ tes ┆ 216 ┆ 228 ┆ 236 ┆ 248 ┆ 256 ┆ 262 ┆ 257 ┆ 247 ┆ 237 ┆ 229 ┆ 223 ┆ 218 ┆ 214 ┆ 210 ┆ 203 ┆ 199 ┆ 190 ┆ 187 ┆ 189 ┆ 192 ┆ 198 ┆ 204 ┆ 211 ┆ 216 ┆ 222 ┆ 229 ┆ 234 ┆ 238 ┆ 242 ┆ 246 ┆ 249 ┆ 253 ┆ 256 ┆ 259 ┆ 261 ┆ 263 ┆ 265 ┆ 268 ┆ 271 ┆ 274 ┆ 276 ┆ 278 ┆ 279 ┆ 280 ┆ 281 ┆ 282 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 291 ┆ 292 ┆ 293 ┆ 295 ┆ 296 ┆ 297 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 9.6 ┆ 8.5 ┆ 7.4 ┆ 6.6 ┆ 5.5 ┆ 5.2 ┆ 3.8 ┆ 3.3 ┆ 1.9 ┆ 1.7 ┆ 9.5 ┆ 5.7 ┆ 1.2 ┆ 5.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.9 ┆ 2.2 ┆ 3.0 ┆ 3.2 ┆ 7.4 ┆ 3.0 ┆ 1.3 ┆ 1.9 ┆ 5.8 ┆ 1.2 ┆ 6.1 ┆ 1.8 ┆ 0.0 ┆ 0.0 ┆ 6.4 ┆ 5.2 ┆ 7.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.5 ┆ 6.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.2 ┆ 0.0 ┆ 0.0 ┆ 5.3 ┆ 4.9 ┆ 1.4 ┆ 5.0 ┆ 6.1 ┆ 6.5 ┆ 6.8 ┆ 7.7 ┆ 8.1 ┆ 8.3 ┆ 6.3 ┆ 4.2 ┆ 2.4 ┆ 1.3 ┆ 7.4 ┆ 3.4 ┆ 1.9 ┆ 4.1 ┆ 5.6 ┆ 3.4 ┆ 5.5 ┆ 8.8 ┆ 1.0 ┆ 8.7 ┆ 2.6 ┆ 0.0 ┆ 0.0 ┆ 3.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.7 ┆ 6.0 ┆ 2.2 ┆ 2.2 ┆ 3.2 ┆ 6.0 ┆ 1.1 ┆ 1.9 ┆ 1.5 ┆ 2.1 ┆ 1.5 ┆ 1.3 ┆ 6.8 ┆ 7.3 ┆ 3.4 ┆ 6.8 ┆ 3.6 ┆ 2.8 ┆ 9.6 ┆ 2.1 ┆ 0.0 ┆ 0.0 ┆ 4.3 ┆ 8.7 ┆ 2.2 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 51. ┆ 42. ┆ 37. ┆ 35. ┆ 34. ┆ 27. ┆ 29. ┆ 22. ┆ 14. ┆ 8.4 ┆ 6.6 ┆ 6.8 ┆ 4.7 ┆ 0.1 ┆ -1. ┆ 2.4 ┆ 7.7 ┆ 15. ┆ 26. ┆ 34. ┆ 40. ┆ 46. ┆ 48. ┆ 48. ┆ 48. ┆ 47. ┆ 46. ┆ 43. ┆ 39. ┆ 33. ┆ 28. ┆ 24. ┆ 22. ┆ 20. ┆ 18. ┆ 16. ┆ 15. ┆ 13. ┆ 12. ┆ 10. ┆ 8.4 ┆ 6.6 ┆ 5.0 ┆ 3.9 ┆ 3.3 ┆ 2.5 ┆ 1.6 ┆ 0.7 ┆ -0. ┆ -0. ┆ -1. ┆ -2. ┆ -3. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ 18. ┆ -22 ┆ -17 ┆ 4.9 ┆ 3.7 ┆ 10. ┆ -5. ┆ -4. ┆ -1. ┆ 0.7 ┆ 1.5 ┆ 1.2 ┆ 1.4 ┆ 0.5 ┆ -1. ┆ -1. ┆ -3. ┆ -6. ┆ -9. ┆ -9. ┆ -11 ┆ -5. ┆ 5.4 ┆ 8.2 ┆ 6.3 ┆ 2.5 ┆ -2. ┆ -7. ┆ -11 ┆ -11 ┆ -10 ┆ -6. ┆ -3. ┆ -0. ┆ 0.7 ┆ 2.2 ┆ 3.4 ┆ 3.7 ┆ 3.5 ┆ 2.9 ┆ 2.4 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 2.5 ┆ 2.2 ┆ 1.9 ┆ 1.8 ┆ 1.6 ┆ 1.5 ┆ 1.5 ┆ 1.4 ┆ 1.2 ┆ 1.0 ┆ 0.3 ┆ 0.0 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 100 ┆ 898 ┆ 98. ┆ 5.0 ┆ 0.0 ┆ 0.0 ┆ 0.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 452 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 3.0 ┆ 5.4 ┆ 9.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.2 ┆ 1.8 ┆ 1.1 ┆ 8.8 ┆ 7.2 ┆ 6.2 ┆ 5.6 ┆ 5.5 ┆ 5.5 ┆ 5.6 ┆ 5.7 ┆ 5.8 ┆ 6.0 ┆ 6.2 ┆ 6.3 ┆ 6.5 ┆ 6.6 ┆ 6.7 ┆ 6.7 ┆ 6.7 ┆ 6.6 ┆ 6.5 ┆ 6.4 ┆ 6.3 ┆ 6.2 ┆ 6.0 ┆ 5.9 ┆ 5.8 ┆ 5.7 ┆ 5.7 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 5.7 ┆ 5.7 ┆ 5.6 ┆ 5.6 ┆ 5.6 ┆ 1.8 ┆ 2.0 ┆ 2.3 ┆ 2.7 ┆ 3.1 ┆ 3.5 ┆ 3.9 ┆ 4.4 ┆ 4.9 ┆ 5.4 ┆ 5.9 ┆ 6.4 ┆ 6.9 ┆ 7.4 ┆ 7.8 ┆ 8.3 ┆ 8.7 ┆ 9.2 ┆ 9.6 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.4 ┆ 4.3 ┆ 5.3 ┆ 6.5 ┆ 8.0 ┆ 9.8 ┆ 1.1 ┆ 1.4 ┆ 1.6 ┆ 1.9 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.0 ┆ 3.3 ┆ 3.7 ┆ 4.0 ┆ 4.3 ┆ 4.6 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_5 ┆ .89 ┆ .91 ┆ .65 ┆ .99 ┆ .38 ┆ .36 ┆ .79 ┆ .51 ┆ .79 ┆ .43 ┆ .84 ┆ .98 ┆ .45 ┆ .25 ┆ .82 ┆ .01 ┆ .87 ┆ .29 ┆ .28 ┆ .70 ┆ .79 ┆ .75 ┆ .28 ┆ .76 ┆ .77 ┆ .00 ┆ .25 ┆ .51 ┆ .56 ┆ .31 ┆ .83 ┆ .30 ┆ .47 ┆ .04 ┆ .27 ┆ .22 ┆ .78 ┆ .67 ┆ .68 ┆ .39 ┆ .69 ┆ .63 ┆ .65 ┆ .50 ┆ .79 ┆ .94 ┆ .16 ┆ .33 ┆ .56 ┆ .63 ┆ .56 ┆ .56 ┆ .32 ┆ .00 ┆ .72 ┆ .79 ┆ .87 ┆ .05 ┆ .26 ┆ .49 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 004 ┆ 004 ┆ 003 ┆ 003 ┆ 002 ┆ 003 ┆ 006 ┆ 011 ┆ 019 ┆ 025 ┆ 029 ┆ 030 ┆ 031 ┆ 033 ┆ 035 ┆ 042 ┆ 054 ┆ 065 ┆ 072 ┆ 080 ┆ 087 ┆ 089 ┆ 092 ┆ 098 ┆ 102 ┆ 109 ┆ 119 ┆ 130 ┆ 134 ┆ 138 ┆ 139 ┆ 139 ┆ 140 ┆ 255 ┆ 831 ┆ 105 ┆ 848 ┆ 743 ┆ 362 ┆ 533 ┆ 248 ┆ 658 ┆ 240 ┆ 839 ┆ 738 ┆ 609 ┆ 903 ┆ 957 ┆     ┆     ┆     ┆     ┆     ┆ 242 ┆ 527 ┆ 741 ┆ 172 ┆ 653 ┆ 357 ┆ 435 ┆ 898 ┆ 084 ┆ 265 ┆ 973 ┆ 722 ┆     ┆     ┆ 923 ┆ 863 ┆ 171 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 247 ┆ 122 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 283 ┆ 000 ┆ 000 ┆ 185 ┆ 293 ┆ 069 ┆ 483 ┆ 028 ┆ 125 ┆ 174 ┆ 943 ┆ 095 ┆ 080 ┆ 072 ┆ 078 ┆ 430 ┆ 018 ┆ 322 ┆ 289 ┆ 021 ┆ 825 ┆ 206 ┆ 514 ┆ 620 ┆ 530 ┆ 012 ┆ 511 ┆ 333 ┆     ┆     ┆ 540 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 206 ┆ 768 ┆ 670 ┆ 869 ┆ 742 ┆ 206 ┆ 166 ┆ 000 ┆ 679 ┆ 275 ┆ 721 ┆ 016 ┆ 575 ┆ 267 ┆ 838 ┆ 420 ┆ 097 ┆ 057 ┆ 009 ┆ 185 ┆     ┆     ┆ 523 ┆ 768 ┆ 667 ┆     ┆     ┆     ┆     ┆     ┆ 209 ┆ 419 ┆ 281 ┆ 648 ┆ 451 ┆ 809 ┆ 607 ┆ 240 ┆ 340 ┆ 704 ┆ 837 ┆ 570 ┆ 378 ┆ 683 ┆ 061 ┆ 119 ┆ 352 ┆ 792 ┆ 320 ┆ 285 ┆ 828 ┆ 740 ┆ 625 ┆ 333 ┆ 036 ┆ 471 ┆ 671 ┆ 513 ┆ 277 ┆ 958 ┆ 955 ┆ 944 ┆ 012 ┆ 014 ┆ 198 ┆ 515 ┆ 106 ┆ 656 ┆ 162 ┆ 343 ┆ 655 ┆ 114 ┆ 512 ┆ 861 ┆ 115 ┆ 292 ┆ 528 ┆ 810 ┆ 040 ┆ 972 ┆ 912 ┆ 710 ┆ 392 ┆ 092 ┆ 537 ┆ 577 ┆ 500 ┆ 448 ┆ 368 ┆ 239 ┆ 015 ┆ .50 ┆ .31 ┆ 184 ┆ 657 ┆ 192 ┆ 364 ┆ 811 ┆ 973 ┆ 201 ┆ 112 ┆ 775 ┆ 268 ┆ 147 ┆ 303 ┆ 287 ┆ 506 ┆ 756 ┆ 073 ┆ 815 ┆ .01 ┆ 496 ┆ 248 ┆ 626 ┆ 256 ┆ 711 ┆ 623 ┆ 753 ┆ .24 ┆ .85 ┆ .22 ┆ 929 ┆ 456 ┆ 885 ┆ 845 ┆ 803 ┆ 677 ┆ 678 ┆ 059 ┆ 427 ┆ 050 ┆ 770 ┆ 063 ┆ 659 ┆ 917 ┆ 398 ┆ 785 ┆ 103 ┆ 712 ┆ 977 ┆ 153 ┆ 318 ┆ 408 ┆ 981 ┆ 684 ┆ 581 ┆ 301 ┆ 440 ┆ 531 ┆ 596 ┆ 806 ┆ .33 ┆ 588 ┆ 338 ┆ 236 ┆ 031 ┆ 385 ┆ 6   ┆ 481 ┆ 6   ┆ 481 ┆ .46 ┆     ┆     ┆     ┆     ┆ 087 ┆ 188 ┆ 127 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 125 ┆ 415 ┆ 659 ┆ 404 ┆ 384 ┆ 100 ┆ 742 ┆ 711 ┆ 654 ┆ 333 ┆ 553 ┆ 866 ┆ 504 ┆ 214 ┆ 771 ┆ 290 ┆ 536 ┆ 011 ┆ 497 ┆ 260 ┆ 603 ┆ 950 ┆ 816 ┆ 419 ┆ 086 ┆ 839 ┆ 843 ┆ 898 ┆ 999 ┆ 167 ┆ 854 ┆ 560 ┆ 286 ┆ 029 ┆ 050 ┆ 322 ┆ 588 ┆ 850 ┆ 079 ┆ 031 ┆ 982 ┆ 929 ┆ 862 ┆ 144 ┆ 838 ┆ 905 ┆ 365 ┆ 218 ┆ 435 ┆ 957 ┆ 702 ┆ 579 ┆ 511 ┆ 445 ┆ 356 ┆ 232 ┆ 062 ┆ 819 ┆ 464 ┆ 954 ┆ 259 ┆ 366 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 774 ┆ 110 ┆ 352 ┆ 808 ┆ 738 ┆ 287 ┆ 843 ┆ 097 ┆ 555 ┆ 180 ┆ 942 ┆ 819 ┆ 798 ┆ 866 ┆ 997 ┆ 158 ┆ 306 ┆ 409 ┆ 445 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 782 ┆ 081 ┆ 196 ┆ 469 ┆ 930 ┆ 063 ┆ 062 ┆ 187 ┆ 648 ┆ 522 ┆ 697 ┆ 084 ┆ 714 ┆ 515 ┆ 341 ┆ 449 ┆ 891 ┆ 713 ┆ 040 ┆ 349 ┆ 395 ┆ 049 ┆ 373 ┆ 941 ┆ 731 ┆ 148 ┆ 249 ┆ 177 ┆ 507 ┆ 807 ┆ 885 ┆ 27  ┆ 477 ┆ 925 ┆ 202 ┆ 533 ┆ 004 ┆ 625 ┆ 643 ┆ 322 ┆ 762 ┆ 573 ┆ 943 ┆ 286 ┆ 606 ┆ 704 ┆ 652 ┆ 368 ┆ 840 ┆ 037 ┆ 629 ┆ 920 ┆ 361 ┆ 391 ┆ 669 ┆ 129 ┆ 075 ┆ 226 ┆ 116 ┆ 222 ┆ 587 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 04  ┆ 07  ┆ 11  ┆ 33  ┆ 83  ┆ 59  ┆ 92  ┆ 07  ┆ 31  ┆ 65  ┆ 01  ┆ 98  ┆ 8   ┆ 84  ┆ 78  ┆ 32  ┆ 19  ┆ 05  ┆ 77  ┆ 99  ┆ 6   ┆ 08  ┆ 23  ┆ 04  ┆ 93  ┆ 6   ┆ 64  ┆ 69  ┆ 92  ┆ 76  ┆ 07  ┆ 32  ┆ 99  ┆ 14  ┆ 87  ┆ 28  ┆ 01  ┆ 12  ┆ 56  ┆ 4   ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-4 ┆ e-4 ┆ e-3 ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-8 ┆     ┆     ┆ e-8 ┆ e-7 ┆ e-7 ┆ 01  ┆ 02  ┆ 03  ┆ 03  ┆ e-8 ┆ e-8 ┆ 02  ┆ 52  ┆ 06  ┆ 17  ┆ 04  ┆ 02  ┆ 1   ┆ 11  ┆ 11  ┆ 06  ┆ e-7 ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-8 ┆ e-7 ┆ e-8 ┆ e-1 ┆     ┆     ┆ e-7 ┆ 02  ┆ 04  ┆ 02  ┆ 03  ┆ 02  ┆ 02  ┆ e-7 ┆ e-9 ┆ e-9 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-1 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-1 ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆ 186 ┆ 439 ┆ 976 ┆ 045 ┆ 416 ┆ 329 ┆ 586 ┆ 679 ┆ 837 ┆ 71  ┆ 59  ┆ 6   ┆ 29  ┆ 45  ┆ 041 ┆ 94  ┆ 85  ┆ 098 ┆ 675 ┆ 7   ┆ 029 ┆ 835 ┆ 057 ┆ 362 ┆ 965 ┆ 239 ┆ 519 ┆ 668 ┆ 116 ┆ 462 ┆ 049 ┆ 975 ┆ 148 ┆ 659 ┆ 438 ┆ 235 ┆ 666 ┆ 904 ┆ 052 ┆ 934 ┆ 71  ┆ 52  ┆ 28  ┆ 9   ┆ 1   ┆ 46  ┆ 07  ┆ 19  ┆ 192 ┆ 076 ┆ 147 ┆ 149 ┆ 743 ┆ 902 ┆ 605 ┆ 928 ┆ 485 ┆ 912 ┆ 188 ┆ 9   ┆ 506 ┆ 621 ┆ 347 ┆ 76  ┆ 59  ┆ 913 ┆ 113 ┆ 059 ┆ 06  ┆ 58  ┆ 61  ┆ 31  ┆ 16  ┆ 34  ┆ 389 ┆ 716 ┆ 73  ┆ 223 ┆ 841 ┆ 227 ┆ 597 ┆ 088 ┆ 86  ┆ 81  ┆ 5   ┆ 59  ┆ 422 ┆ 617 ┆ 074 ┆ 661 ┆ 719 ┆ 758 ┆ 147 ┆ 487 ┆ 14  ┆ 07  ┆ 87  ┆ 92  ┆ 61  ┆     ┆ 72  ┆ 2   ┆ 74  ┆ 9   ┆ 51  ┆ 6   ┆ 71  ┆ 35  ┆ 7   ┆ 33  ┆ 46  ┆ 27  ┆ 66  ┆ 05  ┆ 6   ┆ 9   ┆ 357 ┆ 503 ┆ 31  ┆ 656 ┆ .04 ┆ 628 ┆ 601 ┆ 12  ┆ 82  ┆ 76  ┆ 27  ┆     ┆ 38  ┆     ┆ 38  ┆ 519 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 09  ┆ 14  ┆ 15  ┆ 15  ┆ 15  ┆ 12  ┆ 09  ┆ 06  ┆ 03  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 20  ┆ 5   ┆ 9   ┆ 4   ┆ 1   ┆ 1   ┆ 9   ┆ 3   ┆ 8   ┆ 6   ┆ 3   ┆ 8   ┆ 9   ┆ 2   ┆ 2   ┆ 3   ┆ 6   ┆ 1   ┆ 4   ┆ 3   ┆ 3   ┆ 7   ┆     ┆ 5   ┆ 9   ┆ 4   ┆ 8   ┆ 5   ┆ 2   ┆ 7   ┆ 5   ┆     ┆     ┆ 6   ┆ 1   ┆ 4   ┆ 8   ┆ 9   ┆ 3   ┆ 8   ┆ 7   ┆     ┆ 7   ┆ 4   ┆ 3   ┆ 2   ┆ 4   ┆ 2   ┆ 8   ┆ 2   ┆ 9   ┆ 1   ┆ 4   ┆ 8   ┆ 2   ┆ 3   ┆ 9   ┆ 6   ┆ 7   ┆ 6   ┆ 8   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 9   ┆ 4   ┆ 7   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆ 8   ┆ 4   ┆ 0   ┆ 7   ┆ 2   ┆ 7   ┆ 3   ┆ 9   ┆ 4   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 3   ┆ 3   ┆ 8   ┆ 2   ┆ 2   ┆ 6   ┆ 0   ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆ 1   ┆     ┆     ┆ 2   ┆ 0   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 6   ┆ 4   ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 292 ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 215 ┆ 229 ┆ 237 ┆ 246 ┆ 259 ┆ 260 ┆ 258 ┆ 253 ┆ 238 ┆ 228 ┆ 223 ┆ 220 ┆ 216 ┆ 210 ┆ 203 ┆ 194 ┆ 188 ┆ 184 ┆ 186 ┆ 191 ┆ 198 ┆ 204 ┆ 209 ┆ 215 ┆ 222 ┆ 228 ┆ 234 ┆ 239 ┆ 244 ┆ 248 ┆ 251 ┆ 254 ┆ 257 ┆ 261 ┆ 263 ┆ 266 ┆ 270 ┆ 273 ┆ 276 ┆ 279 ┆ 281 ┆ 282 ┆ 282 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 292 ┆ 292 ┆ 293 ┆ 294 ┆ 295 ┆ 296 ┆ 297 ┆ 298 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.4 ┆ 8.2 ┆ 8.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 9.5 ┆ 8.4 ┆ 7.8 ┆ 6.7 ┆ 5.3 ┆ 4.2 ┆ 3.2 ┆ 1.7 ┆ 4.7 ┆ 1.2 ┆ 5.4 ┆ 9.5 ┆ 1.8 ┆ 4.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.2 ┆ 2.4 ┆ 2.7 ┆ 1.6 ┆ 4.6 ┆ 1.5 ┆ 3.3 ┆ 6.6 ┆ 6.7 ┆ 3.6 ┆ 2.8 ┆ 5.0 ┆ 2.7 ┆ 1.3 ┆ 3.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.9 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 5.9 ┆ 2.8 ┆ 8.3 ┆ 6.1 ┆ 6.5 ┆ 7.6 ┆ 9.8 ┆ 8.5 ┆ 9.6 ┆ 7.4 ┆ 6.0 ┆ 3.9 ┆ 1.9 ┆ 9.4 ┆ 1.3 ┆ 6.3 ┆ 1.0 ┆ 1.6 ┆ 1.9 ┆ 1.1 ┆ 2.2 ┆ 7.3 ┆ 1.3 ┆ 6.2 ┆ 1.2 ┆ 3.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.0 ┆ 8.9 ┆ 1.5 ┆ 4.6 ┆ 2.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.4 ┆ 1.3 ┆ 2.2 ┆ 0.0 ┆ 20. ┆ 11. ┆ -1. ┆ -11 ┆ 0.5 ┆ -13 ┆ -0. ┆ -6. ┆ -11 ┆ -7. ┆ -3. ┆ 2.1 ┆ -0. ┆ -3. ┆ -5. ┆ -2. ┆ 9.4 ┆ 16. ┆ 20. ┆ 24. ┆ 28. ┆ 29. ┆ 29. ┆ 30. ┆ 30. ┆ 30. ┆ 29. ┆ 26. ┆ 24. ┆ 21. ┆ 20. ┆ 18. ┆ 17. ┆ 15. ┆ 12. ┆ 9.7 ┆ 7.3 ┆ 5.0 ┆ 2.4 ┆ -0. ┆ -3. ┆ -6. ┆ -9. ┆ -11 ┆ -11 ┆ -10 ┆ -9. ┆ -8. ┆ -8. ┆ -7. ┆ -6. ┆ -6. ┆ -5. ┆ -5. ┆ -4. ┆ -4. ┆ -3. ┆ -3. ┆ -2. ┆ -2. ┆ -1. ┆ -16 ┆ -2. ┆ 11. ┆ 1.0 ┆ 0.5 ┆ -4. ┆ -0. ┆ 0.2 ┆ -0. ┆ 1.8 ┆ 0.9 ┆ 0.3 ┆ -0. ┆ -1. ┆ -0. ┆ 3.0 ┆ -1. ┆ -11 ┆ -17 ┆ -20 ┆ -16 ┆ -11 ┆ -11 ┆ -13 ┆ -14 ┆ -15 ┆ -15 ┆ -14 ┆ -11 ┆ -9. ┆ -6. ┆ -4. ┆ -2. ┆ -0. ┆ 1.1 ┆ 1.8 ┆ 1.8 ┆ 1.7 ┆ 1.6 ┆ 1.4 ┆ 1.2 ┆ 1.2 ┆ 1.0 ┆ 0.6 ┆ 0.2 ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.0 ┆ 100 ┆ 854 ┆ 49. ┆ 2.2 ┆ 0.0 ┆ -0. ┆ 0.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 457 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 3.0 ┆ 5.5 ┆ 9.9 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.3 ┆ 2.5 ┆ 1.2 ┆ 8.6 ┆ 7.2 ┆ 6.3 ┆ 5.6 ┆ 5.1 ┆ 4.9 ┆ 4.8 ┆ 4.7 ┆ 4.7 ┆ 4.8 ┆ 4.9 ┆ 5.1 ┆ 5.2 ┆ 5.4 ┆ 5.5 ┆ 5.6 ┆ 5.7 ┆ 5.8 ┆ 5.7 ┆ 5.7 ┆ 5.6 ┆ 5.4 ┆ 5.2 ┆ 5.0 ┆ 5.0 ┆ 4.9 ┆ 4.8 ┆ 4.8 ┆ 4.7 ┆ 4.7 ┆ 4.7 ┆ 4.7 ┆ 4.7 ┆ 4.6 ┆ 4.6 ┆ 4.6 ┆ 4.5 ┆ 4.5 ┆ 4.5 ┆ 4.5 ┆ 4.5 ┆ 1.8 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.1 ┆ 3.6 ┆ 4.0 ┆ 4.5 ┆ 5.0 ┆ 5.5 ┆ 6.0 ┆ 6.5 ┆ 7.0 ┆ 7.5 ┆ 8.0 ┆ 8.5 ┆ 8.9 ┆ 9.4 ┆ 9.8 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.8 ┆ 4.7 ┆ 5.8 ┆ 7.2 ┆ 8.7 ┆ 1.0 ┆ 1.2 ┆ 1.5 ┆ 1.7 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 2.9 ┆ 3.2 ┆ 3.5 ┆ 3.8 ┆ 4.1 ┆ 4.4 ┆ 4.7 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_3 ┆ .41 ┆ .13 ┆ .97 ┆ .21 ┆ .18 ┆ .83 ┆ .33 ┆ .23 ┆ .78 ┆ .37 ┆ .06 ┆ .78 ┆ .49 ┆ .73 ┆ .95 ┆ .71 ┆ .66 ┆ .44 ┆ .86 ┆ .69 ┆ .85 ┆ .82 ┆ .95 ┆ .45 ┆ .53 ┆ .90 ┆ .61 ┆ .63 ┆ .02 ┆ .02 ┆ .40 ┆ .36 ┆ .64 ┆ .00 ┆ .93 ┆ .80 ┆ .14 ┆ .53 ┆ .59 ┆ .32 ┆ .59 ┆ .52 ┆ .73 ┆ .41 ┆ .32 ┆ .38 ┆ .56 ┆ .78 ┆ .78 ┆ .61 ┆ .44 ┆ .25 ┆ .03 ┆ .83 ┆ .70 ┆ .59 ┆ .36 ┆ .33 ┆ .44 ┆ .65 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 727 ┆ 581 ┆ 970 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 004 ┆ 007 ┆ 009 ┆ 013 ┆ 015 ┆ 017 ┆ 018 ┆ 019 ┆ 019 ┆ 017 ┆ 015 ┆ 014 ┆ 014 ┆ 015 ┆ 024 ┆ 055 ┆ 076 ┆ 092 ┆ 103 ┆ 111 ┆ 115 ┆ 119 ┆ 125 ┆ 130 ┆ 135 ┆ 140 ┆ 146 ┆ 150 ┆ 154 ┆ 162 ┆ 167 ┆ 170 ┆ 170 ┆ 195 ┆ 385 ┆ 813 ┆ 176 ┆ 270 ┆ 512 ┆ 916 ┆ 226 ┆ 679 ┆ 905 ┆ 387 ┆ 993 ┆ 386 ┆ 132 ┆ 479 ┆     ┆     ┆     ┆     ┆     ┆     ┆ 306 ┆ 816 ┆ 617 ┆ 750 ┆ 384 ┆ 931 ┆ 353 ┆ 828 ┆ 236 ┆ 194 ┆ 905 ┆ 545 ┆ 186 ┆ 351 ┆ 022 ┆     ┆     ┆     ┆     ┆     ┆ 953 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 203 ┆ 390 ┆ 071 ┆ 567 ┆ 463 ┆ 386 ┆ 801 ┆ 007 ┆ 855 ┆ 884 ┆ 782 ┆ 052 ┆ 818 ┆ 869 ┆ 513 ┆ 659 ┆ 206 ┆ 182 ┆ 016 ┆ 009 ┆ 732 ┆ 264 ┆ 369 ┆ 237 ┆ 517 ┆ 348 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 101 ┆ 458 ┆ 849 ┆ 950 ┆ 531 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 408 ┆     ┆     ┆     ┆ 422 ┆ 707 ┆ 438 ┆     ┆ 524 ┆ 802 ┆ 470 ┆ .65 ┆ 279 ┆ .06 ┆ 530 ┆ 100 ┆ .25 ┆ 793 ┆ 578 ┆ 557 ┆ 065 ┆ 212 ┆ 278 ┆ 017 ┆ 590 ┆ 106 ┆ 314 ┆ 469 ┆ 051 ┆ 734 ┆ 425 ┆ 037 ┆ 483 ┆ 392 ┆ 191 ┆ 628 ┆ 036 ┆ 964 ┆ 169 ┆ 769 ┆ 378 ┆ 586 ┆ 791 ┆ 593 ┆ 303 ┆ 999 ┆ 627 ┆ 475 ┆ 580 ┆ 801 ┆ 821 ┆ .13 ┆ .23 ┆ .84 ┆ 942 ┆ 949 ┆ 185 ┆ 528 ┆ 958 ┆ 406 ┆ 896 ┆ 402 ┆ 908 ┆ 420 ┆ 736 ┆ 195 ┆ 809 ┆ 640 ┆ 339 ┆ .64 ┆ 232 ┆ 885 ┆ 244 ┆ 788 ┆ 366 ┆ 900 ┆ 913 ┆ 477 ┆ 085 ┆ 618 ┆ 699 ┆ 326 ┆ 623 ┆ 154 ┆ 376 ┆ 834 ┆ .39 ┆ .25 ┆ .44 ┆ .96 ┆ .80 ┆ .61 ┆ .26 ┆ .39 ┆ .26 ┆ .22 ┆ .02 ┆ .76 ┆ 361 ┆ 713 ┆ 097 ┆ 069 ┆ 301 ┆ 867 ┆ 407 ┆ 725 ┆ 443 ┆ 268 ┆ 520 ┆ 803 ┆ 355 ┆ 097 ┆ 511 ┆ 341 ┆ 276 ┆ 767 ┆ 052 ┆ 148 ┆ 168 ┆ 085 ┆ 953 ┆ 782 ┆ 562 ┆ 399 ┆ 240 ┆ 113 ┆ 029 ┆ 053 ┆ 671 ┆ .94 ┆ 095 ┆ 665 ┆ 114 ┆ 000 ┆ 076 ┆ 6   ┆ 542 ┆ 6   ┆ 542 ┆ .90 ┆     ┆     ┆     ┆     ┆ 920 ┆ 688 ┆ 815 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 905 ┆ 238 ┆ 105 ┆ 844 ┆ 369 ┆ 178 ┆ 360 ┆ 582 ┆ 421 ┆ 068 ┆ 286 ┆ 727 ┆ 320 ┆ 717 ┆ 208 ┆ 622 ┆ 034 ┆ 389 ┆ 554 ┆ 748 ┆ 021 ┆ 819 ┆ 618 ┆ 221 ┆ 234 ┆ 337 ┆ 674 ┆ 012 ┆ 384 ┆ 787 ┆ 249 ┆ 991 ┆ 749 ┆ 523 ┆ 311 ┆ 032 ┆ 711 ┆ 398 ┆ 090 ┆ 797 ┆ 562 ┆ 324 ┆ 140 ┆ 077 ┆ 501 ┆ 249 ┆ 376 ┆ 904 ┆ 833 ┆ 133 ┆ 745 ┆ 583 ┆ 557 ┆ 586 ┆ 617 ┆ 625 ┆ 597 ┆ 522 ┆ 373 ┆ 110 ┆ 688 ┆ 077 ┆ 265 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 830 ┆ 824 ┆ 800 ┆ 065 ┆ 863 ┆ 632 ┆ 738 ┆ 081 ┆ 625 ┆ 328 ┆ 159 ┆ 097 ┆ 129 ┆ 240 ┆ 406 ┆ 592 ┆ 759 ┆ 871 ┆ 911 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 956 ┆ 788 ┆ 775 ┆ 374 ┆ 847 ┆ 907 ┆ 965 ┆ 356 ┆ 163 ┆ 810 ┆ 941 ┆ 990 ┆ 734 ┆ 970 ┆ 083 ┆ 377 ┆ 866 ┆ 637 ┆ 918 ┆ 506 ┆ 884 ┆ 162 ┆ 850 ┆ 030 ┆ 647 ┆ 356 ┆ 614 ┆ 255 ┆ 828 ┆ 186 ┆ 330 ┆ 232 ┆ 898 ┆ 614 ┆ 244 ┆ 14  ┆ 131 ┆ 145 ┆ 537 ┆ 370 ┆ 713 ┆ 78  ┆ 806 ┆ 489 ┆ 881 ┆ 042 ┆ 680 ┆ 887 ┆ 451 ┆ 168 ┆ 291 ┆ 995 ┆ 550 ┆ 860 ┆ 850 ┆ 928 ┆ 754 ┆ 828 ┆ 505 ┆ 006 ┆ 093 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 04  ┆ 05  ┆ 11  ┆ 41  ┆ 88  ┆ 53  ┆ 77  ┆ 99  ┆ 45  ┆ 97  ┆ 12  ┆ 84  ┆ 26  ┆ 56  ┆ 3   ┆ 1   ┆ 77  ┆ 88  ┆ 56  ┆ 28  ┆ 44  ┆ 32  ┆ 59  ┆ 83  ┆ 52  ┆ 12  ┆ 17  ┆ 51  ┆ 78  ┆ 44  ┆ 48  ┆ 56  ┆ 97  ┆ 05  ┆ 51  ┆ 57  ┆ 14  ┆ 36  ┆ 14  ┆ 99  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-9 ┆     ┆     ┆     ┆     ┆     ┆ e-7 ┆ 2   ┆ 09  ┆ 16  ┆ 08  ┆ 2   ┆ 13  ┆ 1   ┆ 08  ┆ 11  ┆ 08  ┆ 11  ┆ 17  ┆ 14  ┆ 06  ┆ 02  ┆ e-8 ┆ e-9 ┆ e-9 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-9 ┆ e-8 ┆ e-1 ┆ e-9 ┆ e-8 ┆ 07  ┆ 16  ┆ 18  ┆ 17  ┆ 19  ┆ 2   ┆ 16  ┆ 04  ┆ 02  ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-9 ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-1 ┆     ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆     ┆ 008 ┆ 658 ┆ 289 ┆ 099 ┆ 38  ┆ 830 ┆ 987 ┆ 695 ┆ 407 ┆ 831 ┆ 696 ┆ 02  ┆ 155 ┆ 529 ┆ 644 ┆ 981 ┆ 58  ┆ 525 ┆ 203 ┆ 862 ┆ 703 ┆ 808 ┆ 833 ┆ 842 ┆ 579 ┆ 413 ┆ 96  ┆ 55  ┆ 171 ┆ 985 ┆ 26  ┆ 084 ┆ 007 ┆ 056 ┆ 198 ┆ 07  ┆ 99  ┆ 18  ┆ 69  ┆ 358 ┆ 935 ┆ 777 ┆ 074 ┆ 747 ┆ 459 ┆ 138 ┆ 276 ┆ 552 ┆ 709 ┆ 274 ┆ 012 ┆ 455 ┆ 166 ┆ 691 ┆ 382 ┆ 759 ┆ 797 ┆ 257 ┆ 322 ┆ 99  ┆ 814 ┆ 409 ┆ 021 ┆ 605 ┆ 42  ┆ 48  ┆ 504 ┆ 303 ┆ 13  ┆ 247 ┆ 5   ┆ 09  ┆ 59  ┆ 075 ┆ 963 ┆ 821 ┆ 85  ┆ 551 ┆ 708 ┆ 599 ┆ 733 ┆ 416 ┆ 124 ┆ 175 ┆ 279 ┆ 923 ┆ 294 ┆ 029 ┆ 861 ┆ 185 ┆ 573 ┆ 737 ┆ 246 ┆ 601 ┆ 056 ┆ 39  ┆ 07  ┆ 13  ┆ 03  ┆ 73  ┆ 98  ┆     ┆     ┆ 37  ┆ 34  ┆ 68  ┆ 45  ┆ 151 ┆ 228 ┆ 744 ┆ 753 ┆ 72  ┆ 024 ┆ 604 ┆ 62  ┆ 534 ┆ 04  ┆ 426 ┆ 615 ┆ 54  ┆ .58 ┆ 506 ┆ 183 ┆ 3   ┆ 82  ┆ 041 ┆ 85  ┆     ┆ 07  ┆     ┆ 07  ┆ 763 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 09  ┆ 14  ┆ 16  ┆ 17  ┆ 16  ┆ 13  ┆ 09  ┆ 06  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 95  ┆ 1   ┆ 7   ┆ 5   ┆ 9   ┆ 5   ┆     ┆ 9   ┆ 9   ┆ 1   ┆     ┆ 9   ┆ 9   ┆ 5   ┆ 1   ┆ 6   ┆ 4   ┆ 1   ┆ 4   ┆ 7   ┆ 1   ┆ 8   ┆ 1   ┆ 2   ┆ 5   ┆ 9   ┆ 7   ┆ 2   ┆ 2   ┆ 3   ┆ 2   ┆ 8   ┆ 6   ┆ 4   ┆ 4   ┆     ┆ 6   ┆ 5   ┆ 5   ┆ 5   ┆ 2   ┆     ┆ 4   ┆ 5   ┆ 2   ┆ 8   ┆ 3   ┆ 4   ┆ 2   ┆ 8   ┆ 8   ┆ 5   ┆ 6   ┆ 6   ┆ 3   ┆ 5   ┆ 5   ┆ 7   ┆ 4   ┆ 8   ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 9   ┆ 9   ┆ 0   ┆ 5   ┆ 8   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 5   ┆ 9   ┆ 5   ┆ 1   ┆ 6   ┆ 2   ┆ 8   ┆ 4   ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 3   ┆ 3   ┆ 9   ┆ 2   ┆ 7   ┆ 7   ┆ 1   ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆     ┆     ┆     ┆ 4   ┆ 3   ┆ 3   ┆     ┆     ┆     ┆     ┆ 1   ┆     ┆ 8   ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆ 1   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆ 7   ┆ 3   ┆ 7   ┆ 5   ┆ 5   ┆ 7   ┆ 5   ┆ 5   ┆ 7   ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 323 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 6   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 215 ┆ 232 ┆ 240 ┆ 245 ┆ 259 ┆ 262 ┆ 257 ┆ 250 ┆ 238 ┆ 229 ┆ 224 ┆ 220 ┆ 214 ┆ 209 ┆ 203 ┆ 195 ┆ 190 ┆ 186 ┆ 188 ┆ 191 ┆ 197 ┆ 203 ┆ 209 ┆ 216 ┆ 222 ┆ 228 ┆ 234 ┆ 238 ┆ 243 ┆ 247 ┆ 250 ┆ 253 ┆ 256 ┆ 260 ┆ 263 ┆ 267 ┆ 270 ┆ 273 ┆ 276 ┆ 278 ┆ 280 ┆ 281 ┆ 282 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 291 ┆ 291 ┆ 292 ┆ 293 ┆ 294 ┆ 295 ┆ 296 ┆ 297 ┆ 298 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.8 ┆ 7.9 ┆ 8.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 9.5 ┆ 8.6 ┆ 7.6 ┆ 6.6 ┆ 5.3 ┆ 4.4 ┆ 3.1 ┆ 1.7 ┆ 5.1 ┆ 1.2 ┆ 5.1 ┆ 8.1 ┆ 9.3 ┆ 1.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 6.8 ┆ 7.6 ┆ 7.1 ┆ 2.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.1 ┆ 0.0 ┆ 0.0 ┆ 1.7 ┆ 1.4 ┆ 1.6 ┆ 3.9 ┆ 1.6 ┆ 0.0 ┆ 8.2 ┆ 0.0 ┆ 5.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 5.5 ┆ 1.4 ┆ 5.1 ┆ 7.2 ┆ 6.2 ┆ 6.5 ┆ 7.9 ┆ 9.4 ┆ 8.6 ┆ 1.0 ┆ 8.7 ┆ 5.9 ┆ 2.7 ┆ 1.5 ┆ 8.2 ┆ 1.4 ┆ 2.1 ┆ 2.2 ┆ 1.8 ┆ 1.9 ┆ 8.4 ┆ 1.5 ┆ 6.6 ┆ 1.1 ┆ 6.7 ┆ 1.3 ┆ 6.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.2 ┆ 3.2 ┆ 5.2 ┆ 1.9 ┆ 0.0 ┆ 2.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 2.6 ┆ 1.1 ┆ 3.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.8 ┆ 18. ┆ 17. ┆ -1. ┆ -1. ┆ 2.6 ┆ -14 ┆ -7. ┆ -6. ┆ -8. ┆ -3. ┆ -3. ┆ -2. ┆ -3. ┆ -2. ┆ -3. ┆ -1. ┆ 9.9 ┆ 14. ┆ 14. ┆ 17. ┆ 23. ┆ 28. ┆ 30. ┆ 31. ┆ 31. ┆ 29. ┆ 26. ┆ 22. ┆ 20. ┆ 18. ┆ 17. ┆ 16. ┆ 15. ┆ 13. ┆ 11. ┆ 9.3 ┆ 6.9 ┆ 4.2 ┆ 0.6 ┆ -3. ┆ -7. ┆ -9. ┆ -10 ┆ -10 ┆ -10 ┆ -9. ┆ -8. ┆ -7. ┆ -6. ┆ -5. ┆ -5. ┆ -4. ┆ -4. ┆ -3. ┆ -2. ┆ -2. ┆ -1. ┆ -1. ┆ -1. ┆ -0. ┆ -10 ┆ -6. ┆ 13. ┆ 13. ┆ -0. ┆ -0. ┆ -2. ┆ 1.5 ┆ 1.7 ┆ -1. ┆ -1. ┆ -0. ┆ 0.6 ┆ 0.2 ┆ 0.3 ┆ -1. ┆ 3.0 ┆ 6.7 ┆ -1. ┆ -14 ┆ -22 ┆ -24 ┆ -23 ┆ -21 ┆ -21 ┆ -19 ┆ -16 ┆ -12 ┆ -8. ┆ -5. ┆ -2. ┆ -0. ┆ 2.3 ┆ 3.8 ┆ 4.1 ┆ 3.9 ┆ 3.4 ┆ 3.1 ┆ 2.9 ┆ 2.6 ┆ 2.4 ┆ 1.9 ┆ 1.6 ┆ 1.2 ┆ 0.8 ┆ 0.4 ┆ 0.0 ┆ -0. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 100 ┆ 654 ┆ 34. ┆ 1.6 ┆ 0.0 ┆ 0.0 ┆ 0.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 458 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 0.0 ┆ 3.0 ┆ 5.5 ┆ 9.9 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.1 ┆ 2.3 ┆ 1.2 ┆ 8.8 ┆ 7.5 ┆ 6.7 ┆ 6.1 ┆ 5.7 ┆ 5.5 ┆ 5.4 ┆ 5.3 ┆ 5.3 ┆ 5.4 ┆ 5.5 ┆ 5.6 ┆ 5.7 ┆ 5.9 ┆ 6.0 ┆ 6.2 ┆ 6.4 ┆ 6.4 ┆ 6.4 ┆ 6.4 ┆ 6.2 ┆ 6.0 ┆ 5.8 ┆ 5.6 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.5 ┆ 5.4 ┆ 5.4 ┆ 5.3 ┆ 5.3 ┆ 5.2 ┆ 5.2 ┆ 5.1 ┆ 5.1 ┆ 1.8 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.1 ┆ 3.6 ┆ 4.0 ┆ 4.5 ┆ 5.0 ┆ 5.5 ┆ 6.0 ┆ 6.5 ┆ 7.0 ┆ 7.5 ┆ 8.0 ┆ 8.5 ┆ 8.9 ┆ 9.4 ┆ 9.8 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.8 ┆ 4.7 ┆ 5.8 ┆ 7.1 ┆ 8.7 ┆ 1.0 ┆ 1.2 ┆ 1.5 ┆ 1.7 ┆ 2.0 ┆ 2.3 ┆ 2.6 ┆ 2.9 ┆ 3.2 ┆ 3.5 ┆ 3.8 ┆ 4.1 ┆ 4.4 ┆ 4.7 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_8 ┆ .13 ┆ .70 ┆ .14 ┆ .66 ┆ .14 ┆ .64 ┆ .05 ┆ .79 ┆ .52 ┆ .84 ┆ .78 ┆ .42 ┆ .78 ┆ .43 ┆ .06 ┆ .09 ┆ .14 ┆ .09 ┆ .00 ┆ .54 ┆ .89 ┆ .69 ┆ .94 ┆ .05 ┆ .50 ┆ .64 ┆ .13 ┆ .98 ┆ .53 ┆ .31 ┆ .69 ┆ .62 ┆ .73 ┆ .46 ┆ .92 ┆ .40 ┆ .66 ┆ .73 ┆ .56 ┆ .75 ┆ .38 ┆ .71 ┆ .32 ┆ .21 ┆ .41 ┆ .80 ┆ .78 ┆ .81 ┆ .81 ┆ .71 ┆ .61 ┆ .31 ┆ .84 ┆ .61 ┆ .39 ┆ .26 ┆ .21 ┆ .28 ┆ .48 ┆ .73 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 678 ┆ 174 ┆ 687 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 004 ┆ 006 ┆ 009 ┆ 012 ┆ 016 ┆ 017 ┆ 017 ┆ 016 ┆ 014 ┆ 013 ┆ 013 ┆ 014 ┆ 020 ┆ 035 ┆ 050 ┆ 073 ┆ 090 ┆ 100 ┆ 108 ┆ 114 ┆ 118 ┆ 123 ┆ 129 ┆ 133 ┆ 137 ┆ 145 ┆ 150 ┆ 155 ┆ 160 ┆ 165 ┆ 168 ┆ 170 ┆ 170 ┆ 149 ┆ 160 ┆ 333 ┆ 879 ┆ 545 ┆ 905 ┆ 734 ┆ 063 ┆ 183 ┆ 117 ┆ 482 ┆ 465 ┆ 227 ┆ 437 ┆ 528 ┆     ┆     ┆     ┆     ┆     ┆ 628 ┆ 737 ┆ 301 ┆ 488 ┆     ┆     ┆     ┆     ┆     ┆ 595 ┆     ┆     ┆ 669 ┆ 851 ┆ 483 ┆ 592 ┆ 248 ┆     ┆ 662 ┆ 000 ┆ 830 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 911 ┆ 527 ┆ 456 ┆ 670 ┆ 296 ┆ 300 ┆ 199 ┆ 948 ┆ 921 ┆ 068 ┆ 134 ┆ 803 ┆ 788 ┆ 869 ┆ 614 ┆ 209 ┆ 172 ┆ 504 ┆ 651 ┆ 827 ┆ 050 ┆ 358 ┆ 995 ┆ 585 ┆ 242 ┆ 722 ┆ 314 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 095 ┆ 791 ┆ 340 ┆ 285 ┆     ┆ 510 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 102 ┆ 740 ┆ 012 ┆     ┆     ┆     ┆ 479 ┆ 722 ┆ 760 ┆ 939 ┆ 779 ┆ 647 ┆ .81 ┆ 129 ┆ 119 ┆ 868 ┆ 867 ┆ 596 ┆ 567 ┆ 202 ┆ 468 ┆ 474 ┆ 260 ┆ 276 ┆ 317 ┆ 355 ┆ 531 ┆ 833 ┆ 676 ┆ 479 ┆ 606 ┆ 529 ┆ 526 ┆ 226 ┆ 555 ┆ 001 ┆ 341 ┆ 300 ┆ 404 ┆ 029 ┆ 308 ┆ 297 ┆ 167 ┆ 636 ┆ 588 ┆ 719 ┆ 633 ┆ 402 ┆ 951 ┆ .98 ┆ .95 ┆ .45 ┆ 384 ┆ 314 ┆ 342 ┆ 588 ┆ 945 ┆ 321 ┆ 750 ┆ 135 ┆ 448 ┆ 792 ┆ 184 ┆ 566 ┆ 170 ┆ 053 ┆ 982 ┆ .32 ┆ 535 ┆ 815 ┆ 229 ┆ 639 ┆ 859 ┆ 666 ┆ 567 ┆ 950 ┆ 969 ┆ 053 ┆ 142 ┆ 559 ┆ 612 ┆ 677 ┆ 940 ┆ 791 ┆ 838 ┆ 096 ┆ .55 ┆ .15 ┆ .85 ┆ .14 ┆ .89 ┆ .15 ┆ .66 ┆ .87 ┆ .76 ┆ 934 ┆ 541 ┆ 916 ┆ 261 ┆ 708 ┆ 730 ┆ 084 ┆ 338 ┆ 935 ┆ 754 ┆ 371 ┆ 487 ┆ 097 ┆ 850 ┆ 202 ┆ 092 ┆ 600 ┆ 755 ┆ 074 ┆ 521 ┆ 020 ┆ 433 ┆ 737 ┆ 921 ┆ 984 ┆ 916 ┆ 683 ┆ 347 ┆ 017 ┆ 675 ┆ 550 ┆ 479 ┆ 274 ┆ .96 ┆ 293 ┆ 925 ┆ 037 ┆ 021 ┆ 655 ┆ 6   ┆ 872 ┆ 6   ┆ 872 ┆ .87 ┆     ┆     ┆     ┆     ┆ 892 ┆ 639 ┆ 727 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 570 ┆ 401 ┆ 068 ┆ 657 ┆ 348 ┆ 453 ┆ 790 ┆ 566 ┆ 494 ┆ 143 ┆ 332 ┆ 835 ┆ 537 ┆ 551 ┆ 623 ┆ 807 ┆ 063 ┆ 487 ┆ 320 ┆ 197 ┆ 768 ┆ 460 ┆ 153 ┆ 693 ┆ 425 ┆ 260 ┆ 232 ┆ 930 ┆ 642 ┆ 369 ┆ 110 ┆ 123 ┆ 151 ┆ 177 ┆ 202 ┆ 037 ┆ 532 ┆ 040 ┆ 554 ┆ 068 ┆ 618 ┆ 167 ┆ 717 ┆ 601 ┆ 497 ┆ 244 ┆ 371 ┆ 898 ┆ 826 ┆ 125 ┆ 735 ┆ 573 ┆ 545 ┆ 573 ┆ 603 ┆ 610 ┆ 581 ┆ 505 ┆ 355 ┆ 090 ┆ 668 ┆ 056 ┆ 243 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 754 ┆ 736 ┆ 700 ┆ 951 ┆ 736 ┆ 618 ┆ 723 ┆ 065 ┆ 607 ┆ 309 ┆ 140 ┆ 078 ┆ 109 ┆ 220 ┆ 387 ┆ 573 ┆ 740 ┆ 853 ┆ 893 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 894 ┆ 577 ┆ 476 ┆ 191 ┆ 211 ┆ 696 ┆ 789 ┆ 190 ┆ 876 ┆ 877 ┆ 449 ┆ 855 ┆ 946 ┆ 160 ┆ 019 ┆ 389 ┆ 745 ┆ 843 ┆ 06  ┆ 273 ┆ 113 ┆ 570 ┆ 343 ┆ 093 ┆ 236 ┆ 310 ┆ 634 ┆ 903 ┆ 847 ┆ 586 ┆ 566 ┆ 809 ┆ 759 ┆ 388 ┆ 663 ┆ 118 ┆ 746 ┆ 238 ┆ 471 ┆ 196 ┆ 437 ┆ 076 ┆ 261 ┆ 921 ┆ 974 ┆ 004 ┆ 168 ┆ 303 ┆ 357 ┆ 292 ┆ 316 ┆ 327 ┆ 538 ┆ 848 ┆ 163 ┆ 456 ┆ 286 ┆ 424 ┆ 421 ┆ 699 ┆ 142 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 05  ┆ 08  ┆ 19  ┆ 48  ┆ 98  ┆ 79  ┆ 96  ┆ 66  ┆ 69  ┆ 63  ┆ 98  ┆ 02  ┆ 45  ┆ 47  ┆ 22  ┆ 38  ┆ 29  ┆ 24  ┆ 42  ┆ 77  ┆ 08  ┆ 5   ┆ 34  ┆ 6   ┆ 82  ┆ 34  ┆ 41  ┆ 46  ┆ 97  ┆ 11  ┆ 25  ┆ 66  ┆ 04  ┆ 81  ┆ 99  ┆ 94  ┆ 76  ┆ 94  ┆ 09  ┆ 8   ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-5 ┆ e-4 ┆ e-4 ┆     ┆     ┆     ┆     ┆     ┆ e-1 ┆     ┆     ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-9 ┆     ┆ e-8 ┆ 02  ┆ e-7 ┆ 03  ┆ 03  ┆ 04  ┆ 18  ┆ 03  ┆ 04  ┆ 03  ┆ 13  ┆ 08  ┆ 1   ┆ 12  ┆ 37  ┆ 48  ┆ 16  ┆ 1   ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-1 ┆ e-8 ┆ e-1 ┆ e-8 ┆ e-8 ┆ 01  ┆ 01  ┆ 01  ┆ 02  ┆ 04  ┆ 05  ┆ 06  ┆ 04  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-9 ┆     ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆     ┆     ┆     ┆ e-1 ┆ 166 ┆ 093 ┆ 848 ┆ 939 ┆ 98  ┆ 242 ┆ 902 ┆ 783 ┆ 477 ┆ 754 ┆ 304 ┆ 24  ┆ 521 ┆ 386 ┆ 392 ┆ 273 ┆ 5   ┆ 554 ┆ 944 ┆ 598 ┆ 913 ┆ 099 ┆ 286 ┆ 482 ┆ 93  ┆ 812 ┆ 882 ┆ 32  ┆ 379 ┆ 267 ┆ 985 ┆ 362 ┆ 361 ┆ 423 ┆ 323 ┆ 09  ┆ 97  ┆ 2   ┆ 01  ┆ 249 ┆ 048 ┆ 46  ┆ 188 ┆ 976 ┆ 064 ┆ 851 ┆ 138 ┆ 307 ┆ 866 ┆ 333 ┆ 458 ┆ 257 ┆ 325 ┆ 58  ┆ 322 ┆ 244 ┆ 916 ┆ 932 ┆ 412 ┆ 516 ┆ 659 ┆ 773 ┆ 284 ┆ 978 ┆ 377 ┆ 063 ┆ 459 ┆ 82  ┆ 09  ┆ 189 ┆ 233 ┆ 041 ┆ 43  ┆ 01  ┆ 95  ┆ 656 ┆ 26  ┆ 54  ┆ 384 ┆ 769 ┆ 655 ┆ 133 ┆ 228 ┆ 060 ┆ 972 ┆ 498 ┆ 406 ┆ 676 ┆ 568 ┆ 738 ┆ 226 ┆ 89  ┆ 63  ┆ 87  ┆ 3   ┆ 22  ┆ 04  ┆ 73  ┆ 82  ┆ 15  ┆ 12  ┆ 27  ┆ 82  ┆ 34  ┆ 2   ┆ 84  ┆ 63  ┆ 46  ┆ 854 ┆ 941 ┆ 137 ┆ 7   ┆ 218 ┆ 981 ┆ 63  ┆ 555 ┆ 434 ┆ 494 ┆ 38  ┆ 87  ┆ .20 ┆ 609 ┆ 045 ┆ 16  ┆ 38  ┆ 62  ┆ 42  ┆     ┆ 62  ┆     ┆ 62  ┆ 069 ┆     ┆     ┆     ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 09  ┆ 14  ┆ 16  ┆ 17  ┆ 16  ┆ 13  ┆ 09  ┆ 06  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 2   ┆ 2   ┆ 7   ┆ 2   ┆ 3   ┆ 9   ┆ 5   ┆ 4   ┆     ┆ 8   ┆ 9   ┆ 2   ┆ 8   ┆ 1   ┆ 5   ┆ 6   ┆     ┆ 1   ┆     ┆ 8   ┆ 5   ┆ 4   ┆ 1   ┆ 2   ┆ 5   ┆ 2   ┆ 3   ┆ 8   ┆ 5   ┆ 3   ┆     ┆ 7   ┆     ┆ 3   ┆ 8   ┆ 8   ┆ 1   ┆ 6   ┆ 8   ┆ 8   ┆ 9   ┆ 1   ┆ 7   ┆ 5   ┆ 4   ┆ 4   ┆ 9   ┆     ┆ 8   ┆ 2   ┆ 5   ┆ 2   ┆ 1   ┆ 3   ┆ 3   ┆ 3   ┆ 8   ┆ 7   ┆     ┆ 9   ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 9   ┆ 9   ┆ 0   ┆ 4   ┆ 9   ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 4   ┆ 9   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆ 8   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 1   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 3   ┆ 3   ┆ 7   ┆ 2   ┆ 2   ┆ 7   ┆ 2   ┆     ┆ 0   ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 3   ┆ 4   ┆     ┆     ┆     ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 9   ┆ 5   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 2   ┆ 4   ┆ 7   ┆ 4   ┆ 7   ┆ 9   ┆ 7   ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 343 ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 214 ┆ 233 ┆ 238 ┆ 248 ┆ 255 ┆ 261 ┆ 257 ┆ 247 ┆ 237 ┆ 228 ┆ 224 ┆ 220 ┆ 214 ┆ 209 ┆ 203 ┆ 199 ┆ 193 ┆ 189 ┆ 190 ┆ 193 ┆ 198 ┆ 204 ┆ 210 ┆ 216 ┆ 222 ┆ 227 ┆ 232 ┆ 236 ┆ 240 ┆ 244 ┆ 247 ┆ 250 ┆ 253 ┆ 256 ┆ 258 ┆ 261 ┆ 264 ┆ 267 ┆ 270 ┆ 273 ┆ 276 ┆ 278 ┆ 279 ┆ 280 ┆ 281 ┆ 282 ┆ 283 ┆ 284 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 292 ┆ 293 ┆ 294 ┆ 295 ┆ 297 ┆ 298 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 9.7 ┆ 8.5 ┆ 7.5 ┆ 6.6 ┆ 5.4 ┆ 5.2 ┆ 3.8 ┆ 3.3 ┆ 2.1 ┆ 2.0 ┆ 1.0 ┆ 5.4 ┆ 1.1 ┆ 5.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.2 ┆ 1.8 ┆ 3.9 ┆ 2.5 ┆ 8.6 ┆ 3.1 ┆ 1.3 ┆ 1.8 ┆ 2.9 ┆ 2.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 3.9 ┆ 7.2 ┆ 1.1 ┆ 1.0 ┆ 1.7 ┆ 1.8 ┆ 0.0 ┆ 8.8 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.5 ┆ 5.0 ┆ 1.2 ┆ 3.9 ┆ 6.9 ┆ 1.0 ┆ 3.2 ┆ 6.1 ┆ 6.4 ┆ 6.7 ┆ 7.6 ┆ 7.5 ┆ 8.6 ┆ 6.2 ┆ 4.0 ┆ 2.3 ┆ 1.2 ┆ 8.6 ┆ 3.7 ┆ 1.7 ┆ 3.8 ┆ 5.8 ┆ 1.0 ┆ 1.4 ┆ 1.2 ┆ 8.0 ┆ 1.3 ┆ 6.4 ┆ 9.6 ┆ 3.9 ┆ 6.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 3.0 ┆ 2.8 ┆ 1.4 ┆ 1.4 ┆ 1.1 ┆ 0.0 ┆ 1.1 ┆ 9.3 ┆ 1.1 ┆ 7.1 ┆ 8.4 ┆ 5.6 ┆ 3.8 ┆ 3.3 ┆ 5.0 ┆ 4.7 ┆ 3.0 ┆ 2.7 ┆ 2.4 ┆ 0.0 ┆ 8.1 ┆ 2.1 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 51. ┆ 37. ┆ 38. ┆ 41. ┆ 38. ┆ 30. ┆ 30. ┆ 25. ┆ 18. ┆ 12. ┆ 8.2 ┆ 5.3 ┆ 2.9 ┆ 1.0 ┆ 0.7 ┆ 6.6 ┆ 13. ┆ 20. ┆ 27. ┆ 33. ┆ 40. ┆ 47. ┆ 51. ┆ 51. ┆ 50. ┆ 47. ┆ 43. ┆ 37. ┆ 30. ┆ 25. ┆ 22. ┆ 21. ┆ 20. ┆ 19. ┆ 17. ┆ 16. ┆ 14. ┆ 12. ┆ 10. ┆ 8.9 ┆ 7.1 ┆ 5.4 ┆ 4.0 ┆ 3.0 ┆ 1.9 ┆ 0.8 ┆ -0. ┆ -1. ┆ -2. ┆ -3. ┆ -4. ┆ -5. ┆ -5. ┆ -5. ┆ -5. ┆ -5. ┆ -4. ┆ -4. ┆ -4. ┆ -4. ┆ -1. ┆ -13 ┆ -5. ┆ 10. ┆ 4.0 ┆ 13. ┆ -2. ┆ -1. ┆ -0. ┆ 1.3 ┆ 1.7 ┆ 0.6 ┆ 0.2 ┆ 0.3 ┆ -0. ┆ -0. ┆ 0.1 ┆ -3. ┆ -6. ┆ -11 ┆ -15 ┆ -15 ┆ -10 ┆ -8. ┆ -9. ┆ -11 ┆ -13 ┆ -14 ┆ -12 ┆ -8. ┆ -4. ┆ -1. ┆ 0.3 ┆ 0.9 ┆ 1.3 ┆ 1.7 ┆ 1.7 ┆ 1.5 ┆ 1.3 ┆ 1.3 ┆ 1.3 ┆ 1.6 ┆ 2.0 ┆ 2.2 ┆ 2.1 ┆ 1.6 ┆ 1.0 ┆ 0.5 ┆ 0.0 ┆ -0. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ 987 ┆ 724 ┆ 185 ┆ 25. ┆ 0.0 ┆ 0.0 ┆ 0.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 462 ┆ 0.0 ┆ 0.1 ┆ 0.8 ┆ 0.0 ┆ 3.0 ┆ 5.4 ┆ 9.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.4 ┆ 2.0 ┆ 1.2 ┆ 9.7 ┆ 7.9 ┆ 6.8 ┆ 6.1 ┆ 6.0 ┆ 5.9 ┆ 5.9 ┆ 6.0 ┆ 6.1 ┆ 6.2 ┆ 6.3 ┆ 6.4 ┆ 6.5 ┆ 6.6 ┆ 6.7 ┆ 6.8 ┆ 6.8 ┆ 6.8 ┆ 6.8 ┆ 6.9 ┆ 6.8 ┆ 6.8 ┆ 6.8 ┆ 6.8 ┆ 6.7 ┆ 6.6 ┆ 6.6 ┆ 6.5 ┆ 6.5 ┆ 6.5 ┆ 6.5 ┆ 6.5 ┆ 6.5 ┆ 6.5 ┆ 6.6 ┆ 6.7 ┆ 6.7 ┆ 6.8 ┆ 6.8 ┆ 6.8 ┆ 1.8 ┆ 2.0 ┆ 2.3 ┆ 2.7 ┆ 3.1 ┆ 3.5 ┆ 3.9 ┆ 4.4 ┆ 4.9 ┆ 5.4 ┆ 5.9 ┆ 6.4 ┆ 6.8 ┆ 7.3 ┆ 7.8 ┆ 8.3 ┆ 8.7 ┆ 9.1 ┆ 9.6 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.4 ┆ 4.2 ┆ 5.2 ┆ 6.4 ┆ 7.9 ┆ 9.7 ┆ 1.1 ┆ 1.3 ┆ 1.6 ┆ 1.9 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.0 ┆ 3.3 ┆ 3.6 ┆ 4.0 ┆ 4.3 ┆ 4.6 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_7 ┆ .90 ┆ .68 ┆ .15 ┆ .79 ┆ .43 ┆ .87 ┆ .73 ┆ .84 ┆ .01 ┆ .72 ┆ .29 ┆ .07 ┆ .65 ┆ .63 ┆ .13 ┆ .30 ┆ .92 ┆ .00 ┆ .08 ┆ .27 ┆ .85 ┆ .43 ┆ .75 ┆ .07 ┆ .25 ┆ .99 ┆ .82 ┆ .83 ┆ .40 ┆ .05 ┆ .40 ┆ .76 ┆ .87 ┆ .35 ┆ .63 ┆ .27 ┆ .39 ┆ .52 ┆ .58 ┆ .46 ┆ .04 ┆ .10 ┆ .69 ┆ .59 ┆ .03 ┆ .10 ┆ .29 ┆ .58 ┆ .76 ┆ .92 ┆ .77 ┆ .76 ┆ .70 ┆ .89 ┆ .12 ┆ .35 ┆ .57 ┆ .81 ┆ .05 ┆ .32 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 002 ┆ 003 ┆ 003 ┆ 002 ┆ 002 ┆ 002 ┆ 004 ┆ 008 ┆ 014 ┆ 018 ┆ 020 ┆ 021 ┆ 020 ┆ 019 ┆ 020 ┆ 022 ┆ 027 ┆ 037 ┆ 056 ┆ 070 ┆ 080 ┆ 085 ┆ 092 ┆ 097 ┆ 105 ┆ 112 ┆ 122 ┆ 125 ┆ 126 ┆ 126 ┆ 127 ┆ 128 ┆ 128 ┆ 129 ┆ 240 ┆ 601 ┆ 978 ┆ 063 ┆ 207 ┆ 017 ┆ 994 ┆ 816 ┆ 322 ┆ 390 ┆ 676 ┆ 688 ┆ 397 ┆ 950 ┆ 025 ┆     ┆     ┆     ┆     ┆     ┆ 262 ┆ 205 ┆ 505 ┆ 968 ┆ 021 ┆ 059 ┆ 330 ┆ 119 ┆ 816 ┆ 595 ┆     ┆     ┆     ┆ 384 ┆ 420 ┆ 810 ┆ 490 ┆ 525 ┆ 097 ┆     ┆ 280 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 407 ┆ 271 ┆ 904 ┆ 397 ┆ 656 ┆ 900 ┆ 887 ┆ 281 ┆ 930 ┆ 827 ┆ 458 ┆ 475 ┆ 658 ┆ 857 ┆ 058 ┆ 955 ┆ 466 ┆ 311 ┆ 115 ┆ 075 ┆ 695 ┆ 055 ┆ 269 ┆ 395 ┆ 922 ┆ 708 ┆ 658 ┆ 196 ┆ 679 ┆ 776 ┆ 408 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 622 ┆ 218 ┆ 132 ┆ 059 ┆ 835 ┆     ┆ 867 ┆ 804 ┆ 343 ┆ 379 ┆ 519 ┆ 288 ┆ 800 ┆ 601 ┆ 977 ┆ 965 ┆ 257 ┆ 207 ┆ 084 ┆     ┆ 785 ┆ 322 ┆     ┆     ┆     ┆     ┆     ┆ 985 ┆ 025 ┆ 208 ┆ 439 ┆ 001 ┆ 190 ┆ 021 ┆ 929 ┆ 735 ┆ 256 ┆ 900 ┆ 552 ┆ 540 ┆ 040 ┆ 584 ┆ 197 ┆ 552 ┆ 842 ┆ 181 ┆ 445 ┆ 895 ┆ 974 ┆ 218 ┆ 514 ┆ 413 ┆ 723 ┆ 560 ┆ 232 ┆ 558 ┆ 744 ┆ 663 ┆ 501 ┆ 784 ┆ 586 ┆ 881 ┆ 178 ┆ 704 ┆ 959 ┆ 988 ┆ 574 ┆ 662 ┆ 211 ┆ 739 ┆ 309 ┆ 353 ┆ 984 ┆ 237 ┆ 384 ┆ 503 ┆ 631 ┆ 770 ┆ 135 ┆ 241 ┆ 185 ┆ 096 ┆ 027 ┆ 974 ┆ 901 ┆ 805 ┆ 648 ┆ 965 ┆ .12 ┆ 666 ┆ 940 ┆ 355 ┆ 363 ┆ 621 ┆ 983 ┆ 392 ┆ 798 ┆ 694 ┆ 867 ┆ 768 ┆ 269 ┆ 402 ┆ 026 ┆ 934 ┆ 025 ┆ 598 ┆ .72 ┆ .12 ┆ .23 ┆ .11 ┆ 825 ┆ 523 ┆ .55 ┆ .71 ┆ .10 ┆ .71 ┆ 868 ┆ 939 ┆ 703 ┆ 219 ┆ 540 ┆ 693 ┆ 359 ┆ 569 ┆ 373 ┆ 179 ┆ 207 ┆ 611 ┆ 268 ┆ 542 ┆ 875 ┆ 286 ┆ 673 ┆ 955 ┆ 523 ┆ 280 ┆ 460 ┆ 217 ┆ 664 ┆ 227 ┆ 426 ┆ 512 ┆ 519 ┆ 584 ┆ 594 ┆ 590 ┆ 542 ┆ 62. ┆ .00 ┆ .43 ┆ 138 ┆ 613 ┆ 335 ┆ 146 ┆ 844 ┆ 934 ┆ 604 ┆ 707 ┆ .39 ┆     ┆ 247 ┆ 752 ┆     ┆ 082 ┆ 180 ┆ 112 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 315 ┆ 092 ┆ 922 ┆ 813 ┆ 838 ┆ 105 ┆ 687 ┆ 120 ┆ 512 ┆ 809 ┆ 668 ┆ 699 ┆ 575 ┆ 422 ┆ 365 ┆ 408 ┆ 470 ┆ 327 ┆ 205 ┆ 819 ┆ 903 ┆ 986 ┆ 011 ┆ 856 ┆ 708 ┆ 568 ┆ 128 ┆ 445 ┆ 796 ┆ 180 ┆ 696 ┆ 551 ┆ 416 ┆ 290 ┆ 170 ┆ 273 ┆ 925 ┆ 569 ┆ 213 ┆ 861 ┆ 092 ┆ 143 ┆ 196 ┆ 082 ┆ 767 ┆ 824 ┆ 272 ┆ 112 ┆ 314 ┆ 821 ┆ 550 ┆ 411 ┆ 326 ┆ 243 ┆ 137 ┆ 997 ┆ 811 ┆ 552 ┆ 181 ┆ 656 ┆ 945 ┆ 038 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 248 ┆ 493 ┆ 630 ┆ 969 ┆ 770 ┆ 180 ┆ 718 ┆ 957 ┆ 400 ┆ 012 ┆ 760 ┆ 624 ┆ 592 ┆ 649 ┆ 771 ┆ 923 ┆ 063 ┆ 159 ┆ 190 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 938 ┆ 118 ┆ 025 ┆ 687 ┆ 014 ┆ 833 ┆ 309 ┆ 480 ┆ 304 ┆ 243 ┆ 790 ┆ 236 ┆ 939 ┆ 319 ┆ 926 ┆ 867 ┆ 208 ┆ 160 ┆ 081 ┆ 849 ┆ 943 ┆ 863 ┆ 528 ┆ 070 ┆ 604 ┆ 030 ┆ 032 ┆ 634 ┆ 737 ┆ 277 ┆ 168 ┆ 132 ┆ 600 ┆ 800 ┆ 643 ┆ 842 ┆ 543 ┆ 608 ┆ 172 ┆ 689 ┆ 174 ┆ 715 ┆ 473 ┆ 211 ┆ 679 ┆ 419 ┆ 263 ┆ 587 ┆ 184 ┆ 060 ┆ 946 ┆ 447 ┆ 447 ┆ 444 ┆ 118 ┆ 964 ┆ 581 ┆ 525 ┆ 086 ┆ 500 ┆ 99  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 06  ┆ 07  ┆ 12  ┆ 39  ┆ 84  ┆ 53  ┆ 68  ┆ 59  ┆ 6   ┆ 97  ┆ 75  ┆ 98  ┆ 4   ┆ 63  ┆ 29  ┆ 63  ┆ 66  ┆ 47  ┆ 95  ┆ 96  ┆ 45  ┆ 01  ┆ 25  ┆ 83  ┆ 56  ┆ 51  ┆ 19  ┆ 82  ┆ 66  ┆ 02  ┆ 39  ┆ 49  ┆ 67  ┆ 45  ┆ 26  ┆ 95  ┆ 59  ┆ 06  ┆ 57  ┆ 66  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-4 ┆ e-4 ┆ e-3 ┆ e-3 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆     ┆     ┆     ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆     ┆ e-8 ┆ 02  ┆ 03  ┆ 01  ┆ 08  ┆ 13  ┆ 26  ┆ 15  ┆ 17  ┆ 1   ┆ 11  ┆ 06  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-7 ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-9 ┆ e-9 ┆ e-8 ┆ e-7 ┆ 01  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 04  ┆ 03  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆     ┆ e-8 ┆ e-9 ┆ e-8 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆ e-9 ┆     ┆ e-1 ┆ e-1 ┆     ┆     ┆     ┆     ┆     ┆ 733 ┆ 343 ┆ 433 ┆ 662 ┆ 918 ┆ 197 ┆ 533 ┆ 628 ┆ 208 ┆ 085 ┆ 56  ┆ 6   ┆ 92  ┆ 33  ┆ 37  ┆ 94  ┆ 734 ┆ 486 ┆ 561 ┆ 408 ┆ 726 ┆ 711 ┆ 588 ┆ 585 ┆ 059 ┆ 948 ┆ 689 ┆ 692 ┆ 484 ┆ 329 ┆ 329 ┆ 361 ┆ 877 ┆ 174 ┆ 074 ┆ 472 ┆ 635 ┆ 59  ┆ 883 ┆ 45  ┆ 12  ┆ 42  ┆ 32  ┆ 66  ┆ 02  ┆ 79  ┆ 4   ┆ 259 ┆ 452 ┆ 182 ┆ 764 ┆ 978 ┆ 799 ┆ 748 ┆ 547 ┆ 384 ┆ 548 ┆ 653 ┆ 298 ┆ 118 ┆ 289 ┆ 997 ┆ 485 ┆ 575 ┆ 44  ┆ 701 ┆ 11  ┆ 662 ┆ 965 ┆ 43  ┆ 08  ┆ 81  ┆ 95  ┆ 28  ┆ 234 ┆ 033 ┆ 82  ┆ 883 ┆ 888 ┆ 510 ┆ 488 ┆ 471 ┆ 534 ┆ 838 ┆ 812 ┆ 645 ┆ 486 ┆ 029 ┆ 861 ┆ 519 ┆ 118 ┆ 198 ┆ 15  ┆ 65  ┆ 31  ┆ 42  ┆ 8   ┆ 75  ┆ 13  ┆ 69  ┆ 74  ┆ 89  ┆ 47  ┆ 33  ┆ 86  ┆ 22  ┆ 01  ┆ 84  ┆ 08  ┆ 208 ┆ 157 ┆ 914 ┆ 255 ┆ 104 ┆ 243 ┆ 86  ┆ 885 ┆ 903 ┆ 013 ┆ 154 ┆ 158 ┆ 807 ┆ 786 ┆ 019 ┆ 92  ┆ 24  ┆ 17  ┆ 67  ┆ 85  ┆ 76  ┆ 29  ┆ 474 ┆     ┆ 11  ┆ 89  ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 09  ┆ 14  ┆ 15  ┆ 15  ┆ 14  ┆ 12  ┆ 09  ┆ 06  ┆ 03  ┆ 02  ┆ 01  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 2   ┆ 4   ┆ 5   ┆     ┆ 9   ┆ 8   ┆ 4   ┆ 7   ┆     ┆ 8   ┆ 3   ┆ 8   ┆ 6   ┆ 1   ┆ 2   ┆ 5   ┆ 9   ┆ 3   ┆ 1   ┆ 7   ┆     ┆ 4   ┆     ┆ 7   ┆ 5   ┆ 2   ┆ 2   ┆ 1   ┆ 5   ┆ 4   ┆ 9   ┆ 4   ┆ 3   ┆ 9   ┆ 6   ┆ 2   ┆ 7   ┆ 9   ┆ 9   ┆ 3   ┆ 3   ┆ 2   ┆ 6   ┆ 6   ┆ 1   ┆ 2   ┆ 4   ┆ 8   ┆ 7   ┆ 3   ┆ 2   ┆     ┆ 9   ┆ 5   ┆ 8   ┆ 1   ┆     ┆ 3   ┆     ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 4   ┆ 7   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆ 4   ┆ 9   ┆ 5   ┆ 0   ┆ 7   ┆ 2   ┆ 7   ┆ 3   ┆ 9   ┆ 8   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 3   ┆ 3   ┆ 8   ┆ 2   ┆ 3   ┆ 7   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 0   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 5   ┆     ┆ 3   ┆ 2   ┆     ┆     ┆ 4   ┆ 8   ┆ 6   ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 347 ┆ 1   ┆ 9   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ tes ┆ 214 ┆ 233 ┆ 237 ┆ 248 ┆ 252 ┆ 261 ┆ 257 ┆ 248 ┆ 237 ┆ 229 ┆ 224 ┆ 219 ┆ 214 ┆ 209 ┆ 202 ┆ 198 ┆ 195 ┆ 190 ┆ 190 ┆ 193 ┆ 199 ┆ 204 ┆ 210 ┆ 215 ┆ 221 ┆ 226 ┆ 231 ┆ 235 ┆ 239 ┆ 243 ┆ 246 ┆ 250 ┆ 253 ┆ 256 ┆ 258 ┆ 261 ┆ 264 ┆ 267 ┆ 269 ┆ 272 ┆ 274 ┆ 276 ┆ 278 ┆ 280 ┆ 282 ┆ 283 ┆ 285 ┆ 286 ┆ 287 ┆ 288 ┆ 289 ┆ 290 ┆ 290 ┆ 291 ┆ 292 ┆ 293 ┆ 294 ┆ 296 ┆ 297 ┆ 298 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ 9.6 ┆ 8.6 ┆ 7.5 ┆ 6.5 ┆ 5.3 ┆ 5.1 ┆ 3.8 ┆ 3.3 ┆ 2.1 ┆ 1.9 ┆ 9.7 ┆ 5.1 ┆ 1.4 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 7.6 ┆ 7.1 ┆ 2.3 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 8.1 ┆ 2.6 ┆ 1.1 ┆ 1.1 ┆ 1.7 ┆ 0.0 ┆ 0.0 ┆ 6.1 ┆ 6.8 ┆ 9.5 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 4.8 ┆ 5.4 ┆ 1.1 ┆ 3.1 ┆ 6.2 ┆ 6.5 ┆ 6.7 ┆ 7.5 ┆ 7.5 ┆ 8.9 ┆ 6.4 ┆ 4.1 ┆ 2.1 ┆ 1.2 ┆ 8.6 ┆ 3.2 ┆ 1.7 ┆ 1.2 ┆ 1.0 ┆ 9.7 ┆ 4.8 ┆ 0.0 ┆ 5.4 ┆ 1.2 ┆ 2.4 ┆ 1.4 ┆ 5.7 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.4 ┆ 8.3 ┆ 1.2 ┆ 3.8 ┆ 8.5 ┆ 8.1 ┆ 2.4 ┆ 3.4 ┆ 4.9 ┆ 6.3 ┆ 8.6 ┆ 9.9 ┆ 1.0 ┆ 9.7 ┆ 8.5 ┆ 7.7 ┆ 6.7 ┆ 1.3 ┆ 0.0 ┆ 0.0 ┆ 5.1 ┆ 2.2 ┆ 4.1 ┆ 6.6 ┆ 4.8 ┆ 26. ┆ 38. ┆ 44. ┆ 50. ┆ 40. ┆ 31. ┆ 30. ┆ 27. ┆ 20. ┆ 14. ┆ 9.7 ┆ 4.8 ┆ 2.6 ┆ 2.7 ┆ 2.9 ┆ 7.0 ┆ 15. ┆ 21. ┆ 26. ┆ 30. ┆ 35. ┆ 40. ┆ 41. ┆ 41. ┆ 38. ┆ 34. ┆ 31. ┆ 27. ┆ 24. ┆ 23. ┆ 21. ┆ 20. ┆ 19. ┆ 18. ┆ 16. ┆ 14. ┆ 12. ┆ 10. ┆ 9.0 ┆ 7.3 ┆ 5.8 ┆ 4.5 ┆ 3.3 ┆ 2.3 ┆ 1.4 ┆ 0.6 ┆ -0. ┆ -0. ┆ -1. ┆ -2. ┆ -2. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -2. ┆ -2. ┆ -19 ┆ -7. ┆ 5.0 ┆ 16. ┆ 6.0 ┆ 14. ┆ 0.5 ┆ 0.0 ┆ -1. ┆ -0. ┆ 0.7 ┆ 0.9 ┆ 0.2 ┆ 0.3 ┆ 1.0 ┆ 1.4 ┆ 2.5 ┆ 2.2 ┆ 0.0 ┆ -5. ┆ -10 ┆ -12 ┆ -12 ┆ -12 ┆ -11 ┆ -10 ┆ -6. ┆ -2. ┆ 1.0 ┆ 3.7 ┆ 5.0 ┆ 5.2 ┆ 5.2 ┆ 4.8 ┆ 4.2 ┆ 3.6 ┆ 3.0 ┆ 2.6 ┆ 2.3 ┆ 2.1 ┆ 2.0 ┆ 1.8 ┆ 1.6 ┆ 1.4 ┆ 1.0 ┆ 0.5 ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ -2. ┆ -2. ┆ -2. ┆ -2. ┆ -3. ┆ -3. ┆ -3. ┆ -3. ┆ -2. ┆ -2. ┆ 984 ┆ 525 ┆ 132 ┆ 18. ┆ 0.0 ┆ 0.0 ┆ 0.3 ┆ 0.1 ┆ 0.1 ┆ 0.0 ┆ 0.0 ┆ 460 ┆ 0.0 ┆ 0.4 ┆ 0.5 ┆ 0.0 ┆ 3.0 ┆ 5.4 ┆ 9.6 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 9.3 ┆ 4.0 ┆ 1.9 ┆ 1.2 ┆ 9.8 ┆ 8.2 ┆ 7.2 ┆ 6.5 ┆ 6.2 ┆ 6.0 ┆ 6.0 ┆ 6.0 ┆ 6.1 ┆ 6.2 ┆ 6.2 ┆ 6.3 ┆ 6.4 ┆ 6.5 ┆ 6.6 ┆ 6.7 ┆ 6.8 ┆ 6.9 ┆ 7.0 ┆ 7.0 ┆ 7.0 ┆ 7.1 ┆ 6.9 ┆ 6.7 ┆ 6.5 ┆ 6.4 ┆ 6.4 ┆ 6.3 ┆ 6.3 ┆ 6.3 ┆ 6.3 ┆ 6.3 ┆ 6.2 ┆ 6.2 ┆ 6.1 ┆ 6.0 ┆ 5.9 ┆ 5.7 ┆ 5.7 ┆ 5.7 ┆ 1.8 ┆ 2.0 ┆ 2.3 ┆ 2.7 ┆ 3.1 ┆ 3.5 ┆ 3.9 ┆ 4.4 ┆ 4.9 ┆ 5.4 ┆ 5.9 ┆ 6.4 ┆ 6.8 ┆ 7.3 ┆ 7.8 ┆ 8.3 ┆ 8.7 ┆ 9.1 ┆ 9.5 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 9.9 ┆ 3.3 ┆ 4.2 ┆ 5.2 ┆ 6.4 ┆ 7.9 ┆ 9.6 ┆ 1.1 ┆ 1.3 ┆ 1.6 ┆ 1.8 ┆ 2.1 ┆ 2.4 ┆ 2.7 ┆ 3.0 ┆ 3.3 ┆ 3.6 ┆ 3.9 ┆ 4.3 ┆ 4.6 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 ┆ 4.9 │\n",
       "│ t_6 ┆ .54 ┆ .46 ┆ .85 ┆ .36 ┆ .35 ┆ .42 ┆ .17 ┆ .37 ┆ .31 ┆ .16 ┆ .54 ┆ .77 ┆ .13 ┆ .30 ┆ .74 ┆ .44 ┆ .44 ┆ .55 ┆ .98 ┆ .93 ┆ .34 ┆ .20 ┆ .20 ┆ .43 ┆ .24 ┆ .46 ┆ .25 ┆ .39 ┆ .42 ┆ .19 ┆ .84 ┆ .23 ┆ .23 ┆ .03 ┆ .78 ┆ .77 ┆ .58 ┆ .21 ┆ .92 ┆ .45 ┆ .88 ┆ .98 ┆ .82 ┆ .44 ┆ .11 ┆ .70 ┆ .05 ┆ .34 ┆ .24 ┆ .19 ┆ .23 ┆ .06 ┆ .91 ┆ .78 ┆ .83 ┆ .82 ┆ .96 ┆ .00 ┆ .14 ┆ .33 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 001 ┆ 001 ┆ 003 ┆ 004 ┆ 006 ┆ 007 ┆ 009 ┆ 010 ┆ 013 ┆ 016 ┆ 017 ┆ 020 ┆ 024 ┆ 030 ┆ 037 ┆ 044 ┆ 051 ┆ 059 ┆ 066 ┆ 072 ┆ 078 ┆ 085 ┆ 090 ┆ 099 ┆ 106 ┆ 110 ┆ 116 ┆ 123 ┆ 129 ┆ 132 ┆ 137 ┆ 138 ┆ 142 ┆ 143 ┆ 145 ┆ 053 ┆ 761 ┆ 073 ┆ 284 ┆ 289 ┆ 307 ┆ 958 ┆ 087 ┆ 045 ┆ 781 ┆ 838 ┆ 569 ┆ 639 ┆ 710 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 737 ┆ 301 ┆ 488 ┆     ┆     ┆     ┆     ┆ 064 ┆ 918 ┆ 465 ┆ 626 ┆ 548 ┆ 000 ┆ 000 ┆ 742 ┆ 795 ┆ 800 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆     ┆ 502 ┆ 026 ┆ 691 ┆ 324 ┆ 827 ┆ 066 ┆ 600 ┆ 840 ┆ 201 ┆ 972 ┆ 027 ┆ 368 ┆ 367 ┆ 561 ┆ 818 ┆ 672 ┆ 113 ┆ 354 ┆ 416 ┆ 141 ┆ 089 ┆     ┆ 496 ┆ 317 ┆ 247 ┆ 823 ┆ 093 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 068 ┆ 139 ┆ 426 ┆ 286 ┆ 483 ┆ 107 ┆ 125 ┆ 059 ┆ 822 ┆ 725 ┆ 643 ┆ 024 ┆ 750 ┆ 164 ┆ 857 ┆ 663 ┆ 589 ┆ 300 ┆     ┆     ┆ 521 ┆ 926 ┆ 159 ┆ 932 ┆ 389 ┆ 766 ┆ 326 ┆ 364 ┆ 504 ┆ 427 ┆ 241 ┆ 402 ┆ 722 ┆ 383 ┆ 493 ┆ 586 ┆ 785 ┆ 832 ┆ 609 ┆ 635 ┆ 875 ┆ 348 ┆ 242 ┆ 108 ┆ 126 ┆ 599 ┆ 136 ┆ 652 ┆ 121 ┆ 686 ┆ 965 ┆ 105 ┆ 512 ┆ 737 ┆ 037 ┆ 860 ┆ 854 ┆ 693 ┆ 156 ┆ 349 ┆ 623 ┆ 867 ┆ 999 ┆ 428 ┆ 435 ┆ 934 ┆ 539 ┆ 750 ┆ 741 ┆ 389 ┆ 036 ┆ 173 ┆ 930 ┆ 617 ┆ 284 ┆ 797 ┆ 156 ┆ 366 ┆ 481 ┆ 525 ┆ 454 ┆ 384 ┆ 113 ┆ 909 ┆ 618 ┆ .71 ┆ 008 ┆ 984 ┆ 692 ┆ 398 ┆ 258 ┆ 305 ┆ 540 ┆ 110 ┆ 972 ┆ 543 ┆ 950 ┆ 395 ┆ 931 ┆ 700 ┆ 122 ┆ 652 ┆ 346 ┆ 583 ┆ 735 ┆ .93 ┆ .98 ┆ .88 ┆ .57 ┆ .84 ┆ .10 ┆ 941 ┆ 603 ┆ 492 ┆ 231 ┆ 487 ┆ 964 ┆ 358 ┆ 085 ┆ 367 ┆ 257 ┆ 153 ┆ 051 ┆ 492 ┆ 552 ┆ 172 ┆ 579 ┆ 872 ┆ 015 ┆ 061 ┆ 147 ┆ 013 ┆ 566 ┆ 087 ┆ 582 ┆ 035 ┆ 415 ┆ 685 ┆ 897 ┆ 029 ┆ 136 ┆ 149 ┆ 058 ┆ 957 ┆ 749 ┆ 63. ┆ .70 ┆ .10 ┆ 355 ┆ 611 ┆ 621 ┆ 736 ┆ 426 ┆ 832 ┆ 589 ┆ 944 ┆ .09 ┆     ┆ 014 ┆ 985 ┆     ┆ 046 ┆ 114 ┆ 993 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 000 ┆ 332 ┆ 457 ┆ 842 ┆ 794 ┆ 775 ┆ 898 ┆ 434 ┆ 700 ┆ 111 ┆ 836 ┆ 226 ┆ 869 ┆ 481 ┆ 038 ┆ 637 ┆ 359 ┆ 105 ┆ 292 ┆ 596 ┆ 845 ┆ 833 ┆ 828 ┆ 558 ┆ 732 ┆ 900 ┆ 004 ┆ 089 ┆ 275 ┆ 551 ┆ 353 ┆ 043 ┆ 750 ┆ 476 ┆ 262 ┆ 129 ┆ 004 ┆ 884 ┆ 481 ┆ 659 ┆ 837 ┆ 535 ┆ 958 ┆ 958 ┆ 958 ┆ 050 ┆ 730 ┆ 782 ┆ 224 ┆ 057 ┆ 252 ┆ 751 ┆ 472 ┆ 324 ┆ 230 ┆ 139 ┆ 024 ┆ 876 ┆ 681 ┆ 414 ┆ 035 ┆ 502 ┆ 783 ┆ 869 ┆ 766 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 861 ┆ 986 ┆ 184 ┆ 268 ┆ 547 ┆ 282 ┆ 621 ┆ 655 ┆ 886 ┆ 322 ┆ 926 ┆ 667 ┆ 525 ┆ 487 ┆ 538 ┆ 655 ┆ 802 ┆ 938 ┆ 031 ┆ 058 ┆ 014 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 ┆ 086 │\n",
       "│ 013 ┆ 690 ┆ 088 ┆ 057 ┆ 119 ┆ 375 ┆ 646 ┆ 252 ┆ 148 ┆ 665 ┆ 794 ┆ 255 ┆ 220 ┆ 307 ┆ 952 ┆ 787 ┆ 803 ┆ 095 ┆ 546 ┆ 908 ┆ 086 ┆ 107 ┆ 467 ┆ 923 ┆ 66  ┆ 639 ┆ 801 ┆ 908 ┆ 401 ┆ 433 ┆ 983 ┆ 374 ┆ 620 ┆ 465 ┆ 563 ┆ 251 ┆ 499 ┆ 027 ┆ 350 ┆ 114 ┆ 624 ┆ 559 ┆ 932 ┆ 592 ┆ 445 ┆ 135 ┆ 179 ┆ 509 ┆ 261 ┆ 152 ┆ 072 ┆ 910 ┆ 59  ┆ 658 ┆ 447 ┆ 576 ┆ 904 ┆ 378 ┆ 140 ┆ 843 ┆ 091 ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 02  ┆ 03  ┆ 06  ┆ 08  ┆ 18  ┆ 4   ┆ 78  ┆ 27  ┆ 79  ┆ 05  ┆ 57  ┆ 08  ┆ 61  ┆ 09  ┆ 88  ┆ 83  ┆ 72  ┆ 9   ┆ 11  ┆ 33  ┆ 17  ┆ 47  ┆ 49  ┆ 63  ┆ 65  ┆ 54  ┆ 81  ┆ 52  ┆ 6   ┆ 93  ┆ 12  ┆ 05  ┆ 21  ┆ 8   ┆ 62  ┆ 85  ┆ 53  ┆ 96  ┆ 65  ┆ 51  ┆ 63  ┆ 07  ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-3 ┆ e-4 ┆ e-5 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ e-5 ┆ e-4 ┆ e-4 ┆     ┆     ┆     ┆     ┆ e-2 ┆ e-1 ┆ e-1 ┆ e-9 ┆ e-7 ┆ 02  ┆ 02  ┆ e-7 ┆ e-8 ┆ e-8 ┆ 02  ┆ 07  ┆ 06  ┆ 25  ┆ 22  ┆ 15  ┆ 32  ┆ 31  ┆ 22  ┆ 12  ┆ 31  ┆ 42  ┆ 29  ┆ 16  ┆ 14  ┆ 08  ┆ 02  ┆     ┆ e-7 ┆ e-8 ┆ e-7 ┆ e-8 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-2 ┆ e-2 ┆ e-1 ┆ e-1 ┆     ┆ e-9 ┆ e-7 ┆ e-9 ┆ e-7 ┆ e-7 ┆ 03  ┆ 03  ┆ 01  ┆ 01  ┆ 17  ┆ 34  ┆ 51  ┆ 47  ┆ 34  ┆ 21  ┆ 11  ┆ 06  ┆ e-7 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-9 ┆     ┆     ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ e-1 ┆ 125 ┆ 466 ┆ 158 ┆ 799 ┆ 283 ┆ 199 ┆ 445 ┆ 321 ┆ 986 ┆ 661 ┆ 15  ┆ 46  ┆ 49  ┆ 8   ┆ 64  ┆     ┆ 046 ┆ 244 ┆ 509 ┆ 443 ┆ 535 ┆ 581 ┆ 886 ┆ 061 ┆ 804 ┆ 231 ┆ 139 ┆ 96  ┆ 766 ┆ 146 ┆ 157 ┆ 756 ┆ 072 ┆ 475 ┆ 687 ┆ 607 ┆ 586 ┆ 232 ┆ 02  ┆ 2   ┆ 71  ┆ 44  ┆ 12  ┆ 91  ┆ 02  ┆ 76  ┆ 665 ┆ 428 ┆ 219 ┆ 518 ┆ 098 ┆ 353 ┆ 056 ┆ 461 ┆ 668 ┆ 358 ┆ 439 ┆ 365 ┆ 109 ┆ 813 ┆ 489 ┆ 516 ┆ 22  ┆ 305 ┆ 38  ┆ 133 ┆ 96  ┆ 57  ┆ 178 ┆ 205 ┆ 06  ┆ 24  ┆ 25  ┆ 46  ┆ 35  ┆ 89  ┆ 32  ┆ 06  ┆ 34  ┆ 292 ┆ 217 ┆ 623 ┆ 489 ┆ 391 ┆ 733 ┆ 754 ┆ 908 ┆ 484 ┆ 67  ┆ 52  ┆ 24  ┆ 05  ┆ 92  ┆ 92  ┆ 25  ┆ 06  ┆ 51  ┆ 33  ┆ 06  ┆ 75  ┆ 06  ┆ 69  ┆ 04  ┆ 92  ┆ 44  ┆ 13  ┆ 485 ┆ 443 ┆ 8   ┆ 165 ┆ 671 ┆ 875 ┆ 827 ┆ 569 ┆ 162 ┆ 76  ┆ 133 ┆ 256 ┆ 907 ┆ 553 ┆ 738 ┆ 208 ┆ 793 ┆ 525 ┆ 75  ┆ 1   ┆ 63  ┆ 97  ┆ 34  ┆ 91  ┆ 76  ┆ 122 ┆     ┆ 65  ┆ 35  ┆     ┆ e-7 ┆ e-7 ┆ e-7 ┆ 02  ┆ 03  ┆ 05  ┆ 09  ┆ 14  ┆ 15  ┆ 15  ┆ 14  ┆ 12  ┆ 09  ┆ 06  ┆ 03  ┆ 02  ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-8 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 ┆ e-7 │\n",
       "│ 50  ┆ 9   ┆ 9   ┆ 5   ┆ 9   ┆ 8   ┆ 4   ┆     ┆ 1   ┆ 1   ┆ 1   ┆     ┆ 6   ┆ 7   ┆ 9   ┆ 1   ┆ 7   ┆ 3   ┆ 8   ┆     ┆ 9   ┆ 2   ┆ 4   ┆ 8   ┆     ┆ 1   ┆ 9   ┆ 5   ┆     ┆ 6   ┆ 6   ┆ 1   ┆ 7   ┆ 2   ┆     ┆ 2   ┆ 8   ┆ 6   ┆ 6   ┆ 9   ┆ 1   ┆ 9   ┆ 1   ┆ 8   ┆ 5   ┆ 8   ┆ 5   ┆ 1   ┆ 9   ┆ 6   ┆     ┆ 6   ┆     ┆ 5   ┆ 9   ┆ 6   ┆     ┆ 5   ┆ 6   ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 8   ┆ 9   ┆ 5   ┆ 1   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 4   ┆ 9   ┆ 4   ┆     ┆     ┆     ┆     ┆ 0   ┆ 6   ┆ 2   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 2   ┆ 3   ┆ 3   ┆ 9   ┆ 5   ┆ 3   ┆ 9   ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆ 1   ┆ 1   ┆ 1   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 1   ┆ 1   ┆ 9   ┆ 4   ┆ 8   ┆ 4   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 907 ┆ 5   ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 3   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mbatch\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(batch[0].std(dim=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9721663e-13"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, 180].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8331273364701616e-11"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:, 180].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mdisable(learn\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval(),)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.compiler.disable(learn.model.eval(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='9766' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/9766 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/loss.py:999: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 368])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Exception occured in `Recorder` when calling event `after_batch`:\n\t==:\n23552\n64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_targs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:308\u001b[0m, in \u001b[0;36mLearner.get_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_loss: ctx_mgrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_not_reduced())\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(ctx_mgrs):\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: act \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    310\u001b[0m     res \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mall_tensors()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:201\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:560\u001b[0m, in \u001b[0;36mRecorder.after_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    559\u001b[0m mets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_mets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_mets\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m mets: \u001b[43mmet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:482\u001b[0m, in \u001b[0;36mAvgMetric.accumulate\u001b[0;34m(self, learn)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn):\n\u001b[1;32m    481\u001b[0m     bs \u001b[38;5;241m=\u001b[39m find_bs(learn\u001b[38;5;241m.\u001b[39myb)\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mto_detach(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39mbs\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/metrics.py:288\u001b[0m, in \u001b[0;36mmae\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmae\u001b[39m(inp,targ):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean absolute error between `inp` and `targ`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     inp,targ \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(inp \u001b[38;5;241m-\u001b[39m targ)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:787\u001b[0m, in \u001b[0;36mflatten_check\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck that `inp` and `targ` have the same number of elements and flatten them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m inp,targ \u001b[38;5;241m=\u001b[39m TensorBase(inp\u001b[38;5;241m.\u001b[39mcontiguous())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),TensorBase(targ\u001b[38;5;241m.\u001b[39mcontiguous())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 787\u001b[0m \u001b[43mtest_eq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inp,targ\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/test.py:37\u001b[0m, in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_eq\u001b[39m(a,b):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`test` that `a==b`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mequals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/test.py:27\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: cname\u001b[38;5;241m=\u001b[39mcmp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cmp(a,b),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Exception occured in `Recorder` when calling event `after_batch`:\n\t==:\n23552\n64"
     ]
    }
   ],
   "source": [
    "preds = learn.get_preds(None, dl=test_loader, with_targs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9766/9766 [12:01<00:00, 13.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_loader):\n",
    "        x = (batch[0][0].cuda().type(torch.float), batch[0][1].cuda())\n",
    "        pred = model(x)\n",
    "        #pred = learn.get_preds\n",
    "        #pred = norm_y.denorm(pred)\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2091, -1.1206, -1.5577,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3850, -1.2824, -1.5098,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1913, -0.1536, -1.0023,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.5073,  0.5929, -0.0575,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.4168, -0.2640, -0.8508,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3559, -0.1311, -0.6344,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 376])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.concatenate([pred.cpu() for pred in predictions], axis=0)# * weights[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = predicts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicts, 'preds6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicts = torch.load('preds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pl.DataFrame(test_df['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (625_000, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;test_169651&quot;</td></tr><tr><td>&quot;test_524862&quot;</td></tr><tr><td>&quot;test_634129&quot;</td></tr><tr><td>&quot;test_403572&quot;</td></tr><tr><td>&quot;test_484578&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;test_578220&quot;</td></tr><tr><td>&quot;test_395695&quot;</td></tr><tr><td>&quot;test_88942&quot;</td></tr><tr><td>&quot;test_79382&quot;</td></tr><tr><td>&quot;test_601350&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (625_000, 1)\n",
       "┌─────────────┐\n",
       "│ sample_id   │\n",
       "│ ---         │\n",
       "│ str         │\n",
       "╞═════════════╡\n",
       "│ test_169651 │\n",
       "│ test_524862 │\n",
       "│ test_634129 │\n",
       "│ test_403572 │\n",
       "│ test_484578 │\n",
       "│ …           │\n",
       "│ test_578220 │\n",
       "│ test_395695 │\n",
       "│ test_88942  │\n",
       "│ test_79382  │\n",
       "│ test_601350 │\n",
       "└─────────────┘"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = TARGET_COLS.index('ptend_q0002_26')\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping ptend_q0001_0\n",
      "skipping ptend_q0001_1\n",
      "skipping ptend_q0001_2\n",
      "skipping ptend_q0001_3\n",
      "skipping ptend_q0001_4\n",
      "skipping ptend_q0001_5\n",
      "skipping ptend_q0001_6\n",
      "skipping ptend_q0001_7\n",
      "skipping ptend_q0001_8\n",
      "skipping ptend_q0001_9\n",
      "skipping ptend_q0001_10\n",
      "skipping ptend_q0001_11\n",
      "skipping ptend_q0002_0\n",
      "skipping ptend_q0002_1\n",
      "skipping ptend_q0002_2\n",
      "skipping ptend_q0002_3\n",
      "skipping ptend_q0002_4\n",
      "skipping ptend_q0002_5\n",
      "skipping ptend_q0002_6\n",
      "skipping ptend_q0002_7\n",
      "skipping ptend_q0002_8\n",
      "skipping ptend_q0002_9\n",
      "skipping ptend_q0002_10\n",
      "skipping ptend_q0002_11\n",
      "skipping ptend_q0002_12\n",
      "skipping ptend_q0002_13\n",
      "skipping ptend_q0002_14\n",
      "skipping ptend_q0002_15\n",
      "skipping ptend_q0002_16\n",
      "skipping ptend_q0002_17\n",
      "skipping ptend_q0002_18\n",
      "skipping ptend_q0002_19\n",
      "skipping ptend_q0002_20\n",
      "skipping ptend_q0002_21\n",
      "skipping ptend_q0002_22\n",
      "skipping ptend_q0002_23\n",
      "skipping ptend_q0002_24\n",
      "skipping ptend_q0002_25\n",
      "skipping ptend_q0002_26\n",
      "skipping ptend_q0003_0\n",
      "skipping ptend_q0003_1\n",
      "skipping ptend_q0003_2\n",
      "skipping ptend_q0003_3\n",
      "skipping ptend_q0003_4\n",
      "skipping ptend_q0003_5\n",
      "skipping ptend_q0003_6\n",
      "skipping ptend_q0003_7\n",
      "skipping ptend_q0003_8\n",
      "skipping ptend_q0003_9\n",
      "skipping ptend_q0003_10\n",
      "skipping ptend_q0003_11\n",
      "skipping ptend_u_0\n",
      "skipping ptend_u_1\n",
      "skipping ptend_u_2\n",
      "skipping ptend_u_3\n",
      "skipping ptend_u_4\n",
      "skipping ptend_u_5\n",
      "skipping ptend_u_6\n",
      "skipping ptend_u_7\n",
      "skipping ptend_u_8\n",
      "skipping ptend_u_9\n",
      "skipping ptend_u_10\n",
      "skipping ptend_u_11\n",
      "skipping ptend_v_0\n",
      "skipping ptend_v_1\n",
      "skipping ptend_v_2\n",
      "skipping ptend_v_3\n",
      "skipping ptend_v_4\n",
      "skipping ptend_v_5\n",
      "skipping ptend_v_6\n",
      "skipping ptend_v_7\n",
      "skipping ptend_v_8\n",
      "skipping ptend_v_9\n",
      "skipping ptend_v_10\n",
      "skipping ptend_v_11\n"
     ]
    }
   ],
   "source": [
    "for n, col in enumerate(TARGET_COLS):\n",
    "    if norm_y.zero_mask[n]:\n",
    "        print(f'skipping {col}')\n",
    "        pl_col = pl.lit(0.0, dtype=pl.Float32).alias(col)\n",
    "    else:\n",
    "        pl_col = pl.Series(col, predicts[:, n],  dtype=pl.Float32)\n",
    "    \n",
    "    output = output.with_columns(pl_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write_csv('submission.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.09812656e+04, 2.25024316e+04, 1.88941465e+04, 1.45142451e+04,\n",
       "       1.09443477e+04, 9.06501074e+03, 9.66366895e+03, 1.26885576e+04,\n",
       "       1.98901719e+04, 2.58313730e+04, 3.38903672e+04, 4.41229414e+04,\n",
       "       5.98112578e+04, 7.94340781e+04, 1.07358812e+05, 1.35720844e+05,\n",
       "       1.49399844e+05, 1.28492953e+05, 9.17462344e+04, 7.27487656e+04,\n",
       "       6.65315391e+04, 6.29323047e+04, 5.66102695e+04, 4.94731445e+04,\n",
       "       4.30291836e+04, 3.69126758e+04, 3.14869316e+04, 2.68980723e+04,\n",
       "       2.33166387e+04, 2.04597305e+04, 1.83856836e+04, 1.71114043e+04,\n",
       "       1.63378096e+04, 1.58577598e+04, 1.55809023e+04, 1.54975908e+04,\n",
       "       1.56122559e+04, 1.57978848e+04, 1.59742188e+04, 1.61303955e+04,\n",
       "       1.62613105e+04, 1.63718926e+04, 1.63970195e+04, 1.63254639e+04,\n",
       "       1.62286406e+04, 1.61918096e+04, 1.63412080e+04, 1.66457109e+04,\n",
       "       1.70054941e+04, 1.74302988e+04, 1.79072402e+04, 1.84315527e+04,\n",
       "       1.90324707e+04, 1.97013555e+04, 2.04082363e+04, 2.09672070e+04,\n",
       "       2.11944277e+04, 2.10885215e+04, 1.94379160e+04, 1.36779023e+04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.71528464e+11, 1.08322180e+12, 1.47034751e+11, 3.55560448e+10,\n",
       "       3.51533711e+10, 4.60863693e+10, 2.46893056e+10, 1.13432771e+10,\n",
       "       5.39662490e+09, 2.44935296e+09, 1.13222592e+09, 5.79547840e+08,\n",
       "       3.30219232e+08, 2.07613936e+08, 1.44580288e+08, 1.09933280e+08,\n",
       "       8.87066000e+07, 7.38197760e+07, 6.36159880e+07, 5.72502640e+07,\n",
       "       5.29760720e+07, 4.96531680e+07, 4.65449760e+07, 4.31676080e+07,\n",
       "       3.97243760e+07, 3.63171760e+07, 3.30575120e+07, 2.98690900e+07,\n",
       "       2.69823860e+07, 2.44162360e+07, 2.22736520e+07, 2.05534260e+07,\n",
       "       1.92162400e+07, 1.81676940e+07, 1.75018560e+07, 1.71699380e+07,\n",
       "       1.70053820e+07, 1.69984760e+07, 1.70828900e+07, 1.72279820e+07,\n",
       "       1.74458240e+07, 1.77574040e+07, 1.83460920e+07, 1.94005740e+07,\n",
       "       2.05067220e+07, 2.24696480e+07, 2.34320320e+07, 2.62041640e+07,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 9.99999987e+14, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 9.99999987e+14, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 3.67382993e+12, 3.71405586e+11,\n",
       "       1.42191636e+10, 3.00186291e+09, 1.43276659e+09, 8.84599808e+08,\n",
       "       5.60128000e+08, 3.86052576e+08, 2.87331840e+08, 2.22703664e+08,\n",
       "       1.81069232e+08, 1.54620864e+08, 1.38093776e+08, 1.26605832e+08,\n",
       "       1.17967840e+08, 1.11005816e+08, 1.05186904e+08, 1.00168136e+08,\n",
       "       9.55686480e+07, 9.14574320e+07, 8.88716080e+07, 8.88298000e+07,\n",
       "       9.13981120e+07, 9.65851280e+07, 1.04507696e+08, 1.15895120e+08,\n",
       "       1.31939704e+08, 1.54492944e+08, 1.83147920e+08, 2.15151376e+08,\n",
       "       2.47158320e+08, 2.66792880e+08, 2.79115136e+08, 3.70541504e+08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.77670498e+11, 1.17482691e+12, 1.27060555e+12, 2.17273160e+10,\n",
       "       3.15945677e+09, 1.09065344e+09, 7.27967104e+08, 3.84399552e+08,\n",
       "       2.90787296e+08, 2.32703216e+08, 1.97467456e+08, 1.74310896e+08,\n",
       "       1.60536432e+08, 1.53567104e+08, 1.52120128e+08, 1.53115568e+08,\n",
       "       1.53955552e+08, 1.53734672e+08, 1.54798672e+08, 1.63346208e+08,\n",
       "       1.80013136e+08, 2.00324352e+08, 2.20754608e+08, 2.41290928e+08,\n",
       "       2.62868928e+08, 2.84448896e+08, 3.05681088e+08, 3.27605088e+08,\n",
       "       3.50473312e+08, 3.73964608e+08, 3.98396928e+08, 4.23528352e+08,\n",
       "       4.50447040e+08, 4.78856992e+08, 5.08200320e+08, 5.37309632e+08,\n",
       "       5.66854592e+08, 5.94618816e+08, 6.19715904e+08, 6.41395456e+08,\n",
       "       6.63290048e+08, 6.89274880e+08, 7.18208896e+08, 7.43951232e+08,\n",
       "       7.61776128e+08, 7.72911232e+08, 8.04001152e+08, 7.72448768e+08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.61382350e+06, 1.99930888e+06, 9.04636250e+05, 4.33823625e+05,\n",
       "       2.07201391e+05, 1.07836094e+05, 5.76479141e+04, 4.06065234e+04,\n",
       "       4.77398672e+04, 5.16693555e+04, 5.64381992e+04, 6.04474570e+04,\n",
       "       6.52514141e+04, 7.19208828e+04, 7.85295781e+04, 8.34223047e+04,\n",
       "       8.70369844e+04, 9.03897266e+04, 9.39823906e+04, 9.75780078e+04,\n",
       "       1.01428211e+05, 1.04630695e+05, 1.05685047e+05, 1.03962586e+05,\n",
       "       9.96503203e+04, 9.42905000e+04, 8.95148984e+04, 8.59054609e+04,\n",
       "       8.27849844e+04, 7.91522891e+04, 7.48478125e+04, 7.03788203e+04,\n",
       "       6.54200469e+04, 5.99537500e+04, 5.47642812e+04, 5.03625117e+04,\n",
       "       4.62125703e+04, 4.19975273e+04, 3.76920508e+04, 3.38347344e+04,\n",
       "       3.18460977e+04, 3.19341465e+04, 3.14548125e+04, 3.01054082e+04,\n",
       "       2.69578301e+04, 2.77600449e+04, 2.98533750e+04, 1.91334297e+04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.61994050e+06, 3.14839450e+06, 1.30841500e+06, 5.40515750e+05,\n",
       "       2.15237109e+05, 1.02546727e+05, 6.84536719e+04, 5.06925898e+04,\n",
       "       5.14875195e+04, 5.21047695e+04, 5.40193906e+04, 5.58560234e+04,\n",
       "       6.03473008e+04, 6.89909609e+04, 7.90968906e+04, 8.75743359e+04,\n",
       "       9.41585625e+04, 1.01903633e+05, 1.11746977e+05, 1.22460656e+05,\n",
       "       1.32086688e+05, 1.41041484e+05, 1.46354094e+05, 1.45953094e+05,\n",
       "       1.39496797e+05, 1.28508852e+05, 1.16665516e+05, 1.07458398e+05,\n",
       "       1.00259969e+05, 9.41089844e+04, 8.84398984e+04, 8.27349062e+04,\n",
       "       7.70610859e+04, 7.13335312e+04, 6.59997266e+04, 6.17989961e+04,\n",
       "       5.82373555e+04, 5.47151016e+04, 5.08258438e+04, 4.60591758e+04,\n",
       "       4.07402617e+04, 3.63358008e+04, 3.39815742e+04, 3.35897148e+04,\n",
       "       3.39888867e+04, 3.62729375e+04, 4.11833438e+04, 2.91941230e+04,\n",
       "       4.05361364e-03, 1.38824238e-02, 1.35129888e+08, 1.22197180e+07,\n",
       "       9.07052774e-03, 8.58988520e-03, 2.15368196e-02, 3.36321294e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, targets = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptend_t_0 : 0.01796277053654194\n",
      "ptend_t_1 : 0.01580028049647808\n",
      "ptend_t_2 : 0.005049095023423433\n",
      "ptend_t_3 : 0.0012899171561002731\n",
      "ptend_t_4 : 0.0006612258148379624\n",
      "ptend_t_5 : 0.000465095741674304\n",
      "ptend_t_6 : 0.00043876576819457114\n",
      "ptend_t_7 : 0.0007029272383078933\n",
      "ptend_t_8 : 0.0008303220965899527\n",
      "ptend_t_9 : 0.0010582703398540616\n",
      "ptend_t_10 : 0.0018357549561187625\n",
      "ptend_t_11 : 0.0019260403933003545\n",
      "ptend_t_12 : 0.002866466762498021\n",
      "ptend_t_13 : 0.003381755668669939\n",
      "ptend_t_14 : 0.006356712430715561\n",
      "ptend_t_15 : 0.01387377455830574\n",
      "ptend_t_16 : 0.1253134310245514\n",
      "ptend_t_17 : 0.3069222569465637\n",
      "ptend_t_18 : 0.533825159072876\n",
      "ptend_t_19 : 0.5077593922615051\n",
      "ptend_t_20 : 0.4755598306655884\n",
      "ptend_t_21 : 0.44834259152412415\n",
      "ptend_t_22 : 0.3704204261302948\n",
      "ptend_t_23 : 0.301801860332489\n",
      "ptend_t_24 : 0.26321732997894287\n",
      "ptend_t_25 : 0.2153325378894806\n",
      "ptend_t_26 : 0.17556500434875488\n",
      "ptend_t_27 : 0.14571933448314667\n",
      "ptend_t_28 : 0.12255648523569107\n",
      "ptend_t_29 : 0.10464970022439957\n",
      "ptend_t_30 : 0.0986165851354599\n",
      "ptend_t_31 : 0.09943924099206924\n",
      "ptend_t_32 : 0.10381776094436646\n",
      "ptend_t_33 : 0.11422466486692429\n",
      "ptend_t_34 : 0.12497811764478683\n",
      "ptend_t_35 : 0.14388370513916016\n",
      "ptend_t_36 : 0.16816966235637665\n",
      "ptend_t_37 : 0.1987522691488266\n",
      "ptend_t_38 : 0.2286107987165451\n",
      "ptend_t_39 : 0.25997981429100037\n",
      "ptend_t_40 : 0.2961488962173462\n",
      "ptend_t_41 : 0.33176904916763306\n",
      "ptend_t_42 : 0.36067894101142883\n",
      "ptend_t_43 : 0.3792122006416321\n",
      "ptend_t_44 : 0.4003381133079529\n",
      "ptend_t_45 : 0.42244037985801697\n",
      "ptend_t_46 : 0.44774961471557617\n",
      "ptend_t_47 : 0.46505022048950195\n",
      "ptend_t_48 : 0.4864519238471985\n",
      "ptend_t_49 : 0.5009571313858032\n",
      "ptend_t_50 : 0.5133023262023926\n",
      "ptend_t_51 : 0.5093532204627991\n",
      "ptend_t_52 : 0.5029786825180054\n",
      "ptend_t_53 : 0.49633169174194336\n",
      "ptend_t_54 : 0.48975905776023865\n",
      "ptend_t_55 : 0.47651633620262146\n",
      "ptend_t_56 : 0.4542097747325897\n",
      "ptend_t_57 : 0.4425702393054962\n",
      "ptend_t_58 : 0.39093196392059326\n",
      "ptend_t_59 : 0.2095847725868225\n",
      "ptend_q0001_0 : 8.587687261751853e-06\n",
      "ptend_q0001_1 : 5.340450570656685e-06\n",
      "ptend_q0001_2 : 9.093670087167993e-06\n",
      "ptend_q0001_3 : 8.505003279424272e-06\n",
      "ptend_q0001_4 : 9.545564353174996e-06\n",
      "ptend_q0001_5 : 8.692614756000694e-06\n",
      "ptend_q0001_6 : 8.076121048361529e-06\n",
      "ptend_q0001_7 : 6.556381777045317e-06\n",
      "ptend_q0001_8 : 6.7751634560409e-06\n",
      "ptend_q0001_9 : 8.937179700296838e-06\n",
      "ptend_q0001_10 : 1.5109410469449358e-06\n",
      "ptend_q0001_11 : 4.854899543715874e-06\n",
      "ptend_q0001_12 : 0.027017856016755104\n",
      "ptend_q0001_13 : 0.06375064700841904\n",
      "ptend_q0001_14 : 1.4740583896636963\n",
      "ptend_q0001_15 : 0.22232435643672943\n",
      "ptend_q0001_16 : 0.21864566206932068\n",
      "ptend_q0001_17 : 0.27730515599250793\n",
      "ptend_q0001_18 : 0.5147328972816467\n",
      "ptend_q0001_19 : 0.39965057373046875\n",
      "ptend_q0001_20 : 0.383223295211792\n",
      "ptend_q0001_21 : 0.30502018332481384\n",
      "ptend_q0001_22 : 0.2481049746274948\n",
      "ptend_q0001_23 : 0.1984248012304306\n",
      "ptend_q0001_24 : 0.1683114916086197\n",
      "ptend_q0001_25 : 0.14461641013622284\n",
      "ptend_q0001_26 : 0.13899387419223785\n",
      "ptend_q0001_27 : 0.1316319704055786\n",
      "ptend_q0001_28 : 0.1345384120941162\n",
      "ptend_q0001_29 : 0.1448090821504593\n",
      "ptend_q0001_30 : 0.1570761501789093\n",
      "ptend_q0001_31 : 0.17811191082000732\n",
      "ptend_q0001_32 : 0.20814916491508484\n",
      "ptend_q0001_33 : 0.2668791115283966\n",
      "ptend_q0001_34 : 0.32782602310180664\n",
      "ptend_q0001_35 : 0.3823545575141907\n",
      "ptend_q0001_36 : 0.4306071400642395\n",
      "ptend_q0001_37 : 0.48373451828956604\n",
      "ptend_q0001_38 : 0.5268698930740356\n",
      "ptend_q0001_39 : 0.5697033405303955\n",
      "ptend_q0001_40 : 0.5917365550994873\n",
      "ptend_q0001_41 : 0.6009968519210815\n",
      "ptend_q0001_42 : 0.6004118919372559\n",
      "ptend_q0001_43 : 0.60442715883255\n",
      "ptend_q0001_44 : 0.6065268516540527\n",
      "ptend_q0001_45 : 0.6028252243995667\n",
      "ptend_q0001_46 : 0.613685131072998\n",
      "ptend_q0001_47 : 0.6220695972442627\n",
      "ptend_q0001_48 : 0.631276547908783\n",
      "ptend_q0001_49 : 0.6428205370903015\n",
      "ptend_q0001_50 : 0.6435698866844177\n",
      "ptend_q0001_51 : 0.6344837546348572\n",
      "ptend_q0001_52 : 0.6183449625968933\n",
      "ptend_q0001_53 : 0.5944846868515015\n",
      "ptend_q0001_54 : 0.5696128010749817\n",
      "ptend_q0001_55 : 0.5379188060760498\n",
      "ptend_q0001_56 : 0.4848650097846985\n",
      "ptend_q0001_57 : 0.457197904586792\n",
      "ptend_q0001_58 : 0.3936512768268585\n",
      "ptend_q0001_59 : 0.368914932012558\n",
      "ptend_q0002_0 : 8.138318662531674e-06\n",
      "ptend_q0002_1 : 9.615941962692887e-06\n",
      "ptend_q0002_2 : 8.32778914627852e-06\n",
      "ptend_q0002_3 : 6.585183655261062e-06\n",
      "ptend_q0002_4 : 8.829120815789793e-06\n",
      "ptend_q0002_5 : 7.701221875322517e-06\n",
      "ptend_q0002_6 : 3.6680730772786774e-06\n",
      "ptend_q0002_7 : 8.278887435153592e-06\n",
      "ptend_q0002_8 : 5.947088084212737e-06\n",
      "ptend_q0002_9 : 7.497388196497923e-06\n",
      "ptend_q0002_10 : 3.7821862406417495e-06\n",
      "ptend_q0002_11 : 7.236420060507953e-06\n",
      "ptend_q0002_12 : 1.4507040759781376e-06\n",
      "ptend_q0002_13 : 6.597384071937995e-06\n",
      "ptend_q0002_14 : 8.752737812756095e-06\n",
      "ptend_q0002_15 : 8.742944373807404e-06\n",
      "ptend_q0002_16 : 4.204177912470186e-06\n",
      "ptend_q0002_17 : 3.90713285014499e-06\n",
      "ptend_q0002_18 : 5.226392204349395e-06\n",
      "ptend_q0002_19 : 9.594779839972034e-06\n",
      "ptend_q0002_20 : 9.515611054666806e-06\n",
      "ptend_q0002_21 : 9.705990123620722e-06\n",
      "ptend_q0002_22 : 9.74902377492981e-06\n",
      "ptend_q0002_23 : 9.471959856455214e-06\n",
      "ptend_q0002_24 : 9.143805073108524e-06\n",
      "ptend_q0002_25 : 9.392786523676477e-06\n",
      "ptend_q0002_26 : 9.72382076724898e-06\n",
      "ptend_q0002_27 : 0.023450469598174095\n",
      "ptend_q0002_28 : 0.05640455707907677\n",
      "ptend_q0002_29 : 0.21784748136997223\n",
      "ptend_q0002_30 : 0.3848077058792114\n",
      "ptend_q0002_31 : 0.5561830997467041\n",
      "ptend_q0002_32 : 0.5945393443107605\n",
      "ptend_q0002_33 : 0.5838892459869385\n",
      "ptend_q0002_34 : 0.5774684548377991\n",
      "ptend_q0002_35 : 0.5712723731994629\n",
      "ptend_q0002_36 : 0.5670346617698669\n",
      "ptend_q0002_37 : 0.5722234845161438\n",
      "ptend_q0002_38 : 0.5860705375671387\n",
      "ptend_q0002_39 : 0.590247392654419\n",
      "ptend_q0002_40 : 0.5830978155136108\n",
      "ptend_q0002_41 : 0.5754106640815735\n",
      "ptend_q0002_42 : 0.5774444341659546\n",
      "ptend_q0002_43 : 0.591089129447937\n",
      "ptend_q0002_44 : 0.6039829850196838\n",
      "ptend_q0002_45 : 0.6021322011947632\n",
      "ptend_q0002_46 : 0.6017800569534302\n",
      "ptend_q0002_47 : 0.5981848239898682\n",
      "ptend_q0002_48 : 0.6023703217506409\n",
      "ptend_q0002_49 : 0.607934296131134\n",
      "ptend_q0002_50 : 0.6126512289047241\n",
      "ptend_q0002_51 : 0.612880289554596\n",
      "ptend_q0002_52 : 0.6054807901382446\n",
      "ptend_q0002_53 : 0.5958830714225769\n",
      "ptend_q0002_54 : 0.5486251711845398\n",
      "ptend_q0002_55 : 0.4494171142578125\n",
      "ptend_q0002_56 : 0.3332922160625458\n",
      "ptend_q0002_57 : 0.21290767192840576\n",
      "ptend_q0002_58 : 0.12958943843841553\n",
      "ptend_q0002_59 : 0.09645377099514008\n",
      "ptend_q0003_0 : 5.819930265715811e-06\n",
      "ptend_q0003_1 : 8.388437890971545e-06\n",
      "ptend_q0003_2 : 1.0411281436972786e-05\n",
      "ptend_q0003_3 : 9.210354619426653e-06\n",
      "ptend_q0003_4 : 7.604169695696328e-06\n",
      "ptend_q0003_5 : 6.711194600939052e-06\n",
      "ptend_q0003_6 : 6.972745723032858e-06\n",
      "ptend_q0003_7 : 8.396597877435852e-06\n",
      "ptend_q0003_8 : 3.5031073366553755e-06\n",
      "ptend_q0003_9 : 7.493234988942277e-06\n",
      "ptend_q0003_10 : 1.0192224181082565e-05\n",
      "ptend_q0003_11 : 6.687492714263499e-06\n",
      "ptend_q0003_12 : 0.017481261864304543\n",
      "ptend_q0003_13 : 0.02655377984046936\n",
      "ptend_q0003_14 : 0.050433892756700516\n",
      "ptend_q0003_15 : 1.1975325345993042\n",
      "ptend_q0003_16 : 2.121037244796753\n",
      "ptend_q0003_17 : 0.8343653082847595\n",
      "ptend_q0003_18 : 0.29284873604774475\n",
      "ptend_q0003_19 : 0.40244659781455994\n",
      "ptend_q0003_20 : 0.35151922702789307\n",
      "ptend_q0003_21 : 0.3424481153488159\n",
      "ptend_q0003_22 : 0.40080514550209045\n",
      "ptend_q0003_23 : 0.4280017912387848\n",
      "ptend_q0003_24 : 0.4621414542198181\n",
      "ptend_q0003_25 : 0.4813304543495178\n",
      "ptend_q0003_26 : 0.5059670209884644\n",
      "ptend_q0003_27 : 0.49002212285995483\n",
      "ptend_q0003_28 : 0.5002546310424805\n",
      "ptend_q0003_29 : 0.49004897475242615\n",
      "ptend_q0003_30 : 0.4513409733772278\n",
      "ptend_q0003_31 : 0.4208166003227234\n",
      "ptend_q0003_32 : 0.4178207516670227\n",
      "ptend_q0003_33 : 0.4253040552139282\n",
      "ptend_q0003_34 : 0.421541303396225\n",
      "ptend_q0003_35 : 0.40971747040748596\n",
      "ptend_q0003_36 : 0.38497796654701233\n",
      "ptend_q0003_37 : 0.397562175989151\n",
      "ptend_q0003_38 : 0.4211099147796631\n",
      "ptend_q0003_39 : 0.4538787603378296\n",
      "ptend_q0003_40 : 0.4787750840187073\n",
      "ptend_q0003_41 : 0.4864274263381958\n",
      "ptend_q0003_42 : 0.49884992837905884\n",
      "ptend_q0003_43 : 0.5248963832855225\n",
      "ptend_q0003_44 : 0.5262092351913452\n",
      "ptend_q0003_45 : 0.5320965647697449\n",
      "ptend_q0003_46 : 0.5173166990280151\n",
      "ptend_q0003_47 : 0.4901857078075409\n",
      "ptend_q0003_48 : 0.4564543068408966\n",
      "ptend_q0003_49 : 0.40745100378990173\n",
      "ptend_q0003_50 : 0.37155836820602417\n",
      "ptend_q0003_51 : 0.3292009234428406\n",
      "ptend_q0003_52 : 0.27844950556755066\n",
      "ptend_q0003_53 : 0.2311905473470688\n",
      "ptend_q0003_54 : 0.18908695876598358\n",
      "ptend_q0003_55 : 0.155936136841774\n",
      "ptend_q0003_56 : 0.12714883685112\n",
      "ptend_q0003_57 : 0.1024840921163559\n",
      "ptend_q0003_58 : 0.08912813663482666\n",
      "ptend_q0003_59 : 0.07357762008905411\n",
      "ptend_u_0 : 4.404862011142541e-06\n",
      "ptend_u_1 : 4.060190804011654e-06\n",
      "ptend_u_2 : 6.11784344073385e-06\n",
      "ptend_u_3 : 6.392143404809758e-06\n",
      "ptend_u_4 : 9.415293789061252e-06\n",
      "ptend_u_5 : 9.195406164508313e-06\n",
      "ptend_u_6 : 5.585679900832474e-06\n",
      "ptend_u_7 : 5.619472176476847e-06\n",
      "ptend_u_8 : 9.343561941932421e-06\n",
      "ptend_u_9 : 8.980468010122422e-06\n",
      "ptend_u_10 : 9.10574453882873e-06\n",
      "ptend_u_11 : 8.327555406140164e-06\n",
      "ptend_u_12 : 0.06768877804279327\n",
      "ptend_u_13 : 0.06633441150188446\n",
      "ptend_u_14 : 0.05851114168763161\n",
      "ptend_u_15 : 0.06066855415701866\n",
      "ptend_u_16 : 0.24678504467010498\n",
      "ptend_u_17 : 0.11805107444524765\n",
      "ptend_u_18 : 0.052681416273117065\n",
      "ptend_u_19 : 0.07343871891498566\n",
      "ptend_u_20 : 0.10580067336559296\n",
      "ptend_u_21 : 0.14549261331558228\n",
      "ptend_u_22 : 0.17301318049430847\n",
      "ptend_u_23 : 0.19562312960624695\n",
      "ptend_u_24 : 0.22531959414482117\n",
      "ptend_u_25 : 0.2666533887386322\n",
      "ptend_u_26 : 0.297401487827301\n",
      "ptend_u_27 : 0.3188321888446808\n",
      "ptend_u_28 : 0.3543592095375061\n",
      "ptend_u_29 : 0.378635048866272\n",
      "ptend_u_30 : 0.40618741512298584\n",
      "ptend_u_31 : 0.4270399212837219\n",
      "ptend_u_32 : 0.4283556640148163\n",
      "ptend_u_33 : 0.45072391629219055\n",
      "ptend_u_34 : 0.4771459102630615\n",
      "ptend_u_35 : 0.5025888085365295\n",
      "ptend_u_36 : 0.4661293029785156\n",
      "ptend_u_37 : 0.45029938220977783\n",
      "ptend_u_38 : 0.4429944157600403\n",
      "ptend_u_39 : 0.45230743288993835\n",
      "ptend_u_40 : 0.47054916620254517\n",
      "ptend_u_41 : 0.46149304509162903\n",
      "ptend_u_42 : 0.4577620029449463\n",
      "ptend_u_43 : 0.4578808844089508\n",
      "ptend_u_44 : 0.4413842558860779\n",
      "ptend_u_45 : 0.4035293757915497\n",
      "ptend_u_46 : 0.3604867458343506\n",
      "ptend_u_47 : 0.31267356872558594\n",
      "ptend_u_48 : 0.27217361330986023\n",
      "ptend_u_49 : 0.2233748435974121\n",
      "ptend_u_50 : 0.1772409975528717\n",
      "ptend_u_51 : 0.13996997475624084\n",
      "ptend_u_52 : 0.1222052350640297\n",
      "ptend_u_53 : 0.12162958830595016\n",
      "ptend_u_54 : 0.11953070014715195\n",
      "ptend_u_55 : 0.11607675999403\n",
      "ptend_u_56 : 0.09671073406934738\n",
      "ptend_u_57 : 0.10037703067064285\n",
      "ptend_u_58 : 0.12210890650749207\n",
      "ptend_u_59 : 0.041311219334602356\n",
      "ptend_v_0 : 8.09752327768365e-06\n",
      "ptend_v_1 : 2.6170662295044167e-06\n",
      "ptend_v_2 : 1.0611969628371298e-05\n",
      "ptend_v_3 : 2.209233571193181e-06\n",
      "ptend_v_4 : 8.016309948288836e-06\n",
      "ptend_v_5 : 9.490809134149458e-06\n",
      "ptend_v_6 : 5.024875918024918e-06\n",
      "ptend_v_7 : 9.200684871757403e-06\n",
      "ptend_v_8 : 6.886073151690653e-06\n",
      "ptend_v_9 : 9.301483260060195e-06\n",
      "ptend_v_10 : 6.6889915615320206e-06\n",
      "ptend_v_11 : 7.740640285192057e-06\n",
      "ptend_v_12 : 0.0696830078959465\n",
      "ptend_v_13 : 0.0633772760629654\n",
      "ptend_v_14 : 0.052547257393598557\n",
      "ptend_v_15 : 0.06518992781639099\n",
      "ptend_v_16 : 0.06335733085870743\n",
      "ptend_v_17 : 0.07634377479553223\n",
      "ptend_v_18 : 0.06251033395528793\n",
      "ptend_v_19 : 0.0840507447719574\n",
      "ptend_v_20 : 0.11159807443618774\n",
      "ptend_v_21 : 0.12358283251523972\n",
      "ptend_v_22 : 0.14357522130012512\n",
      "ptend_v_23 : 0.15240751206874847\n",
      "ptend_v_24 : 0.16887111961841583\n",
      "ptend_v_25 : 0.19781838357448578\n",
      "ptend_v_26 : 0.22950199246406555\n",
      "ptend_v_27 : 0.2452600598335266\n",
      "ptend_v_28 : 0.22696281969547272\n",
      "ptend_v_29 : 0.22090375423431396\n",
      "ptend_v_30 : 0.22840425372123718\n",
      "ptend_v_31 : 0.2600022256374359\n",
      "ptend_v_32 : 0.26171860098838806\n",
      "ptend_v_33 : 0.2633908987045288\n",
      "ptend_v_34 : 0.28401774168014526\n",
      "ptend_v_35 : 0.3012414872646332\n",
      "ptend_v_36 : 0.29520589113235474\n",
      "ptend_v_37 : 0.2725425362586975\n",
      "ptend_v_38 : 0.25751993060112\n",
      "ptend_v_39 : 0.25825372338294983\n",
      "ptend_v_40 : 0.27146026492118835\n",
      "ptend_v_41 : 0.2831685245037079\n",
      "ptend_v_42 : 0.2899341285228729\n",
      "ptend_v_43 : 0.2944375276565552\n",
      "ptend_v_44 : 0.2964748442173004\n",
      "ptend_v_45 : 0.28732776641845703\n",
      "ptend_v_46 : 0.27669426798820496\n",
      "ptend_v_47 : 0.26370635628700256\n",
      "ptend_v_48 : 0.24933890998363495\n",
      "ptend_v_49 : 0.22235476970672607\n",
      "ptend_v_50 : 0.19688890874385834\n",
      "ptend_v_51 : 0.1663772612810135\n",
      "ptend_v_52 : 0.13035818934440613\n",
      "ptend_v_53 : 0.104792021214962\n",
      "ptend_v_54 : 0.089270681142807\n",
      "ptend_v_55 : 0.08425319194793701\n",
      "ptend_v_56 : 0.08115401118993759\n",
      "ptend_v_57 : 0.082495778799057\n",
      "ptend_v_58 : 0.08986340463161469\n",
      "ptend_v_59 : 0.03776145353913307\n",
      "cam_out_NETSW : 0.005439972039312124\n",
      "cam_out_FLWDS : 0.005486462265253067\n",
      "cam_out_PRECSC : 0.05558308586478233\n",
      "cam_out_PRECC : 0.08726556599140167\n",
      "cam_out_SOLS : 0.01344381831586361\n",
      "cam_out_SOLL : 0.02369607612490654\n",
      "cam_out_SOLSD : 0.01945192925632\n",
      "cam_out_SOLLD : 0.07778350263834\n"
     ]
    }
   ],
   "source": [
    "for n, col in enumerate(TARGET_COLS):\n",
    "    r = mse(preds[:, n:n+1], targets[:, n:n+1])\n",
    "    print(f'{col} : {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7032)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared(preds, targets)#, mask=~norm_y.zero_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3763, -1.1669, -0.4884,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1509, -1.2552, -1.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.3688,  0.5166,  0.1309,  ...,  0.0021,  0.1160,  0.0994],\n",
       "        ...,\n",
       "        [ 0.2467,  0.1420, -0.0272,  ...,  0.0320,  0.2035,  0.2017],\n",
       "        [ 1.3127, -0.3512,  1.1094,  ...,  1.6892,  0.9116,  0.2239],\n",
       "        [-0.6008, -0.4522, -0.5116,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
