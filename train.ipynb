{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import fastai.vision.all as fv\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Enable TFloat\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "from torchvision.ops import Permute\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Optional\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "\n",
    "from arch import Net, NetTr, NetMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pl.read_csv('/mnt/ssd/kaggle/test.csv')\n",
    "weightings = pd.read_csv('/mnt/ssd/kaggle/sample_submission.csv', nrows=1)\n",
    "#train_df = pl.read_csv('/mnt/ssd/kaggle/train.csv', n_rows=1_000_000, n_threads=10) # dtypes=[pl.datatypes.String] + [pl.Float32]*924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weightings.iloc[0, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.write_parquet('/mnt/ssd/kaggle/train3.parquet')\n",
    "#test_df.write_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "#test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "train_df = pl.read_parquet('/mnt/ssd/kaggle/train2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config(tbl_cols=-1)\n",
    "train_df = pl.read_parquet('/mnt/ssd/kaggle/train2.parquet')\n",
    "\n",
    "NUM_VERT = 60\n",
    "NUM_VERT_FEAT = 9\n",
    "NUM_VERT_FEAT_Y = 6\n",
    "\n",
    "FEAT_COLS = train_df.columns[1:557]\n",
    "TARGET_COLS= train_df.columns[557:]\n",
    "\n",
    "\n",
    "NUM_2D_FEAT = len(FEAT_COLS) - NUM_VERT*NUM_VERT_FEAT\n",
    "NUM_2D_FEAT_Y = len(TARGET_COLS) - NUM_VERT*NUM_VERT_FEAT_Y\n",
    "\n",
    "# Predict a multiplier of q for q_tends\n",
    "FRAC_IDXS = (NUM_VERT, NUM_VERT*4)\n",
    "\n",
    "NUM_2D_FEAT, NUM_2D_FEAT_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GRID_CELLS = 384\n",
    "emb_idxs = np.arange(len(train_df), dtype=np.int64) % NUM_GRID_CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.select(FEAT_COLS).to_numpy()\n",
    "y_train = train_df.select(TARGET_COLS).to_numpy()*weights[None, :]\n",
    "\n",
    "x_train, x_val, emb_train, emb_val, y_train, y_val = train_test_split(x_train, emb_idxs, y_train, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = test_df.select(FEAT_COLS).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del train_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3983526/2352397551.py:1: RuntimeWarning: divide by zero encountered in divide\n",
      "  std_weights = 1.0/(weights)\n"
     ]
    }
   ],
   "source": [
    "std_weights = 1.0/(weights)\n",
    "std_weights[weights == 0] = 0\n",
    "assert np.isfinite(std_weights).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000130088962e-15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Norm():\n",
    "    def __init__(self, fname=None, stds=None, means=None, zero_mask=None, dataset=None, eps=1e-14):\n",
    "        if dataset is not None:        \n",
    "            self.means, self.stds = np.mean(dataset, axis=0), np.std(dataset, axis=0)\n",
    "            with open(fname, 'w') as f:\n",
    "                f.write(json.dumps({'means' : self.means.tolist(), 'stds' : self.stds.tolist()}))\n",
    "        elif means is not None and stds is not None:\n",
    "            self.stds = stds.copy()\n",
    "            self.means = means.copy()\n",
    "        else:\n",
    "            with open(fname) as f:\n",
    "                stats_dict = json.loads(f.read())\n",
    "                \n",
    "            self.means = np.asarray(stats_dict['means'])\n",
    "            self.stds = np.asarray(stats_dict['stds'])\n",
    "            \n",
    "        self.means = self.means[None, :]\n",
    "        self.stds = self.stds[None, :]\n",
    "\n",
    "        self.zero_mask = self.stds[0] <= eps if zero_mask is None else zero_mask\n",
    "         \n",
    "        self.stds[:, self.zero_mask] = 1.0\n",
    "        \n",
    "        self.eps = eps\n",
    "        #self.df = pd.DataFrame({'col' : names, 'std' : self.stds, 'mean' : self.means})\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        out = (data - self.means) / self.stds\n",
    "        out[:, self.zero_mask] = 0\n",
    "        \n",
    "        return out.astype(np.float32)\n",
    "        \n",
    "    def denorm(self, data):\n",
    "        data = data.astype(np.float64)\n",
    "        out = data * self.stds + self.means \n",
    "        \n",
    "        out[:, self.zero_mask] = 0#self.means[:, self.zero_mask]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 368)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.22775704e-05, 4.44396428e-05, 5.29264435e-05, 6.88978325e-05,\n",
       "       9.13713666e-05, 1.10314264e-04, 1.03480364e-04, 7.88111647e-05,\n",
       "       5.02760849e-05, 3.87126165e-05, 2.95069094e-05, 2.26639477e-05,\n",
       "       1.67192611e-05, 1.25890556e-05, 9.31455907e-06, 7.36806487e-06,\n",
       "       6.69344763e-06, 7.78252797e-06, 1.08996301e-05, 1.37459374e-05,\n",
       "       1.50304659e-05, 1.58900893e-05, 1.76646390e-05, 2.02129868e-05,\n",
       "       2.32400416e-05, 2.70909641e-05, 3.17592094e-05, 3.71773858e-05,\n",
       "       4.28878302e-05, 4.88764999e-05, 5.43901442e-05, 5.84405570e-05,\n",
       "       6.12077129e-05, 6.30606082e-05, 6.41811348e-05, 6.45261607e-05,\n",
       "       6.40522412e-05, 6.32996162e-05, 6.26008696e-05, 6.19947605e-05,\n",
       "       6.14956589e-05, 6.10802963e-05, 6.09866947e-05, 6.12540025e-05,\n",
       "       6.16194520e-05, 6.17596161e-05, 6.11949872e-05, 6.00755338e-05,\n",
       "       5.88045259e-05, 5.73713623e-05, 5.58433348e-05, 5.42547896e-05,\n",
       "       5.25417854e-05, 5.07579280e-05, 4.89998238e-05, 4.76935238e-05,\n",
       "       4.71822132e-05, 4.74191620e-05, 5.14458443e-05, 7.31106265e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.14740942e-12, 9.23171912e-13, 6.80111349e-12, 2.81246137e-11,\n",
       "       2.84467744e-11, 2.16983896e-11, 4.05033680e-11, 8.81579462e-11,\n",
       "       1.85300997e-10, 4.08271084e-10, 8.83215945e-10, 1.72548309e-09,\n",
       "       3.02829117e-09, 4.81663243e-09, 6.91657220e-09, 9.09642672e-09,\n",
       "       1.12731184e-08, 1.35465053e-08, 1.57193192e-08, 1.74671690e-08,\n",
       "       1.88764471e-08, 2.01397015e-08, 2.14845954e-08, 2.31655175e-08,\n",
       "       2.51734598e-08, 2.75351812e-08, 3.02503089e-08, 3.34794272e-08,\n",
       "       3.70612163e-08, 4.09563548e-08, 4.48960940e-08, 4.86536891e-08,\n",
       "       5.20393151e-08, 5.50427579e-08, 5.71367984e-08, 5.82413300e-08,\n",
       "       5.88049112e-08, 5.88288032e-08, 5.85381059e-08, 5.80451029e-08,\n",
       "       5.73203067e-08, 5.63145370e-08, 5.45075203e-08, 5.15448662e-08,\n",
       "       4.87644982e-08, 4.45044783e-08, 4.26766249e-08, 3.81618754e-08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 1.00000000e-15, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 1.00000000e-15, 1.00000000e-15,\n",
       "       1.00000000e-15, 1.00000000e-15, 2.72195515e-13, 2.69247428e-12,\n",
       "       7.03276257e-11, 3.33126471e-10, 6.97950364e-10, 1.13045473e-09,\n",
       "       1.78530624e-09, 2.59032085e-09, 3.48029650e-09, 4.49027171e-09,\n",
       "       5.52274937e-09, 6.46743237e-09, 7.24145588e-09, 7.89853072e-09,\n",
       "       8.47688675e-09, 9.00853703e-09, 9.50688683e-09, 9.98321426e-09,\n",
       "       1.04636824e-08, 1.09340483e-08, 1.12521876e-08, 1.12574838e-08,\n",
       "       1.09411449e-08, 1.03535607e-08, 9.56867297e-09, 8.62849081e-09,\n",
       "       7.57921992e-09, 6.47278764e-09, 5.46006751e-09, 4.64789052e-09,\n",
       "       4.04598977e-09, 3.74822617e-09, 3.58275098e-09, 2.69875300e-09,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.13937971e-12, 8.51189236e-13, 7.87026314e-13, 4.60250137e-11,\n",
       "       3.16510096e-10, 9.16881515e-10, 1.37368850e-09, 2.60145994e-09,\n",
       "       3.43893980e-09, 4.29731939e-09, 5.06412556e-09, 5.73687586e-09,\n",
       "       6.22911545e-09, 6.51181109e-09, 6.57375221e-09, 6.53101440e-09,\n",
       "       6.49538112e-09, 6.50471366e-09, 6.46000364e-09, 6.12196649e-09,\n",
       "       5.55515012e-09, 4.99190422e-09, 4.52991689e-09, 4.14437462e-09,\n",
       "       3.80417742e-09, 3.51557006e-09, 3.27138339e-09, 3.05245562e-09,\n",
       "       2.85328428e-09, 2.67404987e-09, 2.51005949e-09, 2.36111708e-09,\n",
       "       2.22001684e-09, 2.08830619e-09, 1.96772798e-09, 1.86112425e-09,\n",
       "       1.76412085e-09, 1.68174963e-09, 1.61364266e-09, 1.55910052e-09,\n",
       "       1.50763602e-09, 1.45080004e-09, 1.39235257e-09, 1.34417411e-09,\n",
       "       1.31272160e-09, 1.29380961e-09, 1.24377930e-09, 1.29458422e-09,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.16739977e-07, 5.00172860e-07, 1.10541669e-06, 2.30508431e-06,\n",
       "       4.82622227e-06, 9.27333258e-06, 1.73466815e-05, 2.46265845e-05,\n",
       "       2.09468526e-05, 1.93538308e-05, 1.77184957e-05, 1.65432939e-05,\n",
       "       1.53253386e-05, 1.39041676e-05, 1.27340554e-05, 1.19872020e-05,\n",
       "       1.14893683e-05, 1.10632045e-05, 1.06402913e-05, 1.02482109e-05,\n",
       "       9.85918996e-06, 9.55742507e-06, 9.46207638e-06, 9.61884507e-06,\n",
       "       1.00350908e-05, 1.06055222e-05, 1.11713243e-05, 1.16407036e-05,\n",
       "       1.20794857e-05, 1.26338737e-05, 1.33604435e-05, 1.42088202e-05,\n",
       "       1.52858338e-05, 1.66795235e-05, 1.82600779e-05, 1.98560392e-05,\n",
       "       2.16391345e-05, 2.38109260e-05, 2.65307935e-05, 2.95554255e-05,\n",
       "       3.14010213e-05, 3.13144410e-05, 3.17916383e-05, 3.32166237e-05,\n",
       "       3.70949747e-05, 3.60229969e-05, 3.34970500e-05, 5.22645460e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.31234614e-07, 3.17622209e-07, 7.64283527e-07, 1.85008480e-06,\n",
       "       4.64603909e-06, 9.75165221e-06, 1.46084203e-05, 1.97267491e-05,\n",
       "       1.94221830e-05, 1.91921008e-05, 1.85118715e-05, 1.79031722e-05,\n",
       "       1.65707497e-05, 1.44946525e-05, 1.26427221e-05, 1.14188706e-05,\n",
       "       1.06203834e-05, 9.81319317e-06, 8.94878758e-06, 8.16588818e-06,\n",
       "       7.57078578e-06, 7.09011238e-06, 6.83274357e-06, 6.85151645e-06,\n",
       "       7.16862360e-06, 7.78156482e-06, 8.57151281e-06, 9.30592705e-06,\n",
       "       9.97407096e-06, 1.06259777e-05, 1.13071137e-05, 1.20867971e-05,\n",
       "       1.29767186e-05, 1.40186530e-05, 1.51515778e-05, 1.61814933e-05,\n",
       "       1.71711090e-05, 1.82764907e-05, 1.96750298e-05, 2.17112010e-05,\n",
       "       2.45457431e-05, 2.75210659e-05, 2.94277124e-05, 2.97710176e-05,\n",
       "       2.94213805e-05, 2.75687617e-05, 2.42816604e-05, 3.42534695e-05,\n",
       "       2.46693466e+02, 7.20335312e+01, 7.40028749e-09, 8.18349477e-08,\n",
       "       1.10247169e+02, 1.16415993e+02, 4.64321098e+01, 2.97334728e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 556)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y_train*weights[None, :]\n",
    "# my = y.mean(axis=0)\n",
    "# sy = (y*y).mean(axis=0)\n",
    "# sy[sy < 1e-14] = 1e-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # with open('stats_y_squared.json', 'w') as f:\n",
    " #    f.write(json.dumps({'means' : my.tolist(), 'stds' : sy.tolist()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x = Norm(fname='x_stats8.json', eps=1e-7)\n",
    "# Set means to zero for q vars so we can predict a multipler\n",
    "norm_x.means[:, FRAC_IDXS[0]:FRAC_IDXS[1]] = 0.0\n",
    "\n",
    "norm_y = Norm(stds=np.ones(y_train.shape[1]), means=np.zeros_like(std_weights), zero_mask=std_weights<1e-13)# dataset=y_train)\n",
    "# This variable still seems to have norm issues.\n",
    "indxs = [TARGET_COLS.index(a) for a  in ['ptend_q0002_26', 'ptend_q0002_25']]\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indxs] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable still seems to have norm issues.\n",
    "indxs = [TARGET_COLS.index(a) for a  in ['ptend_q0002_26', 'ptend_q0002_25']]\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indxs] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_t = norm_y(y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_norm = norm_x(x_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_norm.max(), pred_norm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = norm_y.denorm(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_train[0:1000] - pred).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    def __init__(self, data_dict, norm_dict):\n",
    "        self.data_dict = data_dict\n",
    "        self.norm_dict = norm_dict\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_dict['x'][idx].copy()\n",
    "        y = self.data_dict['y'][idx].copy() if 'y' in self.data_dict else np.zeros(1)\n",
    "        x = self.norm_dict['x'](x[None, :])[0]\n",
    "        y = self.norm_dict['y'](y[None, :])[0] if 'y' in self.data_dict else np.zeros(1)\n",
    "        emb_idx = self.data_dict['emb'][idx]\n",
    "        return (x.astype(np.float32), emb_idx), y.astype(np.float32)\n",
    "    \n",
    "    def __len__(self,):\n",
    "        return self.data_dict['x'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 200000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_train), len(emb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Loader({'x' : x_train, 'y' : y_train, 'emb' : emb_train}, {'x' : norm_x, 'y' : norm_y})\n",
    "val_ds = Loader({'x' : x_val, 'y' : y_val, 'emb' : emb_val}, {'x' : norm_x, 'y' : norm_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800000, 200000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds), #len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "bs = 128\n",
    "train_loader = fv.DataLoader(train_ds, batch_size=bs, drop_last=True, \n",
    "                          shuffle=True, num_workers=0, pin_memory=False)\n",
    "valid_loader = fv.DataLoader(val_ds, batch_size=bs, drop_last=True,\n",
    "                             shuffle=False, num_workers=0, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# x = batch[0]\n",
    "# y = batch[1]\n",
    "# x[0].mean(), x[0].std(), y.mean(), y.std(), x[0].min(), x[0].max(), y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[0][:, FRAC_IDXS[0]:FRAC_IDXS[1]].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.compile(net)\n",
    "dls = fv.DataLoaders(train_loader, valid_loader).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(NUM_2D_FEAT, NUM_VERT_FEAT, NUM_2D_FEAT_Y, NUM_VERT_FEAT_Y, frac_idxs=None, \n",
    "          dim=512, depth=8)\n",
    "#net(batch[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.compile(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(pred, tar):\n",
    "    diff = torch.abs(tar - pred)\n",
    "    return diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_total = np.mean((y_val - y_val.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88578206"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, tar):\n",
    "    diff = tar - pred\n",
    "    return ((diff)**2).mean()\n",
    "\n",
    "def r_squared(pred, tar, mask=None):\n",
    "    # tar = norm_y.denorm(tar.cpu().numpy())\n",
    "    # pred = norm_y.denorm(pred.detach().cpu().numpy())\n",
    "    #pred = pred*weights[None, :]\n",
    "    #tar = tar*weights[None, :]\n",
    "    # mask = ~norm_y.zero_mask\n",
    "    # if mask is not None:\n",
    "    #     tar = tar[:, mask]\n",
    "    #     pred = pred[:, mask]\n",
    "    #tar_m = norm_y.means[:, mask]\n",
    "    return 1 - (torch.mean((tar - pred)**2) / s_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(temp_data['model'])#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = fv.Learner(dls, net, loss_func=nn.HuberLoss(delta=10.0), metrics=[fv.mae, fv.mse, r_squared], #nn.HuberLoss(delta=2.0)\n",
    "                   wd=0.001, opt_func=fv.ranger, cbs=[fv.SaveModelCallback(monitor='r_squared', comp=np.greater,)\n",
    "                                                      , fv.GradientClip()]).to_fp16()#.load('model_temp') #fv.SaveModelCallback(monitor='r_squared', comp=np.greater,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'hook_fn' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/hook.py:21)\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: ___check_type_id(L['module'], 97391516797472)               \n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:27.839000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'to_detach' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:237)\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: tensor 'L['b'][0]' stride mismatch at index 0. expected 30720, actual 122880\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:29.842000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: '_track' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/hook.py:149)\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: ___check_type_id(L['m'], 97391516797472)                    \n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:30.225000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357]    function: 'apply' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:220)\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357]    last reason: tensor 'L['x']' stride mismatch at index 0. expected 122880, actual 61440\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0512 10:07:30.400000 130080114145088 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab269bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_119'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb91120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_60'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e0b04c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_59'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2239e8c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_70'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e226c3d00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_10'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2058fa30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_71'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2191a0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_11'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabb817e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_80'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22d2b370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_20'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabd7edd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_81'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2038dea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_21'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabb11240>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_78'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21976170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_18'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabd7f1c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_76'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb079a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_74'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20615ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_16'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabc29990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_77'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22389bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_17'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ea21900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_91'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21cfb370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_31'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab853010>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_99'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2095c160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_39'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab73e290>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_97'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21a5f130>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_37'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab91a050>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_95'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e208aa5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_93'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22c532e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_35'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab885870>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_96'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e21daa0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_36'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e223ce3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_62'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22e56f80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_2'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2058e680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_72'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20f40790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_12'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0dffb9a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_82'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e202fa680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_22'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ec05870>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_90'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2081a9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_30'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f3c2cb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_68'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e228c8670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_8'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f264e50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_88'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f743520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_28'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e02b520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_66'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2056d6c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_64'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ed5360>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_6'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ebbb010>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_67'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ba7910>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_7'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e0b3640>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_86'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e207ac670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_84'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f78f400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_26'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e920160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_87'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f4635b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_27'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20aee4d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_108'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ee43400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_48'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab127ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_118'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20c07400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_58'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab41c820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_117'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e226e05e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_57'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0eb99990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_109'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20d011b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_49'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab448a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_115'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e2238a320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_55'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aabc2a680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_113'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab945b40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_111'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0edc0670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_53'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab6511b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_114'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f56fbe0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_54'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x764aab793b50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_100'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ea33f40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_40'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f375ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_106'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0ee15120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_46'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f233d00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_104'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e208aac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_102'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e223cc160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_44'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e49f7f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_105'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f230af0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_45'\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule (Input shape: 128 x torch.Size([128]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     128 x 960 x 1       \n",
       "Conv1d                                    1920       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 16 x 60       \n",
       "Rearrange                                                      \n",
       "____________________________________________________________________________\n",
       "                     128 x 9 x 60        \n",
       "Rearrange                                                      \n",
       "____________________________________________________________________________\n",
       "                     128 x 496 x 60      \n",
       "Conv1d                                    4960       True      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Rearrange                                                      \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1536     \n",
       "Linear                                    786432     True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 8        \n",
       "Linear                                    4104       True      \n",
       "Linear                                    262144     True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Dropout                                                        \n",
       "RMSNorm                                                        \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Rearrange                                                      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 6 x 60        \n",
       "Conv1d                                    3078       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 360           \n",
       "Rearrange                                                      \n",
       "Linear                                    129960     True      \n",
       "Identity                                                       \n",
       "Conv1d                                    4096       True      \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    1050624    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 512      \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "LayerNorm                                 1024       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 1024 x 30     \n",
       "Conv1d                                    1049600    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 4096     \n",
       "Linear                                    4198400    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 30 x 1024     \n",
       "Linear                                    4195328    True      \n",
       "Identity                                                       \n",
       "Conv1d                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "PixelShuffle1D                                                 \n",
       "Identity                                                       \n",
       "Conv1d                                    8192       True      \n",
       "LayerNorm                                 2048       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 2048     \n",
       "Linear                                    2099200    True      \n",
       "GELU                                                           \n",
       "GRN                                                            \n",
       "____________________________________________________________________________\n",
       "                     128 x 60 x 1024     \n",
       "Linear                                    2098176    True      \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     128 x 512 x 60      \n",
       "Conv1d                                    1536       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 8 x 60        \n",
       "Conv1d                                    4104       True      \n",
       "____________________________________________________________________________\n",
       "                     128 x 8             \n",
       "Reduce                                                         \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 72,678,102\n",
       "Total trainable params: 72,678,102\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function ranger at 0x764daf6353f0>\n",
       "Loss function: HuberLoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - CastToTensor\n",
       "  - MixedPrecision\n",
       "  - GradientClip\n",
       "  - Recorder\n",
       "  - ProgressCallback\n",
       "  - SaveModelCallback"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22ab7370>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_4'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e22d2b400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_14'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0f798dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_24'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e20db3760>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_33'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e229b09d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_42'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e0e33dbd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_51'\n",
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x763e238bf0a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 469, in <lambda>\n",
      "    self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 716, in _remove_id\n",
      "    hook()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 700, in __call__\n",
      "    del self.scope[self.name]\n",
      "KeyError: '__compiled_fn_0'\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0005207948270253837)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=300, end_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.00022387212084140629)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=300, end_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='5020' class='' max='118750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      4.23% [5020/118750 16:46&lt;6:20:09 0.3498]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:381\u001b[0m, in \u001b[0;36mLookahead.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_weights()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:111\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,pg,state,hyper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_params(with_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: state \u001b[38;5;241m=\u001b[39m _update(state, \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:255\u001b[0m, in \u001b[0;36mradam_step\u001b[0;34m(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, beta, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (sqr_avg\u001b[38;5;241m/\u001b[39mdebias2)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eps: denom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beta: denom \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(denom, beta)\n\u001b[1;32m    256\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(grad_avg, denom, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mv \u001b[38;5;241m/\u001b[39m debias1)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(grad_avg, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m/\u001b[39m debias1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.138223</td>\n",
       "      <td>0.140573</td>\n",
       "      <td>0.188750</td>\n",
       "      <td>0.332497</td>\n",
       "      <td>0.624629</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.131487</td>\n",
       "      <td>0.180172</td>\n",
       "      <td>0.307260</td>\n",
       "      <td>0.653119</td>\n",
       "      <td>23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137762</td>\n",
       "      <td>0.130025</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.298409</td>\n",
       "      <td>0.663113</td>\n",
       "      <td>23:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123283</td>\n",
       "      <td>0.128005</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>0.291168</td>\n",
       "      <td>0.671286</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.125265</td>\n",
       "      <td>0.125230</td>\n",
       "      <td>0.174869</td>\n",
       "      <td>0.283553</td>\n",
       "      <td>0.679884</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.125402</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.283285</td>\n",
       "      <td>0.680187</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.125074</td>\n",
       "      <td>0.122887</td>\n",
       "      <td>0.172365</td>\n",
       "      <td>0.277708</td>\n",
       "      <td>0.686483</td>\n",
       "      <td>23:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.120514</td>\n",
       "      <td>0.123744</td>\n",
       "      <td>0.172361</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>0.681733</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.123021</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>0.171541</td>\n",
       "      <td>0.281699</td>\n",
       "      <td>0.681976</td>\n",
       "      <td>23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.172979</td>\n",
       "      <td>0.278708</td>\n",
       "      <td>0.685353</td>\n",
       "      <td>23:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.117069</td>\n",
       "      <td>0.122796</td>\n",
       "      <td>0.170272</td>\n",
       "      <td>0.276974</td>\n",
       "      <td>0.687311</td>\n",
       "      <td>23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.118279</td>\n",
       "      <td>0.121287</td>\n",
       "      <td>0.169607</td>\n",
       "      <td>0.275288</td>\n",
       "      <td>0.689215</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.111919</td>\n",
       "      <td>0.118910</td>\n",
       "      <td>0.164591</td>\n",
       "      <td>0.266109</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>23:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.099111</td>\n",
       "      <td>0.115342</td>\n",
       "      <td>0.158696</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.709095</td>\n",
       "      <td>23:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.091975</td>\n",
       "      <td>0.115419</td>\n",
       "      <td>0.157185</td>\n",
       "      <td>0.256743</td>\n",
       "      <td>0.710151</td>\n",
       "      <td>23:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6246293783187866.\n",
      "Better model found at epoch 1 with r_squared value: 0.6531193256378174.\n",
      "Better model found at epoch 2 with r_squared value: 0.6631125807762146.\n",
      "Better model found at epoch 3 with r_squared value: 0.6712861657142639.\n",
      "Better model found at epoch 4 with r_squared value: 0.6798838376998901.\n",
      "Better model found at epoch 5 with r_squared value: 0.6801873445510864.\n",
      "Better model found at epoch 6 with r_squared value: 0.6864829063415527.\n",
      "Better model found at epoch 10 with r_squared value: 0.6873109340667725.\n",
      "Better model found at epoch 11 with r_squared value: 0.6892145872116089.\n",
      "Better model found at epoch 12 with r_squared value: 0.6995758414268494.\n",
      "Better model found at epoch 13 with r_squared value: 0.7090945839881897.\n",
      "Better model found at epoch 14 with r_squared value: 0.7101508975028992.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 1e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.113439</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.169136</td>\n",
       "      <td>0.279328</td>\n",
       "      <td>0.684654</td>\n",
       "      <td>1:06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118187</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.168678</td>\n",
       "      <td>0.276340</td>\n",
       "      <td>0.688027</td>\n",
       "      <td>1:06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111758</td>\n",
       "      <td>0.118573</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>1:06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112457</td>\n",
       "      <td>0.118272</td>\n",
       "      <td>0.166766</td>\n",
       "      <td>0.273213</td>\n",
       "      <td>0.691557</td>\n",
       "      <td>1:06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116703</td>\n",
       "      <td>0.118066</td>\n",
       "      <td>0.166885</td>\n",
       "      <td>0.271521</td>\n",
       "      <td>0.693467</td>\n",
       "      <td>1:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.113750</td>\n",
       "      <td>0.118373</td>\n",
       "      <td>0.166376</td>\n",
       "      <td>0.270759</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>1:06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109780</td>\n",
       "      <td>0.118197</td>\n",
       "      <td>0.166344</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>0.694864</td>\n",
       "      <td>1:06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108059</td>\n",
       "      <td>0.118051</td>\n",
       "      <td>0.165657</td>\n",
       "      <td>0.269603</td>\n",
       "      <td>0.695633</td>\n",
       "      <td>1:06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.117424</td>\n",
       "      <td>0.164565</td>\n",
       "      <td>0.267892</td>\n",
       "      <td>0.697564</td>\n",
       "      <td>1:06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.117578</td>\n",
       "      <td>0.164313</td>\n",
       "      <td>0.268119</td>\n",
       "      <td>0.697308</td>\n",
       "      <td>1:06:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6846542954444885.\n",
      "Better model found at epoch 1 with r_squared value: 0.6880269050598145.\n",
      "Better model found at epoch 2 with r_squared value: 0.6905336976051331.\n",
      "Better model found at epoch 3 with r_squared value: 0.6915570497512817.\n",
      "Better model found at epoch 4 with r_squared value: 0.6934674382209778.\n",
      "Better model found at epoch 5 with r_squared value: 0.6943279504776001.\n",
      "Better model found at epoch 6 with r_squared value: 0.6948638558387756.\n",
      "Better model found at epoch 7 with r_squared value: 0.6956329345703125.\n",
      "Better model found at epoch 8 with r_squared value: 0.6975640654563904.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del learn, \n",
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68757"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/model_temp.pth')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('model_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#4) [0.10548990964889526,0.1505843549966812,0.25680434703826904,0.7100826501846313]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      83.33% [5/6 19:54:01&lt;3:58:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.132663</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.170884</td>\n",
       "      <td>0.308618</td>\n",
       "      <td>0.651587</td>\n",
       "      <td>3:58:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116149</td>\n",
       "      <td>0.119035</td>\n",
       "      <td>0.170044</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>0.666922</td>\n",
       "      <td>3:58:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122057</td>\n",
       "      <td>0.117260</td>\n",
       "      <td>0.164114</td>\n",
       "      <td>0.290335</td>\n",
       "      <td>0.672227</td>\n",
       "      <td>3:58:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115437</td>\n",
       "      <td>0.114895</td>\n",
       "      <td>0.169516</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.681435</td>\n",
       "      <td>3:58:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116658</td>\n",
       "      <td>0.111350</td>\n",
       "      <td>0.157978</td>\n",
       "      <td>0.272206</td>\n",
       "      <td>0.692694</td>\n",
       "      <td>3:58:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='16219' class='' max='59375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.32% [16219/59375 1:04:11&lt;2:50:47 0.1096]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6515868902206421.\n",
      "Better model found at epoch 1 with r_squared value: 0.6669217944145203.\n",
      "Better model found at epoch 2 with r_squared value: 0.6722266674041748.\n",
      "Better model found at epoch 3 with r_squared value: 0.6814352869987488.\n",
      "Better model found at epoch 4 with r_squared value: 0.6926941275596619.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(12, 2e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.105264</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>0.160486</td>\n",
       "      <td>0.281360</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>52:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102502</td>\n",
       "      <td>0.111544</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.286838</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>52:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.112530</td>\n",
       "      <td>0.161047</td>\n",
       "      <td>0.277096</td>\n",
       "      <td>0.712049</td>\n",
       "      <td>52:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>0.112151</td>\n",
       "      <td>0.160246</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>52:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102632</td>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.159459</td>\n",
       "      <td>0.279898</td>\n",
       "      <td>0.716126</td>\n",
       "      <td>52:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.108079</td>\n",
       "      <td>0.152991</td>\n",
       "      <td>0.262921</td>\n",
       "      <td>0.725166</td>\n",
       "      <td>52:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7162206768989563.\n",
      "Better model found at epoch 5 with r_squared value: 0.7251663208007812.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-4, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145379</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.195003</td>\n",
       "      <td>0.342077</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>1:11:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125336</td>\n",
       "      <td>0.126382</td>\n",
       "      <td>0.184133</td>\n",
       "      <td>0.314350</td>\n",
       "      <td>0.679277</td>\n",
       "      <td>1:11:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126290</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.179871</td>\n",
       "      <td>0.308917</td>\n",
       "      <td>0.686294</td>\n",
       "      <td>1:11:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6503068208694458.\n",
      "Better model found at epoch 1 with r_squared value: 0.6792766451835632.\n",
      "Better model found at epoch 2 with r_squared value: 0.6862936615943909.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(3, 1e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.113239</td>\n",
       "      <td>0.166109</td>\n",
       "      <td>0.277009</td>\n",
       "      <td>0.715268</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118272</td>\n",
       "      <td>0.112384</td>\n",
       "      <td>0.165332</td>\n",
       "      <td>0.274715</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108533</td>\n",
       "      <td>0.111935</td>\n",
       "      <td>0.165136</td>\n",
       "      <td>0.272125</td>\n",
       "      <td>0.718529</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.110852</td>\n",
       "      <td>0.164110</td>\n",
       "      <td>0.270032</td>\n",
       "      <td>0.721469</td>\n",
       "      <td>1:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103124</td>\n",
       "      <td>0.110735</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.266873</td>\n",
       "      <td>0.722130</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.106697</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>0.159765</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>0.726570</td>\n",
       "      <td>1:11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>0.105747</td>\n",
       "      <td>0.154466</td>\n",
       "      <td>0.253241</td>\n",
       "      <td>0.735189</td>\n",
       "      <td>1:11:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7152675986289978.\n",
      "Better model found at epoch 1 with r_squared value: 0.7176631093025208.\n",
      "Better model found at epoch 2 with r_squared value: 0.718529224395752.\n",
      "Better model found at epoch 3 with r_squared value: 0.7214685678482056.\n",
      "Better model found at epoch 4 with r_squared value: 0.7221300601959229.\n",
      "Better model found at epoch 5 with r_squared value: 0.7265703678131104.\n",
      "Better model found at epoch 6 with r_squared value: 0.7351890802383423.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(7, 4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/tr_v1.pth')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('tr_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      71.43% [10/14 3:19:23&lt;1:19:45]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.288008</td>\n",
       "      <td>0.174881</td>\n",
       "      <td>0.288008</td>\n",
       "      <td>0.690495</td>\n",
       "      <td>19:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.259229</td>\n",
       "      <td>0.278898</td>\n",
       "      <td>0.169802</td>\n",
       "      <td>0.278898</td>\n",
       "      <td>0.698412</td>\n",
       "      <td>19:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.242027</td>\n",
       "      <td>0.269742</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.269742</td>\n",
       "      <td>0.706795</td>\n",
       "      <td>19:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.267588</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.267588</td>\n",
       "      <td>0.708273</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.242983</td>\n",
       "      <td>0.267330</td>\n",
       "      <td>0.169089</td>\n",
       "      <td>0.267330</td>\n",
       "      <td>0.709385</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232772</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.169060</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.709853</td>\n",
       "      <td>19:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.351062</td>\n",
       "      <td>0.265223</td>\n",
       "      <td>0.170867</td>\n",
       "      <td>0.265223</td>\n",
       "      <td>0.708562</td>\n",
       "      <td>19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.227057</td>\n",
       "      <td>0.265318</td>\n",
       "      <td>0.170030</td>\n",
       "      <td>0.265318</td>\n",
       "      <td>0.709494</td>\n",
       "      <td>19:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.223291</td>\n",
       "      <td>0.269968</td>\n",
       "      <td>0.170160</td>\n",
       "      <td>0.269968</td>\n",
       "      <td>0.708803</td>\n",
       "      <td>19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.219373</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>0.266356</td>\n",
       "      <td>0.709530</td>\n",
       "      <td>19:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='23204' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.16% [23204/29687 15:20&lt;04:17 0.2138]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6904946366345599.\n",
      "Better model found at epoch 1 with r_squared value: 0.6984116024016455.\n",
      "Better model found at epoch 2 with r_squared value: 0.7067945812816843.\n",
      "Better model found at epoch 3 with r_squared value: 0.7082729521368367.\n",
      "Better model found at epoch 4 with r_squared value: 0.7093854982283825.\n",
      "Better model found at epoch 5 with r_squared value: 0.7098527482682689.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(14, 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.312845</td>\n",
       "      <td>0.348528</td>\n",
       "      <td>0.186559</td>\n",
       "      <td>0.348528</td>\n",
       "      <td>0.646830</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.295979</td>\n",
       "      <td>0.386613</td>\n",
       "      <td>0.175216</td>\n",
       "      <td>0.386613</td>\n",
       "      <td>0.669071</td>\n",
       "      <td>47:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312540</td>\n",
       "      <td>0.316801</td>\n",
       "      <td>0.172706</td>\n",
       "      <td>0.316801</td>\n",
       "      <td>0.684723</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274675</td>\n",
       "      <td>0.312294</td>\n",
       "      <td>0.167928</td>\n",
       "      <td>0.312294</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.262781</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.165456</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.702070</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253998</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>0.164715</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>0.704825</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229323</td>\n",
       "      <td>0.278633</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.278633</td>\n",
       "      <td>0.702575</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.234593</td>\n",
       "      <td>0.288640</td>\n",
       "      <td>0.165601</td>\n",
       "      <td>0.288640</td>\n",
       "      <td>0.701547</td>\n",
       "      <td>47:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.281531</td>\n",
       "      <td>0.163712</td>\n",
       "      <td>0.281531</td>\n",
       "      <td>0.702765</td>\n",
       "      <td>47:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.217475</td>\n",
       "      <td>0.370952</td>\n",
       "      <td>0.163606</td>\n",
       "      <td>0.370952</td>\n",
       "      <td>0.695579</td>\n",
       "      <td>47:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6468300950037681.\n",
      "Better model found at epoch 1 with r_squared value: 0.6690712099070321.\n",
      "Better model found at epoch 2 with r_squared value: 0.6847234115019918.\n",
      "Better model found at epoch 3 with r_squared value: 0.694712214234656.\n",
      "Better model found at epoch 4 with r_squared value: 0.7020703559530641.\n",
      "Better model found at epoch 5 with r_squared value: 0.7048247286794248.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, 2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [2/6 1:40:58&lt;3:21:56]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.240573</td>\n",
       "      <td>0.271668</td>\n",
       "      <td>50:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>50:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='2744' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      9.24% [2744/29687 04:37&lt;45:28 0.2250]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbefore_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  f()\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/training.py:40\u001b[0m, in \u001b[0;36mGradientClip.before_step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.142789</td>\n",
       "      <td>0.137072</td>\n",
       "      <td>0.184968</td>\n",
       "      <td>0.323550</td>\n",
       "      <td>0.663831</td>\n",
       "      <td>1:31:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131440</td>\n",
       "      <td>0.131322</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.304489</td>\n",
       "      <td>0.680972</td>\n",
       "      <td>1:31:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131792</td>\n",
       "      <td>0.125048</td>\n",
       "      <td>0.173002</td>\n",
       "      <td>0.288943</td>\n",
       "      <td>0.696811</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115277</td>\n",
       "      <td>0.122783</td>\n",
       "      <td>0.169563</td>\n",
       "      <td>0.283435</td>\n",
       "      <td>0.702603</td>\n",
       "      <td>1:31:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.120829</td>\n",
       "      <td>0.167641</td>\n",
       "      <td>0.277083</td>\n",
       "      <td>0.707759</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110554</td>\n",
       "      <td>0.121103</td>\n",
       "      <td>0.167036</td>\n",
       "      <td>0.276273</td>\n",
       "      <td>0.707414</td>\n",
       "      <td>1:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.106923</td>\n",
       "      <td>0.121396</td>\n",
       "      <td>0.168461</td>\n",
       "      <td>0.275641</td>\n",
       "      <td>0.706874</td>\n",
       "      <td>1:31:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108978</td>\n",
       "      <td>0.121609</td>\n",
       "      <td>0.167722</td>\n",
       "      <td>0.276074</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>1:31:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.105256</td>\n",
       "      <td>0.121704</td>\n",
       "      <td>0.166815</td>\n",
       "      <td>0.274988</td>\n",
       "      <td>0.705985</td>\n",
       "      <td>1:31:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.100636</td>\n",
       "      <td>0.120486</td>\n",
       "      <td>0.164992</td>\n",
       "      <td>0.272843</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>1:31:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6638310887952457.\n",
      "Better model found at epoch 1 with r_squared value: 0.6809722879118543.\n",
      "Better model found at epoch 2 with r_squared value: 0.6968106099706167.\n",
      "Better model found at epoch 3 with r_squared value: 0.7026034652435666.\n",
      "Better model found at epoch 4 with r_squared value: 0.7077585955065709.\n",
      "Better model found at epoch 9 with r_squared value: 0.7093534321797625.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, 1e-3)\n",
    "#learn.fit_flat_cos(5, 4e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/model_okay.pth')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('model_okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145789</td>\n",
       "      <td>0.143075</td>\n",
       "      <td>0.187728</td>\n",
       "      <td>0.403799</td>\n",
       "      <td>0.620909</td>\n",
       "      <td>49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.136473</td>\n",
       "      <td>0.134041</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.355374</td>\n",
       "      <td>0.642884</td>\n",
       "      <td>49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130043</td>\n",
       "      <td>0.127936</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.659469</td>\n",
       "      <td>49:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.121174</td>\n",
       "      <td>0.122204</td>\n",
       "      <td>0.166350</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.673414</td>\n",
       "      <td>49:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.121296</td>\n",
       "      <td>0.166364</td>\n",
       "      <td>0.307960</td>\n",
       "      <td>0.674728</td>\n",
       "      <td>49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116663</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.165333</td>\n",
       "      <td>0.306303</td>\n",
       "      <td>0.674654</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.114686</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.166172</td>\n",
       "      <td>0.302324</td>\n",
       "      <td>0.674308</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.108460</td>\n",
       "      <td>0.122085</td>\n",
       "      <td>0.166011</td>\n",
       "      <td>0.303160</td>\n",
       "      <td>0.672353</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.095856</td>\n",
       "      <td>0.119235</td>\n",
       "      <td>0.161396</td>\n",
       "      <td>0.294277</td>\n",
       "      <td>0.679827</td>\n",
       "      <td>49:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088201</td>\n",
       "      <td>0.118416</td>\n",
       "      <td>0.158530</td>\n",
       "      <td>0.291398</td>\n",
       "      <td>0.682537</td>\n",
       "      <td>49:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3, 1e-3)\n",
    "learn.fit_flat_cos(7, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.107994</td>\n",
       "      <td>0.106488</td>\n",
       "      <td>0.174495</td>\n",
       "      <td>69.499443</td>\n",
       "      <td>0.993962</td>\n",
       "      <td>1:10:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>0.102991</td>\n",
       "      <td>0.169497</td>\n",
       "      <td>67.631531</td>\n",
       "      <td>0.994168</td>\n",
       "      <td>1:10:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.993962274233683.\n",
      "Better model found at epoch 1 with r_squared value: 0.9941675411417258.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      41.67% [5/12 5:55:20&lt;8:17:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109214</td>\n",
       "      <td>0.099076</td>\n",
       "      <td>0.163365</td>\n",
       "      <td>66.396835</td>\n",
       "      <td>0.994430</td>\n",
       "      <td>1:11:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092406</td>\n",
       "      <td>0.098987</td>\n",
       "      <td>0.163418</td>\n",
       "      <td>64.059464</td>\n",
       "      <td>0.994410</td>\n",
       "      <td>1:11:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092613</td>\n",
       "      <td>0.097975</td>\n",
       "      <td>0.162091</td>\n",
       "      <td>62.233765</td>\n",
       "      <td>0.994467</td>\n",
       "      <td>1:11:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091485</td>\n",
       "      <td>0.098095</td>\n",
       "      <td>0.161914</td>\n",
       "      <td>61.303112</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>1:11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.162724</td>\n",
       "      <td>58.517742</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>1:11:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='433' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.46% [433/29687 01:02&lt;1:10:20 0.0878]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.9944301319218759.\n",
      "Better model found at epoch 2 with r_squared value: 0.994466553097564.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:211\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBackwardException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelStepException)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-3)\n",
    "learn.fit_flat_cos(12, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1562 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_example_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_decomp_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_cut_rematerialization_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2822\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2820\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(args)\n\u001b[0;32m-> 2822\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2515\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2515\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1715\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[0;32m-> 1715\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;66;03m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m#   }\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;66;03m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1328\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context(), track_graph_compiling(aot_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1328\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args_with_views_handled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m create_runtime_wrapper(\n\u001b[1;32m   1331\u001b[0m     compiled_fw,\n\u001b[1;32m   1332\u001b[0m     runtime_metadata\u001b[38;5;241m=\u001b[39mmetadata_,\n\u001b[1;32m   1333\u001b[0m     trace_joint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m     keep_input_mutations\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mkeep_inference_input_mutations\n\u001b[1;32m   1335\u001b[0m )\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:430\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler\u001b[0;34m(model, example_inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m model \u001b[38;5;241m=\u001b[39m convert_outplace_to_inplace(model)\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:595\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/debug.py:239\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:177\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m         graph\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39mexample_inputs)\n\u001b[0;32m--> 177\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cudagraphs:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/graph.py:586\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/graph.py:575\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mprint\u001b[39m(code)\n\u001b[0;32m--> 575\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mPyCodeCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/codecache.py:528\u001b[0m, in \u001b[0;36mPyCodeCache.load\u001b[0;34m(cls, source_code)\u001b[0m\n\u001b[1;32m    527\u001b[0m mod\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m--> 528\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# another thread might set this first\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torchinductor_leroy/63/c63uger4g4aqjhr72kg6zcvh3k4io5bf6y7x5hg7i2s3qiqjfntv.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cuda_getCurrentRawStream \u001b[38;5;28;01mas\u001b[39;00m get_cuda_stream\n\u001b[0;32m---> 20\u001b[0m triton__0 \u001b[38;5;241m=\u001b[39m \u001b[43masync_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43mimport triton\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43mimport triton.language as tl\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.ir import ReductionHint\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.ir import TileHint\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.triton_ops.autotune import pointwise\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43mfrom torch._inductor.utils import instance_descriptor\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m@pointwise(size_hints=[2048], filename=__file__, meta=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m0: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*fp32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 1: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*fp32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 2: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m}, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: 0, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstants\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmutated_arg_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: [], \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfigs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m@triton.jit\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43mdef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m    xnumel = 2048\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43m    xoffset = tl.program_id(0) * XBLOCK\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43m    xindex = xoffset + tl.arange(0, XBLOCK)[:]\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43m    xmask = xindex < xnumel\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m    x0 = xindex \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m 16\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43m    x1 = (xindex // 16)\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m    x2 = xindex\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;43m    tmp0 = tl.load(in_ptr0 + (360 + x0 + (376*x1)), xmask)\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m    tl.store(out_ptr0 + (x2 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m triton__1 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mtriton(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124mimport triton\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124mimport triton.language as tl\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m    tl.store(out_ptr0 + (x2 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_inductor/codecache.py:683\u001b[0m, in \u001b[0;36mAsyncCompile.triton\u001b[0;34m(self, source_code)\u001b[0m\n\u001b[1;32m    682\u001b[0m cc \u001b[38;5;241m=\u001b[39m major \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m minor\n\u001b[0;32m--> 683\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_worker_compile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TritonFuture(source_code, future)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py:720\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit_flat_cos(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7e-4\u001b[39m, pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:248\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_train()\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:216\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orig_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mgraph_module\u001b[38;5;241m.\u001b[39m_forward_from_src \u001b[38;5;241m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[38;5;241m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[38;5;241m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m output \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo start tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m gm\u001b[38;5;241m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-3)\n",
    "learn.fit_flat_cos(3, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.000895364792086184)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(num_it=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.122136</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.181493</td>\n",
       "      <td>0.313605</td>\n",
       "      <td>0.673079</td>\n",
       "      <td>34:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121282</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.176421</td>\n",
       "      <td>0.294398</td>\n",
       "      <td>0.690070</td>\n",
       "      <td>34:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6730790734291077.\n",
      "Better model found at epoch 1 with r_squared value: 0.6900703310966492.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/15 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.109887</td>\n",
       "      <td>0.160886</td>\n",
       "      <td>0.268899</td>\n",
       "      <td>0.718790</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107647</td>\n",
       "      <td>0.109236</td>\n",
       "      <td>0.159493</td>\n",
       "      <td>0.265550</td>\n",
       "      <td>0.721290</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104061</td>\n",
       "      <td>0.108931</td>\n",
       "      <td>0.159251</td>\n",
       "      <td>0.263329</td>\n",
       "      <td>0.722589</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>0.109865</td>\n",
       "      <td>0.159870</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>34:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098991</td>\n",
       "      <td>0.110590</td>\n",
       "      <td>0.160912</td>\n",
       "      <td>0.265195</td>\n",
       "      <td>0.718406</td>\n",
       "      <td>34:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098965</td>\n",
       "      <td>0.111460</td>\n",
       "      <td>0.161282</td>\n",
       "      <td>0.267053</td>\n",
       "      <td>0.716199</td>\n",
       "      <td>34:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.093325</td>\n",
       "      <td>0.111975</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>0.268610</td>\n",
       "      <td>0.714019</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.091725</td>\n",
       "      <td>0.112911</td>\n",
       "      <td>0.161246</td>\n",
       "      <td>0.269824</td>\n",
       "      <td>0.712248</td>\n",
       "      <td>34:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090569</td>\n",
       "      <td>0.113749</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>0.271304</td>\n",
       "      <td>0.709899</td>\n",
       "      <td>34:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.087785</td>\n",
       "      <td>0.114089</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>0.708751</td>\n",
       "      <td>34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.161327</td>\n",
       "      <td>0.273465</td>\n",
       "      <td>0.707173</td>\n",
       "      <td>34:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.078328</td>\n",
       "      <td>0.115303</td>\n",
       "      <td>0.160933</td>\n",
       "      <td>0.274253</td>\n",
       "      <td>0.705873</td>\n",
       "      <td>34:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='1562' class='' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1562/1562 00:26&lt;00:00 0.0751]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7187901735305786.\n",
      "Better model found at epoch 1 with r_squared value: 0.7212895750999451.\n",
      "Better model found at epoch 2 with r_squared value: 0.7225892543792725.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelBackwardException)\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelStepException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._step\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:381\u001b[0m, in \u001b[0;36mLookahead.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_weights()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:111\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,pg,state,hyper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_params(with_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: state \u001b[38;5;241m=\u001b[39m _update(state, \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/optimizer.py:255\u001b[0m, in \u001b[0;36mradam_step\u001b[0;34m(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, beta, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (sqr_avg\u001b[38;5;241m/\u001b[39mdebias2)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eps: denom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beta: denom \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(denom, beta)\n\u001b[1;32m    256\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(grad_avg, denom, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mv \u001b[38;5;241m/\u001b[39m debias1)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(grad_avg, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m/\u001b[39m debias1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(2, 3e-3)\n",
    "learn.fit_flat_cos(5, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn.fit(2, 3e-3)\n",
    "learn.fit_flat_cos(15, 7e-4, pct_start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:59: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x789db9a75300>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:278\u001b[0m, in \u001b[0;36mLearner.validate\u001b[0;34m(self, ds_idx, dl, cbs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_context(cbs\u001b[38;5;241m=\u001b[39mcbs): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate(ds_idx, dl)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_record\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/xtras.py:548\u001b[0m, in \u001b[0;36mContextManagers.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:28\u001b[0m, in \u001b[0;36mreplacing_yield\u001b[0;34m(o, attr, val)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext manager to temporarily replace an attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,attr)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(o,attr,val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m: \u001b[38;5;28msetattr\u001b[39m(o,attr,old)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:268\u001b[0m, in \u001b[0;36mLearner.__exit__\u001b[0;34m(self, exc_type, exc_value, tb)\u001b[0m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, tb): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_after_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:101\u001b[0m, in \u001b[0;36mSaveModelCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_epoch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#every improvement\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBetter model found at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/tracker.py:43\u001b[0m, in \u001b[0;36mTrackerCallback.after_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the last value to the best up to now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomp(val \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m val,\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:112\u001b[0m, in \u001b[0;36mL.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m L(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(idx), use_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:116\u001b[0m, in \u001b[0;36mL._get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_indexer(i) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mslice\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m     i \u001b[38;5;241m=\u001b[39m mask2idxs(i)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39m__array__()[(i,)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i_] \u001b[38;5;28;01mfor\u001b[39;00m i_ \u001b[38;5;129;01min\u001b[39;00m i])\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception occured in `SaveModelCallback` when calling event `after_epoch`:\n\tlist index out of range"
     ]
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 32:01&lt;2:08:06]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.161324</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>0.715918</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='8182' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.56% [8182/29687 08:44&lt;22:57 0.0807]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.7159183621406555.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(5, 1e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#4) [0.11172444373369217,0.16132423281669617,0.26352497935295105,0.7159183621406555]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.129519</td>\n",
       "      <td>0.130109</td>\n",
       "      <td>0.191536</td>\n",
       "      <td>0.325054</td>\n",
       "      <td>0.660586</td>\n",
       "      <td>32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.124522</td>\n",
       "      <td>0.122357</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.302771</td>\n",
       "      <td>0.683851</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116473</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.175865</td>\n",
       "      <td>0.292754</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111453</td>\n",
       "      <td>0.117234</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.289118</td>\n",
       "      <td>0.696406</td>\n",
       "      <td>31:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.112941</td>\n",
       "      <td>0.114254</td>\n",
       "      <td>0.171726</td>\n",
       "      <td>0.278642</td>\n",
       "      <td>0.706919</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108034</td>\n",
       "      <td>0.113473</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>0.274081</td>\n",
       "      <td>0.710151</td>\n",
       "      <td>32:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>0.170169</td>\n",
       "      <td>0.272091</td>\n",
       "      <td>0.710448</td>\n",
       "      <td>32:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092441</td>\n",
       "      <td>0.111157</td>\n",
       "      <td>0.163690</td>\n",
       "      <td>0.263591</td>\n",
       "      <td>0.717383</td>\n",
       "      <td>32:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.087232</td>\n",
       "      <td>0.110955</td>\n",
       "      <td>0.160811</td>\n",
       "      <td>0.261027</td>\n",
       "      <td>0.718037</td>\n",
       "      <td>32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.112318</td>\n",
       "      <td>0.160453</td>\n",
       "      <td>0.264056</td>\n",
       "      <td>0.714334</td>\n",
       "      <td>32:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6605860590934753.\n",
      "Better model found at epoch 1 with r_squared value: 0.6838511228561401.\n",
      "Better model found at epoch 2 with r_squared value: 0.6953175067901611.\n",
      "Better model found at epoch 3 with r_squared value: 0.6964060664176941.\n",
      "Better model found at epoch 4 with r_squared value: 0.7069191932678223.\n",
      "Better model found at epoch 5 with r_squared value: 0.7101508975028992.\n",
      "Better model found at epoch 6 with r_squared value: 0.7104479074478149.\n",
      "Better model found at epoch 7 with r_squared value: 0.7173829078674316.\n",
      "Better model found at epoch 8 with r_squared value: 0.7180368900299072.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-14:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-32:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-12:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-11:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-10:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leroy/conda/envs/torch2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-3, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [3/15 1:31:24&lt;6:05:36]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.131708</td>\n",
       "      <td>0.135052</td>\n",
       "      <td>0.186262</td>\n",
       "      <td>0.365141</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>30:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.126473</td>\n",
       "      <td>0.182601</td>\n",
       "      <td>0.318880</td>\n",
       "      <td>0.669278</td>\n",
       "      <td>30:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>0.120116</td>\n",
       "      <td>0.172366</td>\n",
       "      <td>0.304067</td>\n",
       "      <td>0.686703</td>\n",
       "      <td>30:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='25362' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.43% [25362/29687 34:39&lt;05:54 0.1160]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with r_squared value: 0.6361209154129028.\n",
      "Better model found at epoch 1 with r_squared value: 0.6692783236503601.\n",
      "Better model found at epoch 2 with r_squared value: 0.6867033243179321.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 8e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/mwt_v1.pth')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('mwt_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "165/(198*1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.229272</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.152555</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.727625</td>\n",
       "      <td>27:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210327</td>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.153173</td>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.729453</td>\n",
       "      <td>27:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210848</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.153384</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>27:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369781</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.153907</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.728009</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190333</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.154275</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>27:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.194347</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.723863</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.188494</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.155276</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.719586</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.184154</td>\n",
       "      <td>0.256038</td>\n",
       "      <td>0.155658</td>\n",
       "      <td>0.256038</td>\n",
       "      <td>0.719379</td>\n",
       "      <td>27:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.178666</td>\n",
       "      <td>0.258872</td>\n",
       "      <td>0.156046</td>\n",
       "      <td>0.258872</td>\n",
       "      <td>0.715114</td>\n",
       "      <td>27:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.168267</td>\n",
       "      <td>0.257935</td>\n",
       "      <td>0.155296</td>\n",
       "      <td>0.257935</td>\n",
       "      <td>0.716201</td>\n",
       "      <td>27:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.154889</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.713802</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.148415</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.154951</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.710612</td>\n",
       "      <td>27:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.145642</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.154936</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.708474</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.137470</td>\n",
       "      <td>0.265236</td>\n",
       "      <td>0.155330</td>\n",
       "      <td>0.265236</td>\n",
       "      <td>0.704923</td>\n",
       "      <td>27:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.266994</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>0.266994</td>\n",
       "      <td>0.702860</td>\n",
       "      <td>27:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.2600410580635071.\n",
      "Better model found at epoch 1 with valid_loss value: 0.25627413392066956.\n",
      "Better model found at epoch 2 with valid_loss value: 0.2527551054954529.\n",
      "Better model found at epoch 3 with valid_loss value: 0.2521514594554901.\n",
      "Better model found at epoch 4 with valid_loss value: 0.2521290183067322.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(15, 4e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.221799</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.161289</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.624436</td>\n",
       "      <td>19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.216447</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.160901</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.621538</td>\n",
       "      <td>19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211245</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.160596</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.617848</td>\n",
       "      <td>19:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221787</td>\n",
       "      <td>0.248711</td>\n",
       "      <td>0.160692</td>\n",
       "      <td>0.248711</td>\n",
       "      <td>0.620983</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.214866</td>\n",
       "      <td>0.249098</td>\n",
       "      <td>0.160391</td>\n",
       "      <td>0.249098</td>\n",
       "      <td>0.619240</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212531</td>\n",
       "      <td>0.249636</td>\n",
       "      <td>0.160608</td>\n",
       "      <td>0.249636</td>\n",
       "      <td>0.621107</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.210268</td>\n",
       "      <td>0.251274</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.251274</td>\n",
       "      <td>0.621262</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.205438</td>\n",
       "      <td>0.249965</td>\n",
       "      <td>0.160234</td>\n",
       "      <td>0.249965</td>\n",
       "      <td>0.612630</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211535</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.160285</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.616812</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.202592</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>0.160388</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>0.620815</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.203163</td>\n",
       "      <td>0.251837</td>\n",
       "      <td>0.160533</td>\n",
       "      <td>0.251837</td>\n",
       "      <td>0.622562</td>\n",
       "      <td>19:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.202210</td>\n",
       "      <td>0.251259</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.251259</td>\n",
       "      <td>0.623116</td>\n",
       "      <td>19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.203359</td>\n",
       "      <td>0.254081</td>\n",
       "      <td>0.160537</td>\n",
       "      <td>0.254081</td>\n",
       "      <td>0.626861</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.202191</td>\n",
       "      <td>0.252784</td>\n",
       "      <td>0.160110</td>\n",
       "      <td>0.252784</td>\n",
       "      <td>0.620404</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.253724</td>\n",
       "      <td>0.160042</td>\n",
       "      <td>0.253724</td>\n",
       "      <td>0.620057</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.195313</td>\n",
       "      <td>0.253773</td>\n",
       "      <td>0.160025</td>\n",
       "      <td>0.253773</td>\n",
       "      <td>0.622633</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.201135</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.159798</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.620017</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.200917</td>\n",
       "      <td>0.253857</td>\n",
       "      <td>0.159699</td>\n",
       "      <td>0.253857</td>\n",
       "      <td>0.619852</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.200810</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.159684</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.620227</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.200294</td>\n",
       "      <td>0.254061</td>\n",
       "      <td>0.159705</td>\n",
       "      <td>0.254061</td>\n",
       "      <td>0.620749</td>\n",
       "      <td>19:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.24888284504413605.\n",
      "Better model found at epoch 3 with valid_loss value: 0.24871104955673218.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(20, 8e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/good_v3.pth')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('good_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.232228</td>\n",
       "      <td>0.257023</td>\n",
       "      <td>0.163198</td>\n",
       "      <td>0.257023</td>\n",
       "      <td>0.605320</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>0.162589</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>0.616660</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219839</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.161875</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.611379</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.225076</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.161634</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0.161465</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0.613677</td>\n",
       "      <td>19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.248704</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.248704</td>\n",
       "      <td>0.610308</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.300959</td>\n",
       "      <td>0.248664</td>\n",
       "      <td>0.160695</td>\n",
       "      <td>0.248664</td>\n",
       "      <td>0.615366</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.219857</td>\n",
       "      <td>0.248102</td>\n",
       "      <td>0.160497</td>\n",
       "      <td>0.248102</td>\n",
       "      <td>0.617658</td>\n",
       "      <td>19:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.224552</td>\n",
       "      <td>0.248065</td>\n",
       "      <td>0.160396</td>\n",
       "      <td>0.248065</td>\n",
       "      <td>0.619523</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.213481</td>\n",
       "      <td>0.248097</td>\n",
       "      <td>0.160325</td>\n",
       "      <td>0.248097</td>\n",
       "      <td>0.618928</td>\n",
       "      <td>19:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.2570227384567261.\n",
      "Better model found at epoch 1 with valid_loss value: 0.2529500126838684.\n",
      "Better model found at epoch 2 with valid_loss value: 0.2496650665998459.\n",
      "Better model found at epoch 3 with valid_loss value: 0.24904529750347137.\n",
      "Better model found at epoch 5 with valid_loss value: 0.24870361387729645.\n",
      "Better model found at epoch 6 with valid_loss value: 0.2486642599105835.\n",
      "Better model found at epoch 7 with valid_loss value: 0.248102068901062.\n",
      "Better model found at epoch 8 with valid_loss value: 0.24806547164916992.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(10, 1e-4, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_weights_good_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.505111813545227.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [1.6002970933914185,0.505111813545227,1.6571693420410156,0.21333633206727462]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='4' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [4/40 1:47:04&lt;16:03:41]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.326428</td>\n",
       "      <td>0.325009</td>\n",
       "      <td>0.234470</td>\n",
       "      <td>0.355308</td>\n",
       "      <td>0.493184</td>\n",
       "      <td>25:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266216</td>\n",
       "      <td>0.291889</td>\n",
       "      <td>0.213740</td>\n",
       "      <td>0.315320</td>\n",
       "      <td>0.501836</td>\n",
       "      <td>26:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262931</td>\n",
       "      <td>0.283098</td>\n",
       "      <td>0.208533</td>\n",
       "      <td>0.304048</td>\n",
       "      <td>0.549058</td>\n",
       "      <td>27:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256095</td>\n",
       "      <td>0.275646</td>\n",
       "      <td>0.202871</td>\n",
       "      <td>0.295436</td>\n",
       "      <td>0.575205</td>\n",
       "      <td>27:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='3140' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.58% [3140/29687 02:52&lt;24:14 0.2453]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-25 23:31:58,521] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/tmp/ipykernel_23299/3148654658.py:112)\n",
      "   reasons:  ___check_obj_id(self, 140033784157216)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.32500866055488586.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-25 23:32:20,991] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'forward' (/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torchvision/ops/misc.py:319)\n",
      "   reasons:  ___check_obj_id(self, 140033784156448)\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 1 with valid_loss value: 0.2918890714645386.\n",
      "Better model found at epoch 2 with valid_loss value: 0.28309792280197144.\n",
      "Better model found at epoch 3 with valid_loss value: 0.2756464183330536.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(20, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164413</td>\n",
       "      <td>0.165433</td>\n",
       "      <td>0.177333</td>\n",
       "      <td>0.341697</td>\n",
       "      <td>0.417671</td>\n",
       "      <td>07:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157905</td>\n",
       "      <td>0.160126</td>\n",
       "      <td>0.169911</td>\n",
       "      <td>0.326655</td>\n",
       "      <td>0.454772</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.157170</td>\n",
       "      <td>0.166138</td>\n",
       "      <td>0.316997</td>\n",
       "      <td>0.479607</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157877</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.163412</td>\n",
       "      <td>0.310182</td>\n",
       "      <td>0.490179</td>\n",
       "      <td>07:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153847</td>\n",
       "      <td>0.151915</td>\n",
       "      <td>0.159921</td>\n",
       "      <td>0.296685</td>\n",
       "      <td>0.539397</td>\n",
       "      <td>07:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145465</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.153717</td>\n",
       "      <td>0.277628</td>\n",
       "      <td>0.580840</td>\n",
       "      <td>07:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      16.67% [1/6 07:29&lt;37:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196049</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>0.172460</td>\n",
       "      <td>0.320301</td>\n",
       "      <td>0.470430</td>\n",
       "      <td>07:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/29687 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(6, 7e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/unet_v3_big.pth')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('unet_v3_big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='29687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/29687 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_flat_cos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/schedule.py:142\u001b[0m, in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m lr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    141\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr, lr, lr\u001b[38;5;241m/\u001b[39mdiv_final)}\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:201\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:560\u001b[0m, in \u001b[0;36mRecorder.after_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    559\u001b[0m mets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_mets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_mets\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m mets: \u001b[43mmet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:509\u001b[0m, in \u001b[0;36mAvgSmoothLoss.accumulate\u001b[0;34m(self, learn)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn):\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlerp(\u001b[43mto_detach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:244\u001b[0m, in \u001b[0;36mto_detach\u001b[0;34m(b, cpu, gather)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gather: x \u001b[38;5;241m=\u001b[39m maybe_gather(x)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m cpu \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:224\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_listy(x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x)([apply(func, o, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,\u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m {k: apply(func, v, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 224\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:243\u001b[0m, in \u001b[0;36mto_detach.<locals>._inner\u001b[0;34m(x, cpu, gather)\u001b[0m\n\u001b[1;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gather: x \u001b[38;5;241m=\u001b[39m maybe_gather(x)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cpu \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pl.read_csv('/mnt/ssd/kaggle/test.csv')\n",
    "test_df = pl.read_parquet('/mnt/ssd/kaggle/test.parquet')\n",
    "x_test = test_df.select(FEAT_COLS).to_numpy()\n",
    "\n",
    "test_ds = Loader({'x' : x_test, 'emb' : np.zeros(x_test.shape[0], dtype=int)}, {'x' : norm_x})\n",
    "test_loader = fv.DataLoader(test_ds, batch_size=bs, drop_last=False, shuffle=False, \n",
    "                            num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (625_000, 557)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_id</th><th>state_t_0</th><th>state_t_1</th><th>state_t_2</th><th>state_t_3</th><th>state_t_4</th><th>state_t_5</th><th>state_t_6</th><th>state_t_7</th><th>state_t_8</th><th>state_t_9</th><th>state_t_10</th><th>state_t_11</th><th>state_t_12</th><th>state_t_13</th><th>state_t_14</th><th>state_t_15</th><th>state_t_16</th><th>state_t_17</th><th>state_t_18</th><th>state_t_19</th><th>state_t_20</th><th>state_t_21</th><th>state_t_22</th><th>state_t_23</th><th>state_t_24</th><th>state_t_25</th><th>state_t_26</th><th>state_t_27</th><th>state_t_28</th><th>state_t_29</th><th>state_t_30</th><th>state_t_31</th><th>state_t_32</th><th>state_t_33</th><th>state_t_34</th><th>state_t_35</th><th>state_t_36</th><th>state_t_37</th><th>state_t_38</th><th>state_t_39</th><th>state_t_40</th><th>state_t_41</th><th>state_t_42</th><th>state_t_43</th><th>state_t_44</th><th>state_t_45</th><th>state_t_46</th><th>state_t_47</th><th>state_t_48</th><th>state_t_49</th><th>state_t_50</th><th>state_t_51</th><th>state_t_52</th><th>state_t_53</th><th>state_t_54</th><th>state_t_55</th><th>state_t_56</th><th>state_t_57</th><th>state_t_58</th><th>state_t_59</th><th>state_q0001_0</th><th>state_q0001_1</th><th>state_q0001_2</th><th>state_q0001_3</th><th>state_q0001_4</th><th>state_q0001_5</th><th>state_q0001_6</th><th>state_q0001_7</th><th>state_q0001_8</th><th>state_q0001_9</th><th>state_q0001_10</th><th>state_q0001_11</th><th>state_q0001_12</th><th>state_q0001_13</th><th>state_q0001_14</th><th>state_q0001_15</th><th>state_q0001_16</th><th>state_q0001_17</th><th>state_q0001_18</th><th>state_q0001_19</th><th>state_q0001_20</th><th>state_q0001_21</th><th>state_q0001_22</th><th>state_q0001_23</th><th>state_q0001_24</th><th>state_q0001_25</th><th>state_q0001_26</th><th>state_q0001_27</th><th>state_q0001_28</th><th>state_q0001_29</th><th>state_q0001_30</th><th>state_q0001_31</th><th>state_q0001_32</th><th>state_q0001_33</th><th>state_q0001_34</th><th>state_q0001_35</th><th>state_q0001_36</th><th>state_q0001_37</th><th>state_q0001_38</th><th>state_q0001_39</th><th>state_q0001_40</th><th>state_q0001_41</th><th>state_q0001_42</th><th>state_q0001_43</th><th>state_q0001_44</th><th>state_q0001_45</th><th>state_q0001_46</th><th>state_q0001_47</th><th>state_q0001_48</th><th>state_q0001_49</th><th>state_q0001_50</th><th>state_q0001_51</th><th>state_q0001_52</th><th>state_q0001_53</th><th>state_q0001_54</th><th>state_q0001_55</th><th>state_q0001_56</th><th>state_q0001_57</th><th>state_q0001_58</th><th>state_q0001_59</th><th>state_q0002_0</th><th>state_q0002_1</th><th>state_q0002_2</th><th>state_q0002_3</th><th>state_q0002_4</th><th>state_q0002_5</th><th>state_q0002_6</th><th>state_q0002_7</th><th>state_q0002_8</th><th>state_q0002_9</th><th>state_q0002_10</th><th>state_q0002_11</th><th>state_q0002_12</th><th>state_q0002_13</th><th>state_q0002_14</th><th>state_q0002_15</th><th>state_q0002_16</th><th>state_q0002_17</th><th>state_q0002_18</th><th>state_q0002_19</th><th>state_q0002_20</th><th>state_q0002_21</th><th>state_q0002_22</th><th>state_q0002_23</th><th>state_q0002_24</th><th>state_q0002_25</th><th>state_q0002_26</th><th>state_q0002_27</th><th>state_q0002_28</th><th>state_q0002_29</th><th>state_q0002_30</th><th>state_q0002_31</th><th>state_q0002_32</th><th>state_q0002_33</th><th>state_q0002_34</th><th>state_q0002_35</th><th>state_q0002_36</th><th>state_q0002_37</th><th>state_q0002_38</th><th>state_q0002_39</th><th>state_q0002_40</th><th>state_q0002_41</th><th>state_q0002_42</th><th>state_q0002_43</th><th>state_q0002_44</th><th>state_q0002_45</th><th>state_q0002_46</th><th>state_q0002_47</th><th>state_q0002_48</th><th>state_q0002_49</th><th>state_q0002_50</th><th>state_q0002_51</th><th>state_q0002_52</th><th>state_q0002_53</th><th>state_q0002_54</th><th>state_q0002_55</th><th>state_q0002_56</th><th>state_q0002_57</th><th>state_q0002_58</th><th>state_q0002_59</th><th>state_q0003_0</th><th>state_q0003_1</th><th>state_q0003_2</th><th>state_q0003_3</th><th>state_q0003_4</th><th>state_q0003_5</th><th>state_q0003_6</th><th>state_q0003_7</th><th>state_q0003_8</th><th>state_q0003_9</th><th>state_q0003_10</th><th>state_q0003_11</th><th>state_q0003_12</th><th>state_q0003_13</th><th>state_q0003_14</th><th>state_q0003_15</th><th>state_q0003_16</th><th>state_q0003_17</th><th>state_q0003_18</th><th>state_q0003_19</th><th>state_q0003_20</th><th>state_q0003_21</th><th>state_q0003_22</th><th>state_q0003_23</th><th>state_q0003_24</th><th>state_q0003_25</th><th>state_q0003_26</th><th>state_q0003_27</th><th>state_q0003_28</th><th>state_q0003_29</th><th>state_q0003_30</th><th>state_q0003_31</th><th>state_q0003_32</th><th>state_q0003_33</th><th>state_q0003_34</th><th>state_q0003_35</th><th>state_q0003_36</th><th>state_q0003_37</th><th>state_q0003_38</th><th>state_q0003_39</th><th>state_q0003_40</th><th>state_q0003_41</th><th>state_q0003_42</th><th>state_q0003_43</th><th>state_q0003_44</th><th>state_q0003_45</th><th>state_q0003_46</th><th>state_q0003_47</th><th>state_q0003_48</th><th>state_q0003_49</th><th>state_q0003_50</th><th>state_q0003_51</th><th>state_q0003_52</th><th>state_q0003_53</th><th>state_q0003_54</th><th>state_q0003_55</th><th>state_q0003_56</th><th>state_q0003_57</th><th>state_q0003_58</th><th>state_q0003_59</th><th>state_u_0</th><th>state_u_1</th><th>state_u_2</th><th>state_u_3</th><th>state_u_4</th><th>state_u_5</th><th>state_u_6</th><th>state_u_7</th><th>state_u_8</th><th>state_u_9</th><th>state_u_10</th><th>state_u_11</th><th>state_u_12</th><th>state_u_13</th><th>state_u_14</th><th>state_u_15</th><th>state_u_16</th><th>state_u_17</th><th>state_u_18</th><th>state_u_19</th><th>state_u_20</th><th>state_u_21</th><th>state_u_22</th><th>state_u_23</th><th>state_u_24</th><th>state_u_25</th><th>state_u_26</th><th>state_u_27</th><th>state_u_28</th><th>state_u_29</th><th>state_u_30</th><th>state_u_31</th><th>state_u_32</th><th>state_u_33</th><th>state_u_34</th><th>state_u_35</th><th>state_u_36</th><th>state_u_37</th><th>state_u_38</th><th>state_u_39</th><th>state_u_40</th><th>state_u_41</th><th>state_u_42</th><th>state_u_43</th><th>state_u_44</th><th>state_u_45</th><th>state_u_46</th><th>state_u_47</th><th>state_u_48</th><th>state_u_49</th><th>state_u_50</th><th>state_u_51</th><th>state_u_52</th><th>state_u_53</th><th>state_u_54</th><th>state_u_55</th><th>state_u_56</th><th>state_u_57</th><th>state_u_58</th><th>state_u_59</th><th>state_v_0</th><th>state_v_1</th><th>state_v_2</th><th>state_v_3</th><th>state_v_4</th><th>state_v_5</th><th>state_v_6</th><th>state_v_7</th><th>state_v_8</th><th>state_v_9</th><th>state_v_10</th><th>state_v_11</th><th>state_v_12</th><th>state_v_13</th><th>state_v_14</th><th>state_v_15</th><th>state_v_16</th><th>state_v_17</th><th>state_v_18</th><th>state_v_19</th><th>state_v_20</th><th>state_v_21</th><th>state_v_22</th><th>state_v_23</th><th>state_v_24</th><th>state_v_25</th><th>state_v_26</th><th>state_v_27</th><th>state_v_28</th><th>state_v_29</th><th>state_v_30</th><th>state_v_31</th><th>state_v_32</th><th>state_v_33</th><th>state_v_34</th><th>state_v_35</th><th>state_v_36</th><th>state_v_37</th><th>state_v_38</th><th>state_v_39</th><th>state_v_40</th><th>state_v_41</th><th>state_v_42</th><th>state_v_43</th><th>state_v_44</th><th>state_v_45</th><th>state_v_46</th><th>state_v_47</th><th>state_v_48</th><th>state_v_49</th><th>state_v_50</th><th>state_v_51</th><th>state_v_52</th><th>state_v_53</th><th>state_v_54</th><th>state_v_55</th><th>state_v_56</th><th>state_v_57</th><th>state_v_58</th><th>state_v_59</th><th>state_ps</th><th>pbuf_SOLIN</th><th>pbuf_LHFLX</th><th>pbuf_SHFLX</th><th>pbuf_TAUX</th><th>pbuf_TAUY</th><th>pbuf_COSZRS</th><th>cam_in_ALDIF</th><th>cam_in_ALDIR</th><th>cam_in_ASDIF</th><th>cam_in_ASDIR</th><th>cam_in_LWUP</th><th>cam_in_ICEFRAC</th><th>cam_in_LANDFRAC</th><th>cam_in_OCNFRAC</th><th>cam_in_SNOWHLAND</th><th>pbuf_ozone_0</th><th>pbuf_ozone_1</th><th>pbuf_ozone_2</th><th>pbuf_ozone_3</th><th>pbuf_ozone_4</th><th>pbuf_ozone_5</th><th>pbuf_ozone_6</th><th>pbuf_ozone_7</th><th>pbuf_ozone_8</th><th>pbuf_ozone_9</th><th>pbuf_ozone_10</th><th>pbuf_ozone_11</th><th>pbuf_ozone_12</th><th>pbuf_ozone_13</th><th>pbuf_ozone_14</th><th>pbuf_ozone_15</th><th>pbuf_ozone_16</th><th>pbuf_ozone_17</th><th>pbuf_ozone_18</th><th>pbuf_ozone_19</th><th>pbuf_ozone_20</th><th>pbuf_ozone_21</th><th>pbuf_ozone_22</th><th>pbuf_ozone_23</th><th>pbuf_ozone_24</th><th>pbuf_ozone_25</th><th>pbuf_ozone_26</th><th>pbuf_ozone_27</th><th>pbuf_ozone_28</th><th>pbuf_ozone_29</th><th>pbuf_ozone_30</th><th>pbuf_ozone_31</th><th>pbuf_ozone_32</th><th>pbuf_ozone_33</th><th>pbuf_ozone_34</th><th>pbuf_ozone_35</th><th>pbuf_ozone_36</th><th>pbuf_ozone_37</th><th>pbuf_ozone_38</th><th>pbuf_ozone_39</th><th>pbuf_ozone_40</th><th>pbuf_ozone_41</th><th>pbuf_ozone_42</th><th>pbuf_ozone_43</th><th>pbuf_ozone_44</th><th>pbuf_ozone_45</th><th>pbuf_ozone_46</th><th>pbuf_ozone_47</th><th>pbuf_ozone_48</th><th>pbuf_ozone_49</th><th>pbuf_ozone_50</th><th>pbuf_ozone_51</th><th>pbuf_ozone_52</th><th>pbuf_ozone_53</th><th>pbuf_ozone_54</th><th>pbuf_ozone_55</th><th>pbuf_ozone_56</th><th>pbuf_ozone_57</th><th>pbuf_ozone_58</th><th>pbuf_ozone_59</th><th>pbuf_CH4_0</th><th>pbuf_CH4_1</th><th>pbuf_CH4_2</th><th>pbuf_CH4_3</th><th>pbuf_CH4_4</th><th>pbuf_CH4_5</th><th>pbuf_CH4_6</th><th>pbuf_CH4_7</th><th>pbuf_CH4_8</th><th>pbuf_CH4_9</th><th>pbuf_CH4_10</th><th>pbuf_CH4_11</th><th>pbuf_CH4_12</th><th>pbuf_CH4_13</th><th>pbuf_CH4_14</th><th>pbuf_CH4_15</th><th>pbuf_CH4_16</th><th>pbuf_CH4_17</th><th>pbuf_CH4_18</th><th>pbuf_CH4_19</th><th>pbuf_CH4_20</th><th>pbuf_CH4_21</th><th>pbuf_CH4_22</th><th>pbuf_CH4_23</th><th>pbuf_CH4_24</th><th>pbuf_CH4_25</th><th>pbuf_CH4_26</th><th>pbuf_CH4_27</th><th>pbuf_CH4_28</th><th>pbuf_CH4_29</th><th>pbuf_CH4_30</th><th>pbuf_CH4_31</th><th>pbuf_CH4_32</th><th>pbuf_CH4_33</th><th>pbuf_CH4_34</th><th>pbuf_CH4_35</th><th>pbuf_CH4_36</th><th>pbuf_CH4_37</th><th>pbuf_CH4_38</th><th>pbuf_CH4_39</th><th>pbuf_CH4_40</th><th>pbuf_CH4_41</th><th>pbuf_CH4_42</th><th>pbuf_CH4_43</th><th>pbuf_CH4_44</th><th>pbuf_CH4_45</th><th>pbuf_CH4_46</th><th>pbuf_CH4_47</th><th>pbuf_CH4_48</th><th>pbuf_CH4_49</th><th>pbuf_CH4_50</th><th>pbuf_CH4_51</th><th>pbuf_CH4_52</th><th>pbuf_CH4_53</th><th>pbuf_CH4_54</th><th>pbuf_CH4_55</th><th>pbuf_CH4_56</th><th>pbuf_CH4_57</th><th>pbuf_CH4_58</th><th>pbuf_CH4_59</th><th>pbuf_N2O_0</th><th>pbuf_N2O_1</th><th>pbuf_N2O_2</th><th>pbuf_N2O_3</th><th>pbuf_N2O_4</th><th>pbuf_N2O_5</th><th>pbuf_N2O_6</th><th>pbuf_N2O_7</th><th>pbuf_N2O_8</th><th>pbuf_N2O_9</th><th>pbuf_N2O_10</th><th>pbuf_N2O_11</th><th>pbuf_N2O_12</th><th>pbuf_N2O_13</th><th>pbuf_N2O_14</th><th>pbuf_N2O_15</th><th>pbuf_N2O_16</th><th>pbuf_N2O_17</th><th>pbuf_N2O_18</th><th>pbuf_N2O_19</th><th>pbuf_N2O_20</th><th>pbuf_N2O_21</th><th>pbuf_N2O_22</th><th>pbuf_N2O_23</th><th>pbuf_N2O_24</th><th>pbuf_N2O_25</th><th>pbuf_N2O_26</th><th>pbuf_N2O_27</th><th>pbuf_N2O_28</th><th>pbuf_N2O_29</th><th>pbuf_N2O_30</th><th>pbuf_N2O_31</th><th>pbuf_N2O_32</th><th>pbuf_N2O_33</th><th>pbuf_N2O_34</th><th>pbuf_N2O_35</th><th>pbuf_N2O_36</th><th>pbuf_N2O_37</th><th>pbuf_N2O_38</th><th>pbuf_N2O_39</th><th>pbuf_N2O_40</th><th>pbuf_N2O_41</th><th>pbuf_N2O_42</th><th>pbuf_N2O_43</th><th>pbuf_N2O_44</th><th>pbuf_N2O_45</th><th>pbuf_N2O_46</th><th>pbuf_N2O_47</th><th>pbuf_N2O_48</th><th>pbuf_N2O_49</th><th>pbuf_N2O_50</th><th>pbuf_N2O_51</th><th>pbuf_N2O_52</th><th>pbuf_N2O_53</th><th>pbuf_N2O_54</th><th>pbuf_N2O_55</th><th>pbuf_N2O_56</th><th>pbuf_N2O_57</th><th>pbuf_N2O_58</th><th>pbuf_N2O_59</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;test_169651&quot;</td><td>209.802593</td><td>220.698213</td><td>227.783289</td><td>241.386812</td><td>254.602962</td><td>262.319063</td><td>261.308368</td><td>254.062035</td><td>243.90442</td><td>236.302401</td><td>229.972123</td><td>225.227444</td><td>221.043785</td><td>215.828104</td><td>208.998899</td><td>203.059782</td><td>199.041293</td><td>198.233715</td><td>196.024335</td><td>198.165421</td><td>201.194742</td><td>205.367505</td><td>209.875699</td><td>214.748964</td><td>219.542131</td><td>224.305865</td><td>228.952891</td><td>233.365752</td><td>237.65552</td><td>241.956551</td><td>246.022055</td><td>249.808255</td><td>253.459608</td><td>256.971278</td><td>260.294397</td><td>263.400258</td><td>266.453995</td><td>269.335235</td><td>271.931611</td><td>274.345917</td><td>276.364867</td><td>278.199151</td><td>279.774969</td><td>281.114457</td><td>282.216242</td><td>283.134861</td><td>284.583856</td><td>285.660463</td><td>286.682694</td><td>287.647368</td><td>288.319392</td><td>288.954874</td><td>289.686522</td><td>290.47467</td><td>291.374321</td><td>292.347968</td><td>293.43973</td><td>294.438724</td><td>295.38002</td><td>296.391446</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000011</td><td>0.00002</td><td>0.000032</td><td>0.000042</td><td>0.000059</td><td>0.000091</td><td>0.000152</td><td>0.000257</td><td>0.000398</td><td>0.000575</td><td>0.000798</td><td>0.001029</td><td>0.001247</td><td>0.001508</td><td>0.001851</td><td>0.002217</td><td>0.002593</td><td>0.003069</td><td>0.003637</td><td>0.004323</td><td>0.004914</td><td>0.005655</td><td>0.006382</td><td>0.007161</td><td>0.00809</td><td>0.008422</td><td>0.009062</td><td>0.009533</td><td>0.010061</td><td>0.010851</td><td>0.011723</td><td>0.01231</td><td>0.012901</td><td>0.013413</td><td>0.013863</td><td>0.014226</td><td>0.014691</td><td>0.01539</td><td>0.015926</td><td>9.7013e-37</td><td>9.7067e-37</td><td>9.5320e-37</td><td>9.0000e-37</td><td>8.8489e-37</td><td>8.7347e-37</td><td>7.4539e-37</td><td>5.7589e-37</td><td>4.1264e-37</td><td>2.5798e-37</td><td>9.0800e-38</td><td>8.8615e-39</td><td>3.9948e-43</td><td>5.2197e-49</td><td>3.8237e-55</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.2248e-49</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.3843e-31</td><td>8.4492e-27</td><td>1.0226e-28</td><td>2.3281e-18</td><td>5.3019e-20</td><td>0.0</td><td>1.7424e-7</td><td>1.9801e-7</td><td>5.1508e-7</td><td>0.0</td><td>3.3095e-7</td><td>5.4720e-7</td><td>5.0558e-7</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.000004</td><td>0.000001</td><td>0.000004</td><td>0.000003</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000003</td><td>0.000007</td><td>0.000005</td><td>0.00001</td><td>0.00001</td><td>0.000004</td><td>2.4976e-7</td><td>7.4434e-7</td><td>2.4470e-7</td><td>7.5538e-9</td><td>3.2430e-11</td><td>3.5474e-11</td><td>3.9380e-11</td><td>4.7086e-11</td><td>5.1482e-11</td><td>5.5466e-11</td><td>5.5631e-11</td><td>5.0086e-11</td><td>4.1181e-11</td><td>3.0849e-11</td><td>1.2191e-11</td><td>1.2796e-12</td><td>5.5664e-17</td><td>8.9162e-23</td><td>2.3687e-24</td><td>0.0</td><td>1.6279e-24</td><td>5.6999e-20</td><td>8.7989e-17</td><td>3.3620e-11</td><td>1.6776e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.000002</td><td>0.000001</td><td>0.000002</td><td>5.0476e-7</td><td>1.7979e-7</td><td>6.1143e-7</td><td>9.7138e-8</td><td>3.2653e-7</td><td>2.4650e-7</td><td>7.7237e-8</td><td>1.6265e-8</td><td>2.6423e-8</td><td>5.0995e-8</td><td>5.2668e-8</td><td>3.7720e-8</td><td>7.3597e-8</td><td>5.2720e-8</td><td>3.7769e-8</td><td>3.7740e-9</td><td>2.3163e-9</td><td>2.7407e-10</td><td>6.0542e-11</td><td>1.4243e-11</td><td>4.6001e-12</td><td>0.0</td><td>3.0952e-13</td><td>1.9876e-13</td><td>2.0088e-13</td><td>1.2196e-14</td><td>1.2907e-12</td><td>-12.621098</td><td>-35.264994</td><td>-39.288286</td><td>-26.101457</td><td>-27.027251</td><td>-26.977124</td><td>-25.681033</td><td>-24.116904</td><td>-21.753854</td><td>-18.174458</td><td>-13.102039</td><td>-7.057726</td><td>-2.783299</td><td>0.48277</td><td>6.097161</td><td>9.94475</td><td>14.382375</td><td>20.15391</td><td>24.696414</td><td>28.10628</td><td>31.409991</td><td>34.470184</td><td>35.623454</td><td>35.262472</td><td>33.688433</td><td>31.477991</td><td>29.394856</td><td>27.362967</td><td>25.418579</td><td>23.586494</td><td>21.805095</td><td>20.05106</td><td>18.329192</td><td>16.628743</td><td>15.029511</td><td>13.527132</td><td>12.198934</td><td>11.006181</td><td>9.946034</td><td>8.986405</td><td>8.046804</td><td>7.155062</td><td>6.298709</td><td>5.470431</td><td>4.767571</td><td>4.192104</td><td>3.654444</td><td>3.092111</td><td>2.546125</td><td>2.053815</td><td>1.582154</td><td>1.125412</td><td>0.672671</td><td>0.210809</td><td>-0.256035</td><td>-0.681038</td><td>-1.069693</td><td>-1.349754</td><td>-1.312553</td><td>-1.116497</td><td>11.938681</td><td>6.037094</td><td>11.908263</td><td>2.385834</td><td>-3.281934</td><td>0.980046</td><td>5.255962</td><td>5.241453</td><td>1.689835</td><td>0.214683</td><td>1.039891</td><td>1.317828</td><td>0.97116</td><td>3.472131</td><td>4.706899</td><td>2.93737</td><td>3.055634</td><td>2.977136</td><td>2.058352</td><td>1.935215</td><td>2.134694</td><td>2.024551</td><td>1.55467</td><td>1.581369</td><td>1.578897</td><td>1.384252</td><td>1.128416</td><td>0.848605</td><td>0.540527</td><td>0.349101</td><td>0.37602</td><td>0.567414</td><td>0.844342</td><td>1.054674</td><td>1.090906</td><td>0.989244</td><td>0.73123</td><td>0.475887</td><td>0.305124</td><td>0.26951</td><td>0.256586</td><td>0.27095</td><td>0.218726</td><td>0.102173</td><td>0.021138</td><td>0.068425</td><td>0.137398</td><td>0.18148</td><td>0.228599</td><td>0.281241</td><td>0.318975</td><td>0.346498</td><td>0.386231</td><td>0.437631</td><td>0.492904</td><td>0.525097</td><td>0.519203</td><td>0.527845</td><td>0.526496</td><td>0.169698</td><td>100600.027964</td><td>0.0</td><td>4.347225</td><td>-0.246807</td><td>0.001545</td><td>-0.0006</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>438.758554</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6356e-7</td><td>4.7469e-7</td><td>8.5083e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000013</td><td>0.000014</td><td>0.000013</td><td>0.000012</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>8.4424e-7</td><td>4.8861e-7</td><td>3.3337e-7</td><td>2.4476e-7</td><td>1.8146e-7</td><td>1.3446e-7</td><td>1.0342e-7</td><td>8.9834e-8</td><td>8.0409e-8</td><td>7.4827e-8</td><td>7.1977e-8</td><td>6.9728e-8</td><td>6.8498e-8</td><td>6.7445e-8</td><td>6.6510e-8</td><td>6.5643e-8</td><td>6.4768e-8</td><td>6.3963e-8</td><td>6.3137e-8</td><td>6.2009e-8</td><td>6.0600e-8</td><td>5.9198e-8</td><td>5.7647e-8</td><td>5.5942e-8</td><td>5.4316e-8</td><td>5.2773e-8</td><td>5.0042e-8</td><td>4.7254e-8</td><td>4.4604e-8</td><td>4.2089e-8</td><td>4.0019e-8</td><td>3.8186e-8</td><td>3.6475e-8</td><td>3.4873e-8</td><td>3.3392e-8</td><td>3.2752e-8</td><td>3.2130e-8</td><td>3.1515e-8</td><td>3.0900e-8</td><td>3.0344e-8</td><td>2.9831e-8</td><td>2.9312e-8</td><td>2.8908e-8</td><td>1.7047e-7</td><td>1.9578e-7</td><td>2.2460e-7</td><td>2.5711e-7</td><td>2.9331e-7</td><td>3.3293e-7</td><td>3.7542e-7</td><td>4.2000e-7</td><td>4.6582e-7</td><td>5.1216e-7</td><td>5.5852e-7</td><td>6.0466e-7</td><td>6.5047e-7</td><td>6.9585e-7</td><td>7.4055e-7</td><td>7.8419e-7</td><td>8.2638e-7</td><td>8.6682e-7</td><td>9.0540e-7</td><td>9.4221e-7</td><td>9.7737e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.7090e-8</td><td>3.3989e-8</td><td>4.2566e-8</td><td>5.3122e-8</td><td>6.5920e-8</td><td>8.1133e-8</td><td>9.8783e-8</td><td>1.1873e-7</td><td>1.4068e-7</td><td>1.6434e-7</td><td>1.8941e-7</td><td>2.1572e-7</td><td>2.4315e-7</td><td>2.7157e-7</td><td>3.0073e-7</td><td>3.3032e-7</td><td>3.5994e-7</td><td>3.8925e-7</td><td>4.1805e-7</td><td>4.4625e-7</td><td>4.7387e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_524862&quot;</td><td>208.366101</td><td>219.238711</td><td>228.265017</td><td>242.099772</td><td>256.16427</td><td>263.783211</td><td>261.835384</td><td>253.636172</td><td>243.783467</td><td>236.961625</td><td>231.254018</td><td>226.821328</td><td>223.089416</td><td>218.781736</td><td>213.044729</td><td>207.428738</td><td>203.258569</td><td>200.945988</td><td>198.008377</td><td>200.708751</td><td>202.260369</td><td>205.759887</td><td>209.204746</td><td>213.188264</td><td>217.391038</td><td>221.709</td><td>226.124449</td><td>230.24439</td><td>234.354927</td><td>238.466732</td><td>242.440843</td><td>246.280318</td><td>249.962632</td><td>253.491784</td><td>256.890607</td><td>260.038062</td><td>263.105183</td><td>265.969345</td><td>268.651067</td><td>271.115895</td><td>273.156304</td><td>275.05413</td><td>276.859299</td><td>278.323683</td><td>279.625526</td><td>280.895406</td><td>281.976503</td><td>283.153419</td><td>284.207563</td><td>285.304527</td><td>286.422891</td><td>287.165373</td><td>287.948619</td><td>288.710265</td><td>289.541786</td><td>290.338582</td><td>291.228917</td><td>292.075051</td><td>292.772405</td><td>293.552796</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000019</td><td>0.000025</td><td>0.000032</td><td>0.000045</td><td>0.000075</td><td>0.000142</td><td>0.000244</td><td>0.000356</td><td>0.000484</td><td>0.000632</td><td>0.000844</td><td>0.001119</td><td>0.001435</td><td>0.001796</td><td>0.002197</td><td>0.002635</td><td>0.003162</td><td>0.003662</td><td>0.004397</td><td>0.004992</td><td>0.005598</td><td>0.006273</td><td>0.006898</td><td>0.007266</td><td>0.007811</td><td>0.008264</td><td>0.008604</td><td>0.008909</td><td>0.009354</td><td>0.009941</td><td>0.010509</td><td>0.010907</td><td>0.011343</td><td>0.011847</td><td>0.0123</td><td>0.012884</td><td>0.013599</td><td>0.014058</td><td>9.7233e-37</td><td>9.6757e-37</td><td>9.5398e-37</td><td>9.3217e-37</td><td>9.4038e-37</td><td>9.2493e-37</td><td>8.1177e-37</td><td>6.6361e-37</td><td>5.3593e-37</td><td>3.9227e-37</td><td>1.4220e-37</td><td>1.5365e-38</td><td>2.9856e-43</td><td>4.8280e-51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.4922e-40</td><td>3.4044e-35</td><td>1.2520e-30</td><td>0.0</td><td>6.3843e-31</td><td>8.4492e-27</td><td>1.0226e-28</td><td>2.3281e-18</td><td>5.3019e-20</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.3582e-7</td><td>7.9312e-8</td><td>1.3835e-7</td><td>0.000002</td><td>0.000012</td><td>0.000037</td><td>0.00002</td><td>0.000014</td><td>0.000033</td><td>0.000027</td><td>0.000029</td><td>0.000005</td><td>0.000007</td><td>0.000022</td><td>0.000028</td><td>0.000015</td><td>0.000013</td><td>0.000007</td><td>0.000002</td><td>0.000001</td><td>0.000003</td><td>1.6578e-8</td><td>3.2606e-11</td><td>3.4582e-11</td><td>3.9890e-11</td><td>4.5706e-11</td><td>5.0204e-11</td><td>5.5300e-11</td><td>5.7235e-11</td><td>5.4381e-11</td><td>4.8958e-11</td><td>4.1832e-11</td><td>1.8482e-11</td><td>2.3615e-12</td><td>5.3417e-17</td><td>2.3347e-25</td><td>7.4363e-25</td><td>3.2001e-20</td><td>1.6279e-24</td><td>2.5878e-14</td><td>2.4028e-12</td><td>1.5160e-10</td><td>3.5728e-9</td><td>1.5177e-8</td><td>6.6076e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.1687e-7</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.6779e-10</td><td>1.9253e-8</td><td>1.0758e-7</td><td>1.4760e-7</td><td>2.1158e-7</td><td>2.1672e-7</td><td>2.4495e-7</td><td>1.4108e-7</td><td>2.6630e-7</td><td>6.8928e-7</td><td>7.9143e-8</td><td>4.7101e-8</td><td>4.8511e-8</td><td>3.8275e-9</td><td>2.0314e-8</td><td>1.9337e-9</td><td>6.0377e-10</td><td>5.5262e-11</td><td>7.8513e-12</td><td>8.6035e-10</td><td>2.1053e-12</td><td>1.1164e-12</td><td>1.0425e-12</td><td>7.0639e-14</td><td>9.8122e-13</td><td>-29.893361</td><td>-39.554844</td><td>-34.522524</td><td>-21.928289</td><td>-18.223787</td><td>-19.48342</td><td>-19.543606</td><td>-18.398179</td><td>-15.392368</td><td>-12.175464</td><td>-8.241326</td><td>-3.019894</td><td>2.268972</td><td>5.34779</td><td>8.920504</td><td>14.489345</td><td>18.426973</td><td>22.032316</td><td>25.706922</td><td>28.965752</td><td>31.16677</td><td>32.408526</td><td>32.771722</td><td>32.556432</td><td>31.808454</td><td>30.882596</td><td>29.846462</td><td>28.652634</td><td>27.30233</td><td>25.846441</td><td>24.259796</td><td>22.670197</td><td>21.183674</td><td>19.844425</td><td>18.700619</td><td>17.666287</td><td>16.707244</td><td>15.681082</td><td>14.583848</td><td>13.460464</td><td>12.467512</td><td>11.563934</td><td>10.787628</td><td>10.096707</td><td>9.47082</td><td>8.897509</td><td>8.334815</td><td>7.766036</td><td>7.249928</td><td>6.811481</td><td>6.43075</td><td>6.074011</td><td>5.748827</td><td>5.444443</td><td>5.16472</td><td>4.916659</td><td>4.68944</td><td>4.454772</td><td>4.332837</td><td>3.568685</td><td>3.407441</td><td>7.927505</td><td>14.541661</td><td>7.870468</td><td>0.851383</td><td>2.700654</td><td>5.313124</td><td>4.424651</td><td>0.94704</td><td>-1.047635</td><td>-0.371438</td><td>0.849279</td><td>0.934926</td><td>1.496484</td><td>4.686353</td><td>5.375017</td><td>5.183725</td><td>6.110477</td><td>7.373743</td><td>8.273774</td><td>8.783596</td><td>9.070392</td><td>8.906493</td><td>8.417356</td><td>7.664339</td><td>6.864443</td><td>6.386836</td><td>5.944948</td><td>5.524617</td><td>5.08944</td><td>4.663317</td><td>4.18785</td><td>3.588459</td><td>2.90988</td><td>2.254511</td><td>1.656692</td><td>1.202701</td><td>0.945096</td><td>0.816315</td><td>0.691156</td><td>0.492947</td><td>0.210738</td><td>-0.034511</td><td>-0.223913</td><td>-0.30456</td><td>-0.306024</td><td>-0.31283</td><td>-0.318147</td><td>-0.329014</td><td>-0.361018</td><td>-0.399833</td><td>-0.439263</td><td>-0.485964</td><td>-0.515237</td><td>-0.548946</td><td>-0.609114</td><td>-0.656643</td><td>-0.768443</td><td>-1.045759</td><td>-1.761068</td><td>101156.338133</td><td>0.0</td><td>4.390058</td><td>-1.169037</td><td>-0.011148</td><td>0.005213</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>420.971275</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6128e-7</td><td>4.7059e-7</td><td>8.4348e-7</td><td>0.000001</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000012</td><td>0.000013</td><td>0.000014</td><td>0.000013</td><td>0.000012</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>6.0761e-7</td><td>4.2808e-7</td><td>3.2513e-7</td><td>2.4502e-7</td><td>1.7848e-7</td><td>1.2772e-7</td><td>1.0575e-7</td><td>9.0547e-8</td><td>8.1658e-8</td><td>7.6913e-8</td><td>7.3041e-8</td><td>7.0865e-8</td><td>6.8947e-8</td><td>6.7412e-8</td><td>6.6084e-8</td><td>6.4747e-8</td><td>6.3495e-8</td><td>6.2212e-8</td><td>6.0667e-8</td><td>5.8916e-8</td><td>5.7175e-8</td><td>5.5445e-8</td><td>5.3750e-8</td><td>5.2134e-8</td><td>5.0600e-8</td><td>4.8114e-8</td><td>4.5698e-8</td><td>4.3400e-8</td><td>4.1220e-8</td><td>3.9122e-8</td><td>3.7149e-8</td><td>3.5305e-8</td><td>3.3580e-8</td><td>3.2174e-8</td><td>3.1576e-8</td><td>3.0994e-8</td><td>3.0419e-8</td><td>2.9844e-8</td><td>2.9475e-8</td><td>2.9160e-8</td><td>2.8842e-8</td><td>2.8615e-8</td><td>1.6823e-7</td><td>1.9321e-7</td><td>2.2165e-7</td><td>2.5373e-7</td><td>2.8945e-7</td><td>3.2855e-7</td><td>3.7049e-7</td><td>4.1448e-7</td><td>4.5970e-7</td><td>5.0543e-7</td><td>5.5118e-7</td><td>5.9671e-7</td><td>6.4193e-7</td><td>6.8671e-7</td><td>7.3082e-7</td><td>7.7389e-7</td><td>8.1552e-7</td><td>8.5543e-7</td><td>8.9351e-7</td><td>9.2983e-7</td><td>9.6453e-7</td><td>9.9771e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.5728e-8</td><td>3.2356e-8</td><td>4.0614e-8</td><td>5.0801e-8</td><td>6.3180e-8</td><td>7.7926e-8</td><td>9.5069e-8</td><td>1.1448e-7</td><td>1.3589e-7</td><td>1.5899e-7</td><td>1.8351e-7</td><td>2.0928e-7</td><td>2.3618e-7</td><td>2.6408e-7</td><td>2.9275e-7</td><td>3.2186e-7</td><td>3.5102e-7</td><td>3.7992e-7</td><td>4.0832e-7</td><td>4.3617e-7</td><td>4.6344e-7</td><td>4.9013e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_634129&quot;</td><td>213.073771</td><td>229.421551</td><td>233.368455</td><td>242.683788</td><td>252.912786</td><td>260.953981</td><td>260.97731</td><td>253.059885</td><td>240.92989</td><td>232.91791</td><td>226.803598</td><td>221.937333</td><td>217.64686</td><td>213.245735</td><td>206.346627</td><td>197.762532</td><td>190.748626</td><td>189.534278</td><td>191.340472</td><td>192.504004</td><td>198.064985</td><td>204.083699</td><td>210.659256</td><td>216.965954</td><td>222.765872</td><td>228.264432</td><td>233.528284</td><td>238.41498</td><td>243.084839</td><td>247.407864</td><td>251.559082</td><td>255.27033</td><td>258.692439</td><td>261.988075</td><td>265.12583</td><td>268.078964</td><td>270.627849</td><td>272.981317</td><td>275.178158</td><td>277.326784</td><td>279.233698</td><td>280.737749</td><td>281.946893</td><td>283.24825</td><td>284.541981</td><td>285.846237</td><td>286.928886</td><td>287.874164</td><td>288.97811</td><td>289.864875</td><td>290.715822</td><td>291.699133</td><td>292.547412</td><td>293.170274</td><td>294.038241</td><td>295.018406</td><td>296.049939</td><td>297.159615</td><td>298.159719</td><td>299.146501</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000004</td><td>0.000009</td><td>0.000023</td><td>0.000049</td><td>0.000079</td><td>0.000129</td><td>0.000197</td><td>0.000301</td><td>0.000421</td><td>0.000615</td><td>0.000799</td><td>0.00101</td><td>0.001195</td><td>0.001334</td><td>0.001478</td><td>0.001718</td><td>0.002179</td><td>0.00276</td><td>0.003317</td><td>0.003839</td><td>0.004386</td><td>0.005077</td><td>0.006277</td><td>0.007519</td><td>0.008327</td><td>0.009056</td><td>0.009829</td><td>0.010764</td><td>0.011435</td><td>0.012143</td><td>0.01285</td><td>0.013119</td><td>0.0135</td><td>0.014371</td><td>0.015046</td><td>0.015551</td><td>0.015951</td><td>0.016206</td><td>0.016689</td><td>0.017309</td><td>9.7196e-37</td><td>9.6819e-37</td><td>9.4737e-37</td><td>8.6130e-37</td><td>7.4553e-37</td><td>7.0430e-37</td><td>5.8079e-37</td><td>3.7533e-37</td><td>1.8347e-37</td><td>2.6609e-38</td><td>1.5455e-38</td><td>4.6986e-39</td><td>2.0639e-42</td><td>1.7214e-48</td><td>6.7973e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0323e-52</td><td>7.5774e-48</td><td>1.6654e-42</td><td>2.8840e-37</td><td>1.5806e-32</td><td>5.5908e-28</td><td>1.2081e-24</td><td>8.0682e-20</td><td>2.1577e-15</td><td>5.8245e-11</td><td>1.3243e-8</td><td>3.2364e-7</td><td>8.0410e-7</td><td>2.9317e-7</td><td>1.7491e-7</td><td>2.0769e-7</td><td>0.000003</td><td>0.000004</td><td>6.1060e-7</td><td>5.5421e-7</td><td>5.5637e-7</td><td>8.9581e-7</td><td>0.000006</td><td>0.000004</td><td>0.000003</td><td>0.000004</td><td>0.000006</td><td>0.000075</td><td>0.000017</td><td>0.000014</td><td>0.000018</td><td>0.000007</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>0.000002</td><td>8.7045e-7</td><td>1.5553e-7</td><td>1.9860e-8</td><td>1.7795e-9</td><td>3.1575e-11</td><td>3.5242e-11</td><td>3.9756e-11</td><td>4.6787e-11</td><td>4.7899e-11</td><td>4.9267e-11</td><td>4.6184e-11</td><td>3.4904e-11</td><td>2.3260e-11</td><td>6.1475e-12</td><td>2.7373e-12</td><td>7.5081e-13</td><td>2.6179e-16</td><td>2.6664e-22</td><td>5.1042e-25</td><td>2.6412e-18</td><td>9.8686e-14</td><td>3.8788e-11</td><td>1.0422e-10</td><td>1.0476e-9</td><td>1.5938e-8</td><td>1.9242e-7</td><td>6.5540e-7</td><td>0.000002</td><td>0.000002</td><td>0.000006</td><td>0.000009</td><td>0.000009</td><td>0.000006</td><td>8.7136e-7</td><td>0.000003</td><td>0.000003</td><td>0.000002</td><td>6.3892e-7</td><td>5.3047e-7</td><td>2.7988e-7</td><td>2.3266e-7</td><td>2.4639e-8</td><td>1.2039e-9</td><td>1.0484e-9</td><td>0.0</td><td>1.2023e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.8992e-12</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.386828</td><td>-11.512458</td><td>-24.817131</td><td>-20.796563</td><td>-37.669454</td><td>-43.577506</td><td>-43.414096</td><td>-41.314129</td><td>-38.498899</td><td>-33.173973</td><td>-25.493209</td><td>-18.301585</td><td>-14.229118</td><td>-9.429682</td><td>-2.728357</td><td>1.08357</td><td>5.246548</td><td>10.596983</td><td>13.751444</td><td>17.168239</td><td>22.897938</td><td>27.350709</td><td>28.289143</td><td>27.581126</td><td>24.350959</td><td>20.201542</td><td>16.367193</td><td>13.093764</td><td>10.395265</td><td>8.339274</td><td>6.783111</td><td>5.510704</td><td>4.329976</td><td>3.196551</td><td>2.071899</td><td>0.949127</td><td>0.071386</td><td>-0.575528</td><td>-0.958757</td><td>-1.161323</td><td>-1.364285</td><td>-1.642227</td><td>-2.055358</td><td>-2.538765</td><td>-3.052665</td><td>-3.529975</td><td>-3.950433</td><td>-4.343366</td><td>-4.695296</td><td>-5.011322</td><td>-5.323996</td><td>-5.662009</td><td>-5.971492</td><td>-6.224755</td><td>-6.444167</td><td>-6.645438</td><td>-6.78385</td><td>-6.794718</td><td>-6.425409</td><td>-5.456625</td><td>20.983392</td><td>5.249665</td><td>4.479647</td><td>-3.157389</td><td>-5.787299</td><td>-0.279474</td><td>5.039368</td><td>5.344292</td><td>1.770346</td><td>0.714379</td><td>2.077056</td><td>1.639421</td><td>0.589909</td><td>2.806933</td><td>3.728732</td><td>3.032948</td><td>2.687794</td><td>1.551657</td><td>-0.080055</td><td>-2.55187</td><td>-4.148039</td><td>-5.943878</td><td>-6.167202</td><td>-5.431529</td><td>-3.50839</td><td>-2.183702</td><td>-1.755617</td><td>-1.703193</td><td>-1.861061</td><td>-2.088447</td><td>-2.089375</td><td>-1.82621</td><td>-1.27117</td><td>-0.607347</td><td>-0.032128</td><td>0.28745</td><td>0.364168</td><td>0.389445</td><td>0.436952</td><td>0.554593</td><td>0.725587</td><td>0.87684</td><td>0.985213</td><td>1.026965</td><td>1.047064</td><td>1.080241</td><td>1.090479</td><td>1.064247</td><td>1.027141</td><td>0.994794</td><td>0.945611</td><td>0.869893</td><td>0.813989</td><td>0.765646</td><td>0.7399</td><td>0.722111</td><td>0.725107</td><td>0.758654</td><td>0.7801</td><td>0.821863</td><td>99186.272487</td><td>0.0</td><td>70.755197</td><td>-0.892743</td><td>0.033205</td><td>-0.00567</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>455.793439</td><td>0.0</td><td>0.298189</td><td>0.701811</td><td>0.0</td><td>2.7551e-7</td><td>4.9620e-7</td><td>8.8940e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000013</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000013</td><td>0.00001</td><td>0.000007</td><td>0.000004</td><td>0.000002</td><td>0.000001</td><td>5.4335e-7</td><td>3.2919e-7</td><td>2.1876e-7</td><td>1.4950e-7</td><td>1.0933e-7</td><td>8.7582e-8</td><td>7.7536e-8</td><td>7.4163e-8</td><td>7.2076e-8</td><td>7.1080e-8</td><td>7.0781e-8</td><td>7.0635e-8</td><td>7.0695e-8</td><td>7.0802e-8</td><td>7.0773e-8</td><td>7.0642e-8</td><td>7.0367e-8</td><td>6.9130e-8</td><td>6.7862e-8</td><td>6.6464e-8</td><td>6.4919e-8</td><td>6.3383e-8</td><td>6.1233e-8</td><td>5.8041e-8</td><td>5.4994e-8</td><td>5.2104e-8</td><td>4.8473e-8</td><td>4.4674e-8</td><td>4.1062e-8</td><td>3.7635e-8</td><td>3.5023e-8</td><td>3.3181e-8</td><td>3.1460e-8</td><td>2.9850e-8</td><td>2.8331e-8</td><td>2.7395e-8</td><td>2.6727e-8</td><td>2.6068e-8</td><td>2.5409e-8</td><td>2.4823e-8</td><td>2.4685e-8</td><td>2.4545e-8</td><td>2.4471e-8</td><td>1.7700e-7</td><td>2.0328e-7</td><td>2.3320e-7</td><td>2.6696e-7</td><td>3.0454e-7</td><td>3.4568e-7</td><td>3.8980e-7</td><td>4.3609e-7</td><td>4.8367e-7</td><td>5.3178e-7</td><td>5.7992e-7</td><td>6.2782e-7</td><td>6.7540e-7</td><td>7.2251e-7</td><td>7.6892e-7</td><td>8.1424e-7</td><td>8.5804e-7</td><td>9.0003e-7</td><td>9.4009e-7</td><td>9.7831e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.1350e-8</td><td>3.9069e-8</td><td>4.8601e-8</td><td>6.0254e-8</td><td>7.4293e-8</td><td>9.0874e-8</td><td>1.1000e-7</td><td>1.3148e-7</td><td>1.5501e-7</td><td>1.8024e-7</td><td>2.0686e-7</td><td>2.3469e-7</td><td>2.6359e-7</td><td>2.9342e-7</td><td>3.2395e-7</td><td>3.5483e-7</td><td>3.8566e-7</td><td>4.1609e-7</td><td>4.4593e-7</td><td>4.7509e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_403572&quot;</td><td>212.048636</td><td>226.003864</td><td>230.980609</td><td>241.056187</td><td>252.829634</td><td>262.18577</td><td>261.900745</td><td>253.187196</td><td>241.186311</td><td>233.427801</td><td>227.551602</td><td>222.870357</td><td>218.355564</td><td>214.293878</td><td>208.696332</td><td>201.119579</td><td>194.082373</td><td>191.574244</td><td>191.439634</td><td>193.614325</td><td>198.680126</td><td>204.361576</td><td>210.181162</td><td>215.804561</td><td>221.317205</td><td>226.572773</td><td>231.736907</td><td>236.627809</td><td>241.202458</td><td>245.491838</td><td>249.651329</td><td>253.511502</td><td>257.175998</td><td>260.61964</td><td>263.84016</td><td>266.715386</td><td>269.360562</td><td>271.653413</td><td>274.017475</td><td>276.072557</td><td>278.181629</td><td>279.566684</td><td>281.092302</td><td>282.396228</td><td>284.098606</td><td>285.416977</td><td>286.76609</td><td>287.861359</td><td>288.827942</td><td>289.783145</td><td>290.64174</td><td>291.331085</td><td>292.200819</td><td>293.041512</td><td>293.902807</td><td>294.538675</td><td>295.251986</td><td>296.308722</td><td>297.493538</td><td>298.723115</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000011</td><td>0.000022</td><td>0.000042</td><td>0.000061</td><td>0.000085</td><td>0.00011</td><td>0.00017</td><td>0.000244</td><td>0.000322</td><td>0.000395</td><td>0.000491</td><td>0.000627</td><td>0.000833</td><td>0.001111</td><td>0.00155</td><td>0.002122</td><td>0.00287</td><td>0.003517</td><td>0.004173</td><td>0.004784</td><td>0.005847</td><td>0.007088</td><td>0.008175</td><td>0.008792</td><td>0.009477</td><td>0.009974</td><td>0.010389</td><td>0.010789</td><td>0.011092</td><td>0.011604</td><td>0.012546</td><td>0.013101</td><td>0.013655</td><td>0.014322</td><td>0.01524</td><td>0.016034</td><td>0.016453</td><td>0.016592</td><td>0.016683</td><td>9.7321e-37</td><td>9.6702e-37</td><td>9.4260e-37</td><td>8.5773e-37</td><td>7.7302e-37</td><td>7.7039e-37</td><td>6.7762e-37</td><td>4.5470e-37</td><td>2.4676e-37</td><td>5.1243e-38</td><td>2.5684e-38</td><td>6.4836e-39</td><td>8.9827e-43</td><td>1.0076e-47</td><td>5.2435e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.6641e-53</td><td>2.2396e-48</td><td>6.9463e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.0829e-26</td><td>6.2168e-21</td><td>1.6750e-16</td><td>4.5542e-12</td><td>1.0455e-9</td><td>1.4923e-8</td><td>7.4443e-8</td><td>7.5741e-7</td><td>3.2564e-7</td><td>9.1671e-7</td><td>0.000005</td><td>0.000003</td><td>0.000007</td><td>8.5192e-7</td><td>0.000008</td><td>0.000007</td><td>0.000073</td><td>0.000054</td><td>0.000011</td><td>0.000016</td><td>0.00002</td><td>0.000012</td><td>0.000005</td><td>0.000004</td><td>0.000001</td><td>0.000021</td><td>0.000026</td><td>0.000023</td><td>0.000017</td><td>0.000011</td><td>0.000003</td><td>7.8136e-7</td><td>2.0159e-7</td><td>1.8606e-8</td><td>3.1498e-11</td><td>3.4882e-11</td><td>3.9500e-11</td><td>4.7076e-11</td><td>4.9022e-11</td><td>5.1808e-11</td><td>5.1092e-11</td><td>4.1047e-11</td><td>2.8809e-11</td><td>1.0113e-11</td><td>4.0288e-12</td><td>1.0157e-12</td><td>7.9655e-17</td><td>1.2778e-21</td><td>1.6981e-24</td><td>0.0</td><td>2.6637e-14</td><td>8.6123e-12</td><td>1.6406e-11</td><td>7.3944e-9</td><td>5.2734e-8</td><td>1.7848e-7</td><td>1.6908e-7</td><td>0.000003</td><td>0.000005</td><td>0.000003</td><td>8.3423e-7</td><td>0.000002</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>2.1885e-7</td><td>6.5565e-8</td><td>0.000001</td><td>3.9603e-7</td><td>2.4611e-7</td><td>6.2521e-7</td><td>1.5304e-7</td><td>4.4849e-8</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.1317e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.895688</td><td>-29.097592</td><td>-34.203822</td><td>-26.292529</td><td>-32.092229</td><td>-37.212649</td><td>-40.736995</td><td>-38.776084</td><td>-34.378608</td><td>-29.958867</td><td>-23.995797</td><td>-16.313667</td><td>-9.717851</td><td>-5.701623</td><td>0.282212</td><td>6.710835</td><td>11.787517</td><td>17.137322</td><td>21.384466</td><td>26.894039</td><td>31.373947</td><td>32.788974</td><td>32.52744</td><td>30.910859</td><td>27.796946</td><td>24.692624</td><td>22.061817</td><td>19.825191</td><td>17.776414</td><td>15.818793</td><td>13.885651</td><td>11.877284</td><td>9.919114</td><td>8.120361</td><td>6.55236</td><td>5.270232</td><td>4.265699</td><td>3.445589</td><td>2.70188</td><td>2.010272</td><td>1.353232</td><td>0.750537</td><td>0.250007</td><td>-0.209561</td><td>-0.618921</td><td>-1.001443</td><td>-1.411117</td><td>-1.795138</td><td>-2.144735</td><td>-2.465519</td><td>-2.706805</td><td>-2.893646</td><td>-3.064147</td><td>-3.239915</td><td>-3.402748</td><td>-3.687885</td><td>-3.894445</td><td>-3.934927</td><td>-3.892879</td><td>-3.790526</td><td>13.161184</td><td>4.86829</td><td>15.49955</td><td>4.304659</td><td>-4.137783</td><td>-1.489619</td><td>4.318581</td><td>5.38695</td><td>1.536853</td><td>-0.516866</td><td>1.375316</td><td>2.55648</td><td>0.584175</td><td>0.286661</td><td>3.324075</td><td>3.994714</td><td>4.213756</td><td>4.146825</td><td>4.086154</td><td>4.275549</td><td>4.685594</td><td>5.83817</td><td>6.443513</td><td>6.691064</td><td>6.057019</td><td>4.967796</td><td>4.069745</td><td>3.403313</td><td>3.053714</td><td>2.873647</td><td>2.858752</td><td>2.763781</td><td>2.548221</td><td>2.133287</td><td>1.660112</td><td>1.154426</td><td>0.808214</td><td>0.620016</td><td>0.561383</td><td>0.517667</td><td>0.425216</td><td>0.257751</td><td>0.012808</td><td>-0.239181</td><td>-0.408705</td><td>-0.510568</td><td>-0.610817</td><td>-0.685114</td><td>-0.777895</td><td>-0.891026</td><td>-0.998847</td><td>-1.148757</td><td>-1.303713</td><td>-1.469805</td><td>-1.610964</td><td>-1.783924</td><td>-1.781503</td><td>-1.651063</td><td>-1.54045</td><td>-1.441145</td><td>101096.425413</td><td>0.0</td><td>80.041768</td><td>5.595857</td><td>0.020411</td><td>0.006781</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>460.810594</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.7210e-7</td><td>4.9007e-7</td><td>8.7841e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000008</td><td>0.000012</td><td>0.000014</td><td>0.000015</td><td>0.000014</td><td>0.000013</td><td>0.00001</td><td>0.000007</td><td>0.000004</td><td>0.000002</td><td>0.000001</td><td>5.7756e-7</td><td>3.5455e-7</td><td>2.5498e-7</td><td>1.8384e-7</td><td>1.3208e-7</td><td>9.8328e-8</td><td>8.0853e-8</td><td>7.5393e-8</td><td>7.2067e-8</td><td>7.0594e-8</td><td>7.0437e-8</td><td>7.0536e-8</td><td>7.1091e-8</td><td>7.1728e-8</td><td>7.2022e-8</td><td>7.2139e-8</td><td>7.1999e-8</td><td>7.1021e-8</td><td>7.0019e-8</td><td>6.8628e-8</td><td>6.6962e-8</td><td>6.5306e-8</td><td>6.2554e-8</td><td>5.9008e-8</td><td>5.5624e-8</td><td>5.2410e-8</td><td>4.7203e-8</td><td>4.2258e-8</td><td>3.7556e-8</td><td>3.3094e-8</td><td>3.1591e-8</td><td>3.0384e-8</td><td>2.9257e-8</td><td>2.8202e-8</td><td>2.7346e-8</td><td>2.6792e-8</td><td>2.6253e-8</td><td>2.5721e-8</td><td>2.5189e-8</td><td>2.4827e-8</td><td>2.4477e-8</td><td>2.4131e-8</td><td>2.4187e-8</td><td>1.7527e-7</td><td>2.0129e-7</td><td>2.3092e-7</td><td>2.6434e-7</td><td>3.0156e-7</td><td>3.4230e-7</td><td>3.8598e-7</td><td>4.3182e-7</td><td>4.7893e-7</td><td>5.2658e-7</td><td>5.7424e-7</td><td>6.2168e-7</td><td>6.6878e-7</td><td>7.1544e-7</td><td>7.6140e-7</td><td>8.0627e-7</td><td>8.4964e-7</td><td>8.9122e-7</td><td>9.3089e-7</td><td>9.6873e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.0161e-8</td><td>3.7656e-8</td><td>4.6929e-8</td><td>5.8286e-8</td><td>7.1990e-8</td><td>8.8206e-8</td><td>1.0694e-7</td><td>1.2801e-7</td><td>1.5113e-7</td><td>1.7595e-7</td><td>2.0217e-7</td><td>2.2960e-7</td><td>2.5812e-7</td><td>2.8760e-7</td><td>3.1778e-7</td><td>3.4833e-7</td><td>3.7886e-7</td><td>4.0902e-7</td><td>4.3860e-7</td><td>4.6753e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_484578&quot;</td><td>207.926097</td><td>216.697348</td><td>227.82159</td><td>243.555011</td><td>257.498525</td><td>265.294257</td><td>262.886892</td><td>253.702132</td><td>243.453695</td><td>236.891841</td><td>231.651554</td><td>227.565292</td><td>224.415847</td><td>221.184967</td><td>216.809066</td><td>211.962639</td><td>207.474773</td><td>204.363333</td><td>200.804282</td><td>202.558145</td><td>202.750097</td><td>205.465023</td><td>208.13513</td><td>211.352072</td><td>214.992253</td><td>218.816602</td><td>222.907968</td><td>226.8796</td><td>230.926906</td><td>235.015689</td><td>238.957999</td><td>242.838951</td><td>246.552663</td><td>250.100233</td><td>253.434653</td><td>256.558746</td><td>259.531588</td><td>262.387539</td><td>265.074654</td><td>267.555766</td><td>269.743913</td><td>271.38381</td><td>273.096603</td><td>275.01915</td><td>276.454907</td><td>277.961399</td><td>278.975628</td><td>280.096285</td><td>281.158367</td><td>282.398872</td><td>283.620941</td><td>284.62805</td><td>285.485231</td><td>286.267462</td><td>287.136152</td><td>287.896636</td><td>288.753091</td><td>289.687569</td><td>290.58775</td><td>291.274499</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000004</td><td>0.000007</td><td>0.00001</td><td>0.000014</td><td>0.000018</td><td>0.000022</td><td>0.00003</td><td>0.000043</td><td>0.000069</td><td>0.000117</td><td>0.000182</td><td>0.000265</td><td>0.000374</td><td>0.000515</td><td>0.000718</td><td>0.00096</td><td>0.001245</td><td>0.001563</td><td>0.001948</td><td>0.002365</td><td>0.002793</td><td>0.003236</td><td>0.003783</td><td>0.004526</td><td>0.005084</td><td>0.005418</td><td>0.005933</td><td>0.006233</td><td>0.006591</td><td>0.007064</td><td>0.007459</td><td>0.007669</td><td>0.008008</td><td>0.008362</td><td>0.008796</td><td>0.00921</td><td>0.009581</td><td>0.010069</td><td>0.010581</td><td>0.011049</td><td>0.011345</td><td>0.01184</td><td>9.7128e-37</td><td>9.6051e-37</td><td>9.5452e-37</td><td>9.5362e-37</td><td>9.6512e-37</td><td>9.5605e-37</td><td>8.6557e-37</td><td>7.1741e-37</td><td>6.0128e-37</td><td>4.8712e-37</td><td>1.8578e-37</td><td>2.0251e-38</td><td>1.0646e-42</td><td>1.2382e-49</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.7811e-40</td><td>3.3669e-33</td><td>3.3602e-29</td><td>1.5452e-23</td><td>5.2629e-18</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.4444e-8</td><td>3.7406e-8</td><td>0.0</td><td>4.2136e-7</td><td>0.000003</td><td>0.000003</td><td>0.000004</td><td>0.000022</td><td>0.000083</td><td>0.000024</td><td>0.000024</td><td>0.000023</td><td>0.000019</td><td>0.000039</td><td>0.00005</td><td>0.000019</td><td>0.000011</td><td>0.000012</td><td>0.00001</td><td>0.000008</td><td>0.000013</td><td>0.000013</td><td>0.000008</td><td>0.000002</td><td>7.8231e-7</td><td>2.7059e-7</td><td>3.2707e-11</td><td>3.4833e-11</td><td>4.0651e-11</td><td>4.4067e-11</td><td>4.8119e-11</td><td>5.4224e-11</td><td>5.7644e-11</td><td>5.6434e-11</td><td>5.2800e-11</td><td>4.8358e-11</td><td>2.4067e-11</td><td>3.2542e-12</td><td>1.6799e-16</td><td>1.1093e-23</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.9935e-13</td><td>1.2625e-11</td><td>4.1767e-9</td><td>5.1563e-8</td><td>6.2808e-8</td><td>4.8014e-8</td><td>8.5056e-8</td><td>1.5604e-7</td><td>2.0746e-7</td><td>3.8882e-7</td><td>8.5910e-7</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>0.000001</td><td>7.1042e-7</td><td>6.5356e-7</td><td>5.6581e-9</td><td>1.6544e-8</td><td>1.6084e-8</td><td>1.3615e-9</td><td>0.0</td><td>0.0</td><td>1.6286e-11</td><td>1.4384e-16</td><td>6.6560e-12</td><td>3.7114e-10</td><td>5.3211e-10</td><td>1.6815e-10</td><td>-40.514671</td><td>-40.032417</td><td>-31.828419</td><td>-18.524967</td><td>-12.830235</td><td>-14.173486</td><td>-15.047244</td><td>-13.743843</td><td>-10.587021</td><td>-7.645994</td><td>-5.238524</td><td>-1.475789</td><td>3.702735</td><td>8.542001</td><td>11.11544</td><td>15.118564</td><td>20.132157</td><td>23.884062</td><td>26.822964</td><td>29.354526</td><td>31.015959</td><td>31.800139</td><td>32.027682</td><td>32.053957</td><td>31.697876</td><td>30.933278</td><td>29.808729</td><td>28.541658</td><td>27.194826</td><td>25.891485</td><td>24.68164</td><td>23.639124</td><td>22.746605</td><td>21.928873</td><td>21.076565</td><td>20.061099</td><td>18.849762</td><td>17.503729</td><td>16.179417</td><td>15.007115</td><td>14.075261</td><td>13.291599</td><td>12.557146</td><td>11.87178</td><td>11.154768</td><td>10.471049</td><td>9.889161</td><td>9.347019</td><td>8.879216</td><td>8.480535</td><td>8.109997</td><td>7.769493</td><td>7.450909</td><td>7.145693</td><td>6.851057</td><td>6.579524</td><td>6.371834</td><td>6.262167</td><td>5.818672</td><td>3.805306</td><td>-0.848759</td><td>5.809736</td><td>13.565142</td><td>11.622131</td><td>5.192177</td><td>3.832022</td><td>4.521555</td><td>3.157626</td><td>0.36445</td><td>-1.681811</td><td>-1.608787</td><td>-0.285473</td><td>0.659845</td><td>0.252871</td><td>1.124726</td><td>4.257013</td><td>5.224426</td><td>5.710101</td><td>6.679622</td><td>7.853395</td><td>8.394954</td><td>8.425032</td><td>8.2186</td><td>7.864857</td><td>7.452511</td><td>7.143194</td><td>6.886608</td><td>6.479343</td><td>5.897673</td><td>5.157122</td><td>4.328224</td><td>3.496279</td><td>2.786117</td><td>2.240791</td><td>1.878776</td><td>1.655745</td><td>1.49622</td><td>1.296232</td><td>0.978217</td><td>0.525321</td><td>0.013448</td><td>-0.525427</td><td>-1.048021</td><td>-1.479973</td><td>-1.86184</td><td>-2.312476</td><td>-2.749788</td><td>-3.113212</td><td>-3.44963</td><td>-3.714691</td><td>-3.954645</td><td>-4.173306</td><td>-4.35114</td><td>-4.488054</td><td>-4.584904</td><td>-4.650776</td><td>-4.742572</td><td>-4.905684</td><td>-5.619688</td><td>-5.524507</td><td>101283.482359</td><td>0.0</td><td>11.273749</td><td>-2.447222</td><td>-0.020809</td><td>0.030842</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>407.406121</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.6040e-7</td><td>4.6899e-7</td><td>8.4061e-7</td><td>0.000001</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000012</td><td>0.000013</td><td>0.000013</td><td>0.000013</td><td>0.000011</td><td>0.00001</td><td>0.000007</td><td>0.000005</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>6.6758e-7</td><td>4.6964e-7</td><td>3.5960e-7</td><td>2.7383e-7</td><td>2.0008e-7</td><td>1.4038e-7</td><td>1.1380e-7</td><td>9.5438e-8</td><td>8.4788e-8</td><td>7.9227e-8</td><td>7.4739e-8</td><td>7.2256e-8</td><td>7.0076e-8</td><td>6.8281e-8</td><td>6.6685e-8</td><td>6.5100e-8</td><td>6.3685e-8</td><td>6.2235e-8</td><td>6.0612e-8</td><td>5.8859e-8</td><td>5.7117e-8</td><td>5.5244e-8</td><td>5.3288e-8</td><td>5.1422e-8</td><td>4.9651e-8</td><td>4.7129e-8</td><td>4.4702e-8</td><td>4.2394e-8</td><td>4.0204e-8</td><td>3.8262e-8</td><td>3.6457e-8</td><td>3.4770e-8</td><td>3.3193e-8</td><td>3.1927e-8</td><td>3.1338e-8</td><td>3.0764e-8</td><td>3.0197e-8</td><td>2.9631e-8</td><td>2.9297e-8</td><td>2.9010e-8</td><td>2.8719e-8</td><td>2.8492e-8</td><td>1.6681e-7</td><td>1.9158e-7</td><td>2.1978e-7</td><td>2.5159e-7</td><td>2.8701e-7</td><td>3.2578e-7</td><td>3.6736e-7</td><td>4.1098e-7</td><td>4.5582e-7</td><td>5.0116e-7</td><td>5.4652e-7</td><td>5.9167e-7</td><td>6.3651e-7</td><td>6.8091e-7</td><td>7.2465e-7</td><td>7.6735e-7</td><td>8.0863e-7</td><td>8.4820e-7</td><td>8.8596e-7</td><td>9.2198e-7</td><td>9.5638e-7</td><td>9.8928e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>2.4879e-8</td><td>3.1335e-8</td><td>3.9392e-8</td><td>4.9344e-8</td><td>6.1457e-8</td><td>7.5905e-8</td><td>9.2726e-8</td><td>1.1179e-7</td><td>1.3285e-7</td><td>1.5559e-7</td><td>1.7976e-7</td><td>2.0519e-7</td><td>2.3175e-7</td><td>2.5931e-7</td><td>2.8766e-7</td><td>3.1646e-7</td><td>3.4533e-7</td><td>3.7395e-7</td><td>4.0210e-7</td><td>4.2971e-7</td><td>4.5676e-7</td><td>4.8324e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;test_578220&quot;</td><td>216.890815</td><td>228.911969</td><td>236.654694</td><td>248.999301</td><td>256.380631</td><td>262.360629</td><td>257.791873</td><td>247.516488</td><td>237.795226</td><td>229.436973</td><td>223.840848</td><td>218.987149</td><td>214.455152</td><td>210.253412</td><td>203.824493</td><td>199.018916</td><td>190.877131</td><td>187.290404</td><td>189.283493</td><td>192.703953</td><td>198.790497</td><td>204.75373</td><td>211.289415</td><td>216.767319</td><td>222.771484</td><td>229.002498</td><td>234.251775</td><td>238.515072</td><td>242.568077</td><td>246.318855</td><td>249.8327</td><td>253.30477</td><td>256.479256</td><td>259.042021</td><td>261.275334</td><td>263.220048</td><td>265.786259</td><td>268.676433</td><td>271.683228</td><td>274.397627</td><td>276.69573</td><td>278.639437</td><td>279.652864</td><td>280.506063</td><td>281.797042</td><td>282.946524</td><td>284.163682</td><td>285.338408</td><td>286.560372</td><td>287.636299</td><td>288.569201</td><td>289.563614</td><td>290.323918</td><td>291.006692</td><td>291.721293</td><td>292.790759</td><td>293.872266</td><td>295.051167</td><td>296.262226</td><td>297.495878</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000004</td><td>0.000007</td><td>0.000011</td><td>0.000033</td><td>0.000083</td><td>0.000159</td><td>0.000292</td><td>0.000407</td><td>0.000431</td><td>0.000365</td><td>0.000301</td><td>0.000298</td><td>0.00038</td><td>0.000684</td><td>0.001178</td><td>0.001932</td><td>0.002519</td><td>0.002905</td><td>0.003077</td><td>0.003199</td><td>0.00336</td><td>0.003508</td><td>0.004223</td><td>0.005404</td><td>0.006593</td><td>0.00726</td><td>0.008064</td><td>0.008769</td><td>0.008992</td><td>0.009276</td><td>0.009807</td><td>0.010232</td><td>0.010999</td><td>0.011914</td><td>0.013087</td><td>0.013428</td><td>0.013801</td><td>0.013912</td><td>0.013956</td><td>0.01404</td><td>1.0255e-37</td><td>9.6831e-38</td><td>8.5105e-38</td><td>7.4848e-38</td><td>6.6743e-38</td><td>5.5362e-38</td><td>5.2533e-38</td><td>3.8248e-38</td><td>3.3658e-38</td><td>1.9240e-38</td><td>1.7839e-38</td><td>9.5738e-39</td><td>5.7609e-44</td><td>1.2903e-47</td><td>5.4957e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9.9242e-53</td><td>2.2527e-48</td><td>3.0741e-44</td><td>3.2172e-40</td><td>7.4653e-37</td><td>3.0357e-32</td><td>1.3435e-27</td><td>1.9898e-23</td><td>5.8084e-19</td><td>1.2265e-14</td><td>6.1973e-11</td><td>1.8722e-8</td><td>0.0</td><td>0.0</td><td>6.4923e-8</td><td>5.2863e-7</td><td>7.0171e-7</td><td>0.000001</td><td>0.000002</td><td>0.000003</td><td>0.000003</td><td>6.5247e-8</td><td>6.5122e-8</td><td>0.000002</td><td>0.000052</td><td>0.000006</td><td>0.000017</td><td>0.000004</td><td>0.000002</td><td>0.00001</td><td>0.000011</td><td>0.000011</td><td>0.000006</td><td>9.2283e-7</td><td>0.000002</td><td>0.000001</td><td>5.3185e-7</td><td>4.9293e-7</td><td>1.4069e-8</td><td>5.0483e-8</td><td>6.1028e-12</td><td>6.5125e-12</td><td>6.8174e-12</td><td>7.7943e-12</td><td>8.1095e-12</td><td>8.3080e-12</td><td>6.3072e-12</td><td>4.2078e-12</td><td>2.4430e-12</td><td>1.3018e-12</td><td>7.4322e-13</td><td>3.4289e-13</td><td>1.9021e-18</td><td>4.1825e-22</td><td>5.6206e-22</td><td>3.4514e-16</td><td>5.5620e-10</td><td>8.8530e-8</td><td>1.0012e-7</td><td>8.7511e-8</td><td>2.6333e-10</td><td>0.0</td><td>0.0</td><td>3.5540e-7</td><td>0.000002</td><td>0.000004</td><td>0.000002</td><td>0.000003</td><td>0.000002</td><td>0.000002</td><td>8.7206e-7</td><td>6.0768e-9</td><td>2.2670e-9</td><td>2.2869e-7</td><td>3.2742e-7</td><td>6.0206e-7</td><td>1.1166e-7</td><td>1.9000e-7</td><td>1.5679e-7</td><td>2.1275e-7</td><td>1.5721e-7</td><td>1.3016e-7</td><td>6.8575e-8</td><td>7.3267e-8</td><td>3.4838e-8</td><td>6.8420e-10</td><td>3.6097e-9</td><td>2.8057e-9</td><td>9.6009e-9</td><td>2.1185e-11</td><td>0.0</td><td>0.0</td><td>4.3523e-12</td><td>8.7768e-10</td><td>2.2667e-11</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>51.209186</td><td>42.419439</td><td>37.281976</td><td>35.648045</td><td>34.451416</td><td>27.809329</td><td>29.607586</td><td>22.240679</td><td>14.340837</td><td>8.470471</td><td>6.683759</td><td>6.85706</td><td>4.737829</td><td>0.168345</td><td>-1.061041</td><td>2.411994</td><td>7.735285</td><td>15.792098</td><td>26.320675</td><td>34.2857</td><td>40.828029</td><td>46.740835</td><td>48.625057</td><td>48.333362</td><td>48.036965</td><td>47.471239</td><td>46.671519</td><td>43.513668</td><td>39.277116</td><td>33.958462</td><td>28.955049</td><td>24.944975</td><td>22.012148</td><td>20.014659</td><td>18.198438</td><td>16.515235</td><td>15.106666</td><td>13.656904</td><td>12.162052</td><td>10.343934</td><td>8.465571</td><td>6.611452</td><td>5.051228</td><td>3.98619</td><td>3.31151</td><td>2.529246</td><td>1.652807</td><td>0.781019</td><td>-0.040192</td><td>-0.972076</td><td>-1.912147</td><td>-2.710149</td><td>-3.392743</td><td>-4.092902</td><td>-4.537605</td><td>-4.577928</td><td>-4.500485</td><td>-4.448912</td><td>-4.368188</td><td>-4.2399</td><td>18.015506</td><td>-22.50621</td><td>-17.31347</td><td>4.918476</td><td>3.765759</td><td>10.192913</td><td>-5.364113</td><td>-4.811059</td><td>-1.97306</td><td>0.720158</td><td>1.511261</td><td>1.277531</td><td>1.426816</td><td>0.514734</td><td>-1.303389</td><td>-1.287716</td><td>-3.50673</td><td>-6.756223</td><td>-9.073841</td><td>-9.815227</td><td>-11.015977</td><td>-5.496088</td><td>5.424886</td><td>8.262681</td><td>6.32565</td><td>2.571159</td><td>-2.623422</td><td>-7.753617</td><td>-11.240746</td><td>-11.856614</td><td>-10.227195</td><td>-6.929758</td><td>-3.456147</td><td>-0.885487</td><td>0.784514</td><td>2.280307</td><td>3.467787</td><td>3.767892</td><td>3.505961</td><td>2.9427</td><td>2.405072</td><td>2.17702</td><td>2.406374</td><td>2.76599</td><td>2.591751</td><td>2.23986</td><td>1.978571</td><td>1.810335</td><td>1.67127</td><td>1.597733</td><td>1.515346</td><td>1.431827</td><td>1.240866</td><td>1.098105</td><td>0.36846</td><td>0.05819</td><td>-0.301357</td><td>-0.440503</td><td>-0.53131</td><td>-0.596656</td><td>100806.042921</td><td>898.336286</td><td>98.588601</td><td>5.033812</td><td>0.023682</td><td>0.003176</td><td>0.638527</td><td>0.06</td><td>0.048138</td><td>0.06</td><td>0.048138</td><td>452.465195</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0087e-7</td><td>5.4188e-7</td><td>9.7127e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000015</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>4.2125e-7</td><td>1.8415e-7</td><td>1.1659e-7</td><td>8.8404e-8</td><td>7.2384e-8</td><td>6.2100e-8</td><td>5.6742e-8</td><td>5.5711e-8</td><td>5.5654e-8</td><td>5.6333e-8</td><td>5.7553e-8</td><td>5.8866e-8</td><td>6.0504e-8</td><td>6.2214e-8</td><td>6.3771e-8</td><td>6.5290e-8</td><td>6.6536e-8</td><td>6.7011e-8</td><td>6.7497e-8</td><td>6.7260e-8</td><td>6.6603e-8</td><td>6.5950e-8</td><td>6.4816e-8</td><td>6.3419e-8</td><td>6.2086e-8</td><td>6.0839e-8</td><td>5.9843e-8</td><td>5.8898e-8</td><td>5.7999e-8</td><td>5.7167e-8</td><td>5.6854e-8</td><td>5.6560e-8</td><td>5.6286e-8</td><td>5.6029e-8</td><td>5.6050e-8</td><td>5.6322e-8</td><td>5.6588e-8</td><td>5.6850e-8</td><td>5.7079e-8</td><td>5.7031e-8</td><td>5.6982e-8</td><td>5.6929e-8</td><td>5.6862e-8</td><td>1.8144e-7</td><td>2.0838e-7</td><td>2.3905e-7</td><td>2.7365e-7</td><td>3.1218e-7</td><td>3.5435e-7</td><td>3.9957e-7</td><td>4.4702e-7</td><td>4.9579e-7</td><td>5.4511e-7</td><td>5.9445e-7</td><td>6.4356e-7</td><td>6.9232e-7</td><td>7.4062e-7</td><td>7.8819e-7</td><td>8.3464e-7</td><td>8.7954e-7</td><td>9.2259e-7</td><td>9.6366e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.4774e-8</td><td>4.3110e-8</td><td>5.3352e-8</td><td>6.5808e-8</td><td>8.0738e-8</td><td>9.8287e-8</td><td>1.1843e-7</td><td>1.4097e-7</td><td>1.6555e-7</td><td>1.9180e-7</td><td>2.1942e-7</td><td>2.4819e-7</td><td>2.7798e-7</td><td>3.0866e-7</td><td>3.3997e-7</td><td>3.7158e-7</td><td>4.0306e-7</td><td>4.3409e-7</td><td>4.6445e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_395695&quot;</td><td>215.417881</td><td>229.137757</td><td>237.973745</td><td>246.218479</td><td>259.189075</td><td>260.83965</td><td>258.333569</td><td>253.231639</td><td>238.788101</td><td>228.37941</td><td>223.069909</td><td>220.787349</td><td>216.499705</td><td>210.730831</td><td>203.953776</td><td>194.718664</td><td>188.666371</td><td>184.449184</td><td>186.865067</td><td>191.698841</td><td>198.851628</td><td>204.828501</td><td>209.950302</td><td>215.456475</td><td>222.533569</td><td>228.906147</td><td>234.612552</td><td>239.638282</td><td>244.021863</td><td>248.023302</td><td>251.402328</td><td>254.368986</td><td>257.646144</td><td>261.002444</td><td>263.9314</td><td>266.801316</td><td>270.141455</td><td>273.535375</td><td>276.593705</td><td>279.327132</td><td>281.5978</td><td>282.528064</td><td>282.734895</td><td>283.418812</td><td>284.320428</td><td>285.386803</td><td>286.568874</td><td>287.784512</td><td>288.781688</td><td>289.612918</td><td>290.449955</td><td>291.255506</td><td>292.038606</td><td>292.838503</td><td>293.709285</td><td>294.597545</td><td>295.368287</td><td>296.335054</td><td>297.440068</td><td>298.650933</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>8.4727e-7</td><td>8.2581e-7</td><td>8.5970e-7</td><td>0.000002</td><td>0.000004</td><td>0.000005</td><td>0.000011</td><td>0.000041</td><td>0.000088</td><td>0.000153</td><td>0.000277</td><td>0.000499</td><td>0.000745</td><td>0.000997</td><td>0.001312</td><td>0.001584</td><td>0.001726</td><td>0.001856</td><td>0.00193</td><td>0.00191</td><td>0.001777</td><td>0.001588</td><td>0.001456</td><td>0.001428</td><td>0.001544</td><td>0.002432</td><td>0.005559</td><td>0.007683</td><td>0.009252</td><td>0.010312</td><td>0.011117</td><td>0.011551</td><td>0.011978</td><td>0.012544</td><td>0.013048</td><td>0.013556</td><td>0.014097</td><td>0.014605</td><td>0.015051</td><td>0.015457</td><td>0.016214</td><td>0.016736</td><td>0.017014</td><td>0.017099</td><td>1.0195e-37</td><td>9.5385e-38</td><td>8.4813e-38</td><td>7.8176e-38</td><td>6.7270e-38</td><td>5.3512e-38</td><td>4.2916e-38</td><td>3.2226e-38</td><td>1.7679e-38</td><td>4.7905e-39</td><td>1.2387e-39</td><td>5.4993e-40</td><td>9.5386e-45</td><td>1.8132e-48</td><td>4.0479e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.2306e-50</td><td>2.4816e-45</td><td>2.7617e-39</td><td>1.6750e-35</td><td>4.6384e-31</td><td>1.5931e-26</td><td>3.3353e-22</td><td>6.6828e-18</td><td>6.7236e-14</td><td>3.6194e-10</td><td>2.8905e-8</td><td>5.0545e-8</td><td>2.7186e-8</td><td>1.3351e-8</td><td>3.3022e-9</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.9953e-7</td><td>0.00002</td><td>0.000009</td><td>0.000016</td><td>0.000008</td><td>0.00002</td><td>0.000013</td><td>0.00001</td><td>0.000008</td><td>0.000011</td><td>0.000008</td><td>0.000011</td><td>0.000017</td><td>0.000014</td><td>0.000006</td><td>0.000002</td><td>5.9203e-8</td><td>2.8390e-9</td><td>8.3071e-9</td><td>6.1567e-12</td><td>6.5463e-12</td><td>7.6386e-12</td><td>9.8801e-12</td><td>8.5007e-12</td><td>9.6855e-12</td><td>7.4884e-12</td><td>6.0782e-12</td><td>3.9052e-12</td><td>1.9818e-12</td><td>9.4869e-13</td><td>1.3513e-13</td><td>6.3659e-19</td><td>1.0206e-22</td><td>1.6182e-27</td><td>1.9016e-17</td><td>1.1009e-11</td><td>2.2732e-9</td><td>7.3264e-9</td><td>1.3369e-8</td><td>6.2237e-10</td><td>1.2517e-9</td><td>3.5348e-8</td><td>0.000007</td><td>0.000016</td><td>0.000018</td><td>0.000017</td><td>0.000019</td><td>0.00002</td><td>0.000016</td><td>0.000004</td><td>0.000002</td><td>6.0101e-7</td><td>8.9458e-8</td><td>1.5849e-8</td><td>4.6950e-9</td><td>2.5531e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.1408e-12</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.4422e-14</td><td>1.3707e-13</td><td>2.2438e-13</td><td>0.0</td><td>20.524008</td><td>11.802658</td><td>-1.470289</td><td>-11.650991</td><td>0.527938</td><td>-13.068308</td><td>-0.530987</td><td>-6.100695</td><td>-11.254077</td><td>-7.793831</td><td>-3.578696</td><td>2.155702</td><td>-0.065155</td><td>-3.212529</td><td>-5.278644</td><td>-2.017981</td><td>9.459058</td><td>16.106525</td><td>20.314203</td><td>24.469862</td><td>28.051703</td><td>29.734808</td><td>29.425833</td><td>30.037842</td><td>30.483579</td><td>30.392413</td><td>29.19196</td><td>26.62855</td><td>24.036171</td><td>21.964985</td><td>20.16926</td><td>18.769084</td><td>17.378007</td><td>15.586056</td><td>12.791198</td><td>9.759307</td><td>7.330399</td><td>5.099918</td><td>2.462769</td><td>-0.475358</td><td>-3.580935</td><td>-6.801777</td><td>-9.821074</td><td>-11.137475</td><td>-11.234591</td><td>-10.841384</td><td>-9.942276</td><td>-8.949552</td><td>-8.185709</td><td>-7.528274</td><td>-6.958012</td><td>-6.406455</td><td>-5.896166</td><td>-5.402691</td><td>-4.908382</td><td>-4.420759</td><td>-3.736797</td><td>-3.195257</td><td>-2.809322</td><td>-2.64099</td><td>-1.339814</td><td>-16.644096</td><td>-2.232021</td><td>11.885605</td><td>1.024442</td><td>0.578848</td><td>-4.366504</td><td>-0.900303</td><td>0.291313</td><td>-0.477247</td><td>1.80855</td><td>0.961809</td><td>0.369959</td><td>-0.326075</td><td>-1.623963</td><td>-0.154821</td><td>3.037685</td><td>-1.834551</td><td>-11.397085</td><td>-17.255997</td><td>-20.447333</td><td>-16.964167</td><td>-11.801245</td><td>-11.611755</td><td>-13.262797</td><td>-14.399235</td><td>-15.262945</td><td>-15.220297</td><td>-14.028617</td><td>-11.76185</td><td>-9.361573</td><td>-6.713737</td><td>-4.097246</td><td>-2.069601</td><td>-0.301056</td><td>1.186739</td><td>1.840707</td><td>1.872513</td><td>1.744303</td><td>1.626873</td><td>1.452098</td><td>1.2803</td><td>1.2355</td><td>1.009737</td><td>0.651134</td><td>0.234168</td><td>-0.27645</td><td>-0.767151</td><td>-1.052228</td><td>-1.148744</td><td>-1.168753</td><td>-1.08572</td><td>-0.953024</td><td>-0.782604</td><td>-0.56262</td><td>-0.399534</td><td>-0.24004</td><td>-0.113426</td><td>-0.029615</td><td>0.005354</td><td>100671.583235</td><td>854.94506</td><td>49.095183</td><td>2.26653</td><td>0.011482</td><td>-0.000041</td><td>0.607685</td><td>0.06</td><td>0.054207</td><td>0.06</td><td>0.054207</td><td>457.907636</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0920e-7</td><td>5.5688e-7</td><td>9.9815e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000016</td><td>0.000017</td><td>0.000016</td><td>0.000013</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>7.3905e-7</td><td>2.5238e-7</td><td>1.2105e-7</td><td>8.6844e-8</td><td>7.2369e-8</td><td>6.3178e-8</td><td>5.6360e-8</td><td>5.1582e-8</td><td>4.9421e-8</td><td>4.8068e-8</td><td>4.7286e-8</td><td>4.7727e-8</td><td>4.8320e-8</td><td>4.9717e-8</td><td>5.1208e-8</td><td>5.2622e-8</td><td>5.4034e-8</td><td>5.5389e-8</td><td>5.6554e-8</td><td>5.7748e-8</td><td>5.8021e-8</td><td>5.7819e-8</td><td>5.7618e-8</td><td>5.6221e-8</td><td>5.4234e-8</td><td>5.2337e-8</td><td>5.0674e-8</td><td>5.0012e-8</td><td>4.9384e-8</td><td>4.8787e-8</td><td>4.8249e-8</td><td>4.7991e-8</td><td>4.7749e-8</td><td>4.7523e-8</td><td>4.7311e-8</td><td>4.7032e-8</td><td>4.6711e-8</td><td>4.6398e-8</td><td>4.6090e-8</td><td>4.5797e-8</td><td>4.5562e-8</td><td>4.5324e-8</td><td>4.5140e-8</td><td>4.5077e-8</td><td>1.8501e-7</td><td>2.1249e-7</td><td>2.4376e-7</td><td>2.7904e-7</td><td>3.1833e-7</td><td>3.6133e-7</td><td>4.0745e-7</td><td>4.5583e-7</td><td>5.0557e-7</td><td>5.5586e-7</td><td>6.0617e-7</td><td>6.5625e-7</td><td>7.0597e-7</td><td>7.5522e-7</td><td>8.0373e-7</td><td>8.5110e-7</td><td>8.9688e-7</td><td>9.4077e-7</td><td>9.8265e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.8830e-8</td><td>4.7824e-8</td><td>5.8800e-8</td><td>7.2065e-8</td><td>8.7863e-8</td><td>1.0632e-7</td><td>1.2738e-7</td><td>1.5081e-7</td><td>1.7625e-7</td><td>2.0328e-7</td><td>2.3159e-7</td><td>2.6097e-7</td><td>2.9129e-7</td><td>3.2240e-7</td><td>3.5406e-7</td><td>3.8592e-7</td><td>4.1759e-7</td><td>4.4871e-7</td><td>4.7911e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_88942&quot;</td><td>215.135772</td><td>232.704767</td><td>240.141912</td><td>245.662113</td><td>259.146969</td><td>262.647895</td><td>257.051904</td><td>250.79876</td><td>238.528778</td><td>229.844499</td><td>224.788552</td><td>220.429468</td><td>214.781601</td><td>209.430195</td><td>203.063896</td><td>195.09745</td><td>190.148431</td><td>186.0906</td><td>188.002738</td><td>191.541135</td><td>197.895704</td><td>203.693431</td><td>209.940932</td><td>216.052365</td><td>222.503102</td><td>228.646343</td><td>234.139038</td><td>238.988475</td><td>243.535863</td><td>247.31566</td><td>250.698097</td><td>253.62759</td><td>256.733883</td><td>260.466638</td><td>263.921188</td><td>267.407461</td><td>270.662386</td><td>273.734718</td><td>276.561968</td><td>278.754379</td><td>280.380761</td><td>281.712617</td><td>282.329215</td><td>283.219744</td><td>284.410044</td><td>285.801689</td><td>286.78303</td><td>287.813578</td><td>288.812922</td><td>289.713165</td><td>290.613272</td><td>291.315381</td><td>291.848483</td><td>292.611633</td><td>293.394563</td><td>294.262868</td><td>295.214247</td><td>296.28421</td><td>297.486999</td><td>298.731429</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000001</td><td>8.8678e-7</td><td>7.9174e-7</td><td>8.0687e-7</td><td>0.000002</td><td>0.000005</td><td>0.000008</td><td>0.000019</td><td>0.000048</td><td>0.000098</td><td>0.000179</td><td>0.000296</td><td>0.000466</td><td>0.000669</td><td>0.000963</td><td>0.001298</td><td>0.001602</td><td>0.001745</td><td>0.001747</td><td>0.001622</td><td>0.001438</td><td>0.001329</td><td>0.001324</td><td>0.001442</td><td>0.002077</td><td>0.003508</td><td>0.00505</td><td>0.007334</td><td>0.00906</td><td>0.010082</td><td>0.010834</td><td>0.011441</td><td>0.011846</td><td>0.012397</td><td>0.012911</td><td>0.013325</td><td>0.013766</td><td>0.014504</td><td>0.015081</td><td>0.015599</td><td>0.016094</td><td>0.016576</td><td>0.016894</td><td>0.017009</td><td>0.01708</td><td>1.0149e-37</td><td>9.5160e-38</td><td>8.6333e-38</td><td>7.6879e-38</td><td>6.6545e-38</td><td>5.3905e-38</td><td>4.4734e-38</td><td>3.1063e-38</td><td>1.7183e-38</td><td>5.1117e-39</td><td>1.2482e-39</td><td>5.1465e-40</td><td>8.1227e-44</td><td>9.3437e-49</td><td>1.5528e-55</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.8628e-57</td><td>7.6737e-54</td><td>7.1301e-49</td><td>2.3488e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.1595e-18</td><td>0.0</td><td>0.0</td><td>1.7669e-8</td><td>1.4851e-7</td><td>1.6483e-7</td><td>3.9592e-8</td><td>1.6248e-9</td><td>0.0</td><td>8.2662e-8</td><td>0.000002</td><td>5.0830e-7</td><td>0.000003</td><td>0.000003</td><td>0.000004</td><td>0.000018</td><td>0.000003</td><td>0.000004</td><td>0.000003</td><td>0.000013</td><td>0.000008</td><td>0.00001</td><td>0.000012</td><td>0.000037</td><td>0.000048</td><td>0.000016</td><td>0.00001</td><td>5.5911e-7</td><td>1.4527e-7</td><td>5.1456e-8</td><td>7.2670e-8</td><td>6.2296e-12</td><td>6.5300e-12</td><td>7.9199e-12</td><td>9.4948e-12</td><td>8.6921e-12</td><td>1.0068e-11</td><td>8.7134e-12</td><td>5.9803e-12</td><td>2.7788e-12</td><td>1.5869e-12</td><td>8.2614e-13</td><td>1.4209e-13</td><td>2.1172e-17</td><td>2.2504e-22</td><td>1.8651e-22</td><td>1.9827e-17</td><td>8.4050e-12</td><td>1.5358e-9</td><td>6.6995e-10</td><td>1.1585e-8</td><td>6.7242e-10</td><td>1.3722e-8</td><td>6.3314e-8</td><td>0.000001</td><td>0.000001</td><td>0.000001</td><td>0.000002</td><td>0.000004</td><td>0.000005</td><td>0.000006</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>7.2095e-7</td><td>3.2791e-7</td><td>5.2340e-8</td><td>1.9285e-9</td><td>0.0</td><td>2.3510e-10</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.6102e-12</td><td>1.1740e-13</td><td>3.6012e-14</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9.8479e-15</td><td>18.722166</td><td>17.760093</td><td>-1.939848</td><td>-1.779939</td><td>2.664798</td><td>-14.812423</td><td>-7.129902</td><td>-6.119783</td><td>-8.868477</td><td>-3.867754</td><td>-3.596304</td><td>-2.56724</td><td>-3.202521</td><td>-2.468386</td><td>-3.474392</td><td>-1.260273</td><td>9.92765</td><td>14.317554</td><td>14.355944</td><td>17.531598</td><td>23.833913</td><td>28.676099</td><td>30.479286</td><td>31.606482</td><td>31.52993</td><td>29.526812</td><td>26.226882</td><td>22.55532</td><td>20.001379</td><td>18.341267</td><td>17.300985</td><td>16.404362</td><td>15.029361</td><td>13.308423</td><td>11.297323</td><td>9.316709</td><td>6.963697</td><td>4.25882</td><td>0.671901</td><td>-3.633249</td><td>-7.402048</td><td>-9.95146</td><td>-10.981887</td><td>-10.959769</td><td>-10.450645</td><td>-9.384851</td><td>-8.314138</td><td>-7.342307</td><td>-6.588866</td><td>-5.945333</td><td>-5.321458</td><td>-4.750257</td><td>-4.135325</td><td>-3.44858</td><td>-2.792322</td><td>-2.184244</td><td>-1.566916</td><td>-1.170932</td><td>-1.053412</td><td>-0.982516</td><td>-10.326599</td><td>-6.535773</td><td>13.815284</td><td>13.229978</td><td>-0.639377</td><td>-0.859063</td><td>-2.666459</td><td>1.556782</td><td>1.795009</td><td>-1.969189</td><td>-1.053233</td><td>-0.142041</td><td>0.655943</td><td>0.261201</td><td>0.367795</td><td>-1.940656</td><td>3.079126</td><td>6.783854</td><td>-1.096384</td><td>-14.557691</td><td>-22.156552</td><td>-24.851334</td><td>-23.142287</td><td>-21.890604</td><td>-21.159727</td><td>-19.664989</td><td>-16.874067</td><td>-12.766769</td><td>-8.934568</td><td>-5.541738</td><td>-2.916226</td><td>-0.26189</td><td>2.370863</td><td>3.873087</td><td>4.10843</td><td>3.933822</td><td>3.493504</td><td>3.175473</td><td>2.937182</td><td>2.648715</td><td>2.409712</td><td>1.985027</td><td>1.620282</td><td>1.209234</td><td>0.86002</td><td>0.475584</td><td>0.007463</td><td>-0.52146</td><td>-1.020854</td><td>-1.433941</td><td>-1.737137</td><td>-1.9217</td><td>-1.984218</td><td>-1.916981</td><td>-1.68363</td><td>-1.347555</td><td>-1.017434</td><td>-0.675494</td><td>-0.55038</td><td>-0.47987</td><td>100274.203432</td><td>654.966094</td><td>34.293045</td><td>1.692516</td><td>0.003738</td><td>0.002162</td><td>0.465542</td><td>0.06</td><td>0.087262</td><td>0.06</td><td>0.087262</td><td>458.870692</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0892e-7</td><td>5.5639e-7</td><td>9.9727e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000016</td><td>0.000017</td><td>0.000016</td><td>0.000013</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>7.1570e-7</td><td>2.3401e-7</td><td>1.2068e-7</td><td>8.8657e-8</td><td>7.5348e-8</td><td>6.7453e-8</td><td>6.1790e-8</td><td>5.7566e-8</td><td>5.5494e-8</td><td>5.4143e-8</td><td>5.3332e-8</td><td>5.3835e-8</td><td>5.4537e-8</td><td>5.5551e-8</td><td>5.6623e-8</td><td>5.7807e-8</td><td>5.9063e-8</td><td>6.0487e-8</td><td>6.2320e-8</td><td>6.4197e-8</td><td>6.4768e-8</td><td>6.4460e-8</td><td>6.4153e-8</td><td>6.2693e-8</td><td>6.0425e-8</td><td>5.8260e-8</td><td>5.6232e-8</td><td>5.5930e-8</td><td>5.5642e-8</td><td>5.5369e-8</td><td>5.5110e-8</td><td>5.5123e-8</td><td>5.5151e-8</td><td>5.5177e-8</td><td>5.5202e-8</td><td>5.5037e-8</td><td>5.4532e-8</td><td>5.4040e-8</td><td>5.3554e-8</td><td>5.3068e-8</td><td>5.2618e-8</td><td>5.2167e-8</td><td>5.1717e-8</td><td>5.1601e-8</td><td>1.8497e-7</td><td>2.1244e-7</td><td>2.4371e-7</td><td>2.7898e-7</td><td>3.1826e-7</td><td>3.6125e-7</td><td>4.0735e-7</td><td>4.5573e-7</td><td>5.0545e-7</td><td>5.5573e-7</td><td>6.0603e-7</td><td>6.5610e-7</td><td>7.0581e-7</td><td>7.5505e-7</td><td>8.0355e-7</td><td>8.5090e-7</td><td>8.9668e-7</td><td>9.4056e-7</td><td>9.8243e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.8754e-8</td><td>4.7736e-8</td><td>5.8700e-8</td><td>7.1951e-8</td><td>8.7736e-8</td><td>1.0618e-7</td><td>1.2723e-7</td><td>1.5065e-7</td><td>1.7607e-7</td><td>2.0309e-7</td><td>2.3140e-7</td><td>2.6078e-7</td><td>2.9109e-7</td><td>3.2220e-7</td><td>3.5387e-7</td><td>3.8573e-7</td><td>4.1740e-7</td><td>4.4853e-7</td><td>4.7893e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_79382&quot;</td><td>214.901184</td><td>233.680255</td><td>238.15687</td><td>248.790149</td><td>255.438338</td><td>261.873094</td><td>257.734807</td><td>247.84304</td><td>237.012438</td><td>228.727903</td><td>224.292368</td><td>220.079396</td><td>214.653191</td><td>209.639262</td><td>203.138675</td><td>199.302089</td><td>193.921603</td><td>189.000811</td><td>190.088497</td><td>193.27943</td><td>198.858634</td><td>204.43528</td><td>210.750707</td><td>216.076045</td><td>222.250302</td><td>227.990322</td><td>232.826341</td><td>236.837375</td><td>240.402774</td><td>244.051689</td><td>247.401324</td><td>250.766003</td><td>253.878009</td><td>256.356436</td><td>258.638422</td><td>261.275437</td><td>264.396089</td><td>267.521729</td><td>270.586893</td><td>273.461743</td><td>276.047152</td><td>278.104736</td><td>279.692116</td><td>280.596791</td><td>281.034192</td><td>282.102634</td><td>283.295878</td><td>284.581847</td><td>285.760603</td><td>286.929462</td><td>287.77447</td><td>288.764479</td><td>289.704445</td><td>290.891188</td><td>292.129641</td><td>293.35581</td><td>294.575253</td><td>295.81086</td><td>297.055001</td><td>298.3299</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000007</td><td>0.000012</td><td>0.000039</td><td>0.000084</td><td>0.000153</td><td>0.000268</td><td>0.000359</td><td>0.00036</td><td>0.000297</td><td>0.000275</td><td>0.000298</td><td>0.00044</td><td>0.000863</td><td>0.001429</td><td>0.001863</td><td>0.002066</td><td>0.002147</td><td>0.002095</td><td>0.001996</td><td>0.002045</td><td>0.002201</td><td>0.002725</td><td>0.003783</td><td>0.005656</td><td>0.007051</td><td>0.008019</td><td>0.008582</td><td>0.009266</td><td>0.009702</td><td>0.010539</td><td>0.011249</td><td>0.012267</td><td>0.012545</td><td>0.012626</td><td>0.012695</td><td>0.012759</td><td>0.012806</td><td>0.012857</td><td>0.012966</td><td>1.0240e-37</td><td>9.7601e-38</td><td>8.5978e-38</td><td>7.5063e-38</td><td>6.6207e-38</td><td>5.4017e-38</td><td>5.2994e-38</td><td>3.8816e-38</td><td>3.3322e-38</td><td>2.1390e-38</td><td>2.0676e-38</td><td>1.0688e-38</td><td>5.4397e-44</td><td>1.1950e-47</td><td>5.0025e-54</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.2262e-54</td><td>1.8205e-49</td><td>3.9505e-45</td><td>2.5968e-40</td><td>8.6021e-37</td><td>3.1059e-32</td><td>1.3330e-27</td><td>1.8119e-23</td><td>2.9816e-19</td><td>2.1595e-18</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.9384e-8</td><td>7.2420e-8</td><td>1.1810e-7</td><td>1.0490e-7</td><td>1.7525e-7</td><td>1.8097e-7</td><td>0.0</td><td>8.8280e-8</td><td>0.000002</td><td>0.000003</td><td>0.000001</td><td>0.000008</td><td>0.000013</td><td>0.000026</td><td>0.000015</td><td>0.000017</td><td>0.00001</td><td>0.000011</td><td>0.000006</td><td>0.000001</td><td>9.5407e-7</td><td>5.0271e-7</td><td>1.2904e-7</td><td>3.9397e-7</td><td>6.9656e-8</td><td>1.0900e-7</td><td>3.2887e-8</td><td>6.1281e-12</td><td>6.4930e-12</td><td>6.7827e-12</td><td>7.6458e-12</td><td>7.5475e-12</td><td>8.6658e-12</td><td>6.2857e-12</td><td>4.0058e-12</td><td>2.3955e-12</td><td>1.2466e-12</td><td>8.6311e-13</td><td>3.7115e-13</td><td>1.7075e-18</td><td>3.8695e-22</td><td>5.8055e-23</td><td>1.0269e-17</td><td>1.4395e-11</td><td>1.2922e-8</td><td>8.0708e-8</td><td>1.3658e-7</td><td>6.4196e-9</td><td>9.6679e-9</td><td>3.9776e-8</td><td>6.6408e-7</td><td>0.000001</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000004</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>3.0622e-7</td><td>2.8218e-7</td><td>1.4132e-7</td><td>1.4059e-8</td><td>1.1835e-8</td><td>0.0</td><td>1.1867e-8</td><td>9.3804e-9</td><td>1.1343e-8</td><td>7.1379e-9</td><td>8.4519e-9</td><td>5.6288e-9</td><td>3.8800e-9</td><td>3.3601e-9</td><td>5.0977e-9</td><td>4.7965e-9</td><td>3.0257e-9</td><td>2.7207e-9</td><td>2.4084e-9</td><td>0.0</td><td>8.1785e-10</td><td>2.1322e-11</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>51.985733</td><td>37.025343</td><td>38.208433</td><td>41.439662</td><td>38.001918</td><td>30.190197</td><td>30.021533</td><td>25.929628</td><td>18.735208</td><td>12.256085</td><td>8.290056</td><td>5.35526</td><td>2.954092</td><td>1.004033</td><td>0.758437</td><td>6.619794</td><td>13.552734</td><td>20.842486</td><td>27.181561</td><td>33.445408</td><td>40.895726</td><td>47.974711</td><td>51.218588</td><td>51.514585</td><td>50.413059</td><td>47.723948</td><td>43.560689</td><td>37.232692</td><td>30.558484</td><td>25.744329</td><td>22.663329</td><td>21.501361</td><td>20.784877</td><td>19.586174</td><td>17.881074</td><td>16.178472</td><td>14.704635</td><td>12.95959</td><td>10.988883</td><td>8.957445</td><td>7.166212</td><td>5.421142</td><td>4.073932</td><td>3.030966</td><td>1.935302</td><td>0.898479</td><td>-0.2374</td><td>-1.384259</td><td>-2.503452</td><td>-3.631182</td><td>-4.770764</td><td>-5.135978</td><td>-5.241799</td><td>-5.185748</td><td>-5.096547</td><td>-5.027384</td><td>-4.974548</td><td>-4.901653</td><td>-4.805298</td><td>-4.648118</td><td>-1.965289</td><td>-13.129979</td><td>-5.666485</td><td>10.940575</td><td>4.035544</td><td>13.363701</td><td>-2.62111</td><td>-1.983662</td><td>-0.392965</td><td>1.379843</td><td>1.769408</td><td>0.686781</td><td>0.276895</td><td>0.326928</td><td>-0.402234</td><td>-0.026033</td><td>0.193482</td><td>-3.025883</td><td>-6.598888</td><td>-11.725105</td><td>-15.12488</td><td>-15.234713</td><td>-10.115342</td><td>-8.825838</td><td>-9.523812</td><td>-11.556454</td><td>-13.714868</td><td>-14.100296</td><td>-12.718612</td><td>-8.868519</td><td>-4.939118</td><td>-1.703198</td><td>0.321915</td><td>0.954065</td><td>1.369331</td><td>1.735942</td><td>1.75698</td><td>1.537375</td><td>1.317913</td><td>1.320769</td><td>1.361174</td><td>1.626889</td><td>2.054247</td><td>2.287533</td><td>2.128686</td><td>1.667322</td><td>1.095501</td><td>0.552384</td><td>0.028008</td><td>-0.460208</td><td>-1.217157</td><td>-1.664914</td><td>-2.227255</td><td>-2.426104</td><td>-2.512243</td><td>-2.51986</td><td>-2.584885</td><td>-2.594903</td><td>-2.590013</td><td>-2.542154</td><td>98762.158347</td><td>724.008071</td><td>185.437869</td><td>25.138019</td><td>0.061392</td><td>0.033524</td><td>0.514617</td><td>0.084467</td><td>0.093485</td><td>0.060476</td><td>0.070729</td><td>462.394743</td><td>0.0</td><td>0.124711</td><td>0.875289</td><td>0.0</td><td>3.0082e-7</td><td>5.4180e-7</td><td>9.7112e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000014</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>0.000001</td><td>4.4315e-7</td><td>2.0092e-7</td><td>1.2922e-7</td><td>9.7813e-8</td><td>7.9838e-8</td><td>6.8105e-8</td><td>6.1687e-8</td><td>6.0120e-8</td><td>5.9512e-8</td><td>5.9809e-8</td><td>6.0668e-8</td><td>6.1699e-8</td><td>6.2575e-8</td><td>6.3422e-8</td><td>6.4365e-8</td><td>6.5408e-8</td><td>6.6470e-8</td><td>6.7327e-8</td><td>6.8205e-8</td><td>6.8819e-8</td><td>6.8903e-8</td><td>6.8986e-8</td><td>6.9011e-8</td><td>6.8856e-8</td><td>6.8708e-8</td><td>6.8568e-8</td><td>6.8128e-8</td><td>6.7445e-8</td><td>6.6796e-8</td><td>6.6180e-8</td><td>6.5696e-8</td><td>6.5551e-8</td><td>6.5416e-8</td><td>6.5290e-8</td><td>6.5170e-8</td><td>6.5273e-8</td><td>6.5925e-8</td><td>6.6569e-8</td><td>6.7213e-8</td><td>6.7861e-8</td><td>6.8092e-8</td><td>6.8143e-8</td><td>6.8196e-8</td><td>1.8082e-7</td><td>2.0767e-7</td><td>2.3824e-7</td><td>2.7272e-7</td><td>3.1112e-7</td><td>3.5314e-7</td><td>3.9821e-7</td><td>4.4550e-7</td><td>4.9411e-7</td><td>5.4326e-7</td><td>5.9243e-7</td><td>6.4137e-7</td><td>6.8997e-7</td><td>7.3811e-7</td><td>7.8552e-7</td><td>8.3181e-7</td><td>8.7656e-7</td><td>9.1945e-7</td><td>9.6038e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.4248e-8</td><td>4.2493e-8</td><td>5.2630e-8</td><td>6.4969e-8</td><td>7.9770e-8</td><td>9.7180e-8</td><td>1.1718e-7</td><td>1.3957e-7</td><td>1.6400e-7</td><td>1.9012e-7</td><td>2.1760e-7</td><td>2.4624e-7</td><td>2.7592e-7</td><td>3.0649e-7</td><td>3.3771e-7</td><td>3.6923e-7</td><td>4.0063e-7</td><td>4.3159e-7</td><td>4.6190e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr><tr><td>&quot;test_601350&quot;</td><td>214.546909</td><td>233.460889</td><td>237.850575</td><td>248.361199</td><td>252.353758</td><td>261.426464</td><td>257.17252</td><td>248.371481</td><td>237.316651</td><td>229.167941</td><td>224.54255</td><td>219.772206</td><td>214.133077</td><td>209.309529</td><td>202.747871</td><td>198.448037</td><td>195.440953</td><td>190.555468</td><td>190.98908</td><td>193.930869</td><td>199.341072</td><td>204.204674</td><td>210.209238</td><td>215.4366</td><td>221.246391</td><td>226.468019</td><td>231.259085</td><td>235.39401</td><td>239.424336</td><td>243.199836</td><td>246.843741</td><td>250.236207</td><td>253.234652</td><td>256.03563</td><td>258.782512</td><td>261.774998</td><td>264.580276</td><td>267.213506</td><td>269.921149</td><td>272.456241</td><td>274.885599</td><td>276.989321</td><td>278.825928</td><td>280.444455</td><td>282.111358</td><td>283.701795</td><td>285.055091</td><td>286.342619</td><td>287.241526</td><td>288.19072</td><td>289.239106</td><td>290.0659</td><td>290.916585</td><td>291.784479</td><td>292.835766</td><td>293.82904</td><td>294.963785</td><td>296.001406</td><td>297.14843</td><td>298.330913</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000002</td><td>0.000003</td><td>0.000006</td><td>0.000008</td><td>0.000018</td><td>0.00004</td><td>0.000078</td><td>0.000127</td><td>0.000179</td><td>0.000305</td><td>0.000457</td><td>0.000608</td><td>0.000761</td><td>0.000909</td><td>0.001088</td><td>0.001383</td><td>0.001672</td><td>0.00179</td><td>0.002011</td><td>0.002433</td><td>0.003017</td><td>0.003747</td><td>0.004449</td><td>0.005163</td><td>0.005965</td><td>0.006654</td><td>0.007281</td><td>0.007852</td><td>0.00856</td><td>0.009093</td><td>0.009912</td><td>0.010605</td><td>0.011021</td><td>0.01168</td><td>0.012362</td><td>0.012985</td><td>0.013253</td><td>0.013796</td><td>0.013865</td><td>0.014251</td><td>0.014363</td><td>0.014507</td><td>1.0053e-37</td><td>9.6761e-38</td><td>8.6073e-38</td><td>7.5284e-38</td><td>6.5289e-38</td><td>5.3307e-38</td><td>5.1958e-38</td><td>3.8087e-38</td><td>3.3045e-38</td><td>2.1781e-38</td><td>1.9838e-38</td><td>9.7569e-39</td><td>5.1639e-45</td><td>1.4710e-51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.6737e-54</td><td>7.1301e-49</td><td>2.3488e-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8.1064e-20</td><td>2.6918e-16</td><td>1.1465e-12</td><td>1.1626e-9</td><td>1.7548e-7</td><td>0.000002</td><td>0.000002</td><td>6.1742e-7</td><td>6.8795e-8</td><td>9.5800e-8</td><td>0.000002</td><td>0.000007</td><td>0.000006</td><td>0.000025</td><td>0.000022</td><td>0.000015</td><td>0.000032</td><td>0.000031</td><td>0.000022</td><td>0.000012</td><td>0.000031</td><td>0.000042</td><td>0.000029</td><td>0.000016</td><td>0.000014</td><td>0.000008</td><td>0.000002</td><td>0.0</td><td>4.8502e-7</td><td>5.4026e-8</td><td>1.1691e-7</td><td>3.1324e-8</td><td>6.2827e-12</td><td>6.5066e-12</td><td>6.7600e-12</td><td>7.5840e-12</td><td>7.5201e-12</td><td>8.9972e-12</td><td>6.4027e-12</td><td>4.1368e-12</td><td>2.1367e-12</td><td>1.2561e-12</td><td>8.6818e-13</td><td>3.2672e-13</td><td>1.7113e-19</td><td>1.2354e-25</td><td>1.0416e-23</td><td>9.7141e-19</td><td>4.8089e-17</td><td>0.0</td><td>5.4496e-9</td><td>1.2317e-7</td><td>2.4247e-9</td><td>1.4823e-7</td><td>5.7093e-7</td><td>0.000003</td><td>0.000003</td><td>0.000001</td><td>0.000001</td><td>0.000017</td><td>0.000034</td><td>0.000051</td><td>0.000047</td><td>0.000034</td><td>0.000021</td><td>0.000011</td><td>0.000006</td><td>9.4068e-7</td><td>8.3139e-8</td><td>1.2426e-7</td><td>3.8286e-7</td><td>8.5483e-8</td><td>8.1107e-8</td><td>2.4125e-8</td><td>3.4059e-8</td><td>4.9822e-8</td><td>6.3725e-8</td><td>8.6643e-8</td><td>9.9024e-8</td><td>1.0750e-7</td><td>9.7164e-8</td><td>8.5857e-8</td><td>7.7663e-8</td><td>6.7589e-8</td><td>1.3300e-9</td><td>0.0</td><td>0.0</td><td>5.1521e-13</td><td>2.2926e-11</td><td>4.1159e-11</td><td>6.6932e-11</td><td>4.8389e-14</td><td>26.766125</td><td>38.326466</td><td>44.364158</td><td>50.504799</td><td>40.427283</td><td>31.241199</td><td>30.402445</td><td>27.722321</td><td>20.383986</td><td>14.493661</td><td>9.758615</td><td>4.878546</td><td>2.683249</td><td>2.76098</td><td>2.963564</td><td>7.0875</td><td>15.348046</td><td>21.242244</td><td>26.108509</td><td>30.126443</td><td>35.599535</td><td>40.136581</td><td>41.652886</td><td>41.121061</td><td>38.686804</td><td>34.965231</td><td>31.105139</td><td>27.51296</td><td>24.737766</td><td>23.037146</td><td>21.860157</td><td>20.854756</td><td>19.693072</td><td>18.156475</td><td>16.349687</td><td>14.623607</td><td>12.867586</td><td>10.999232</td><td>9.042802</td><td>7.34352</td><td>5.893471</td><td>4.553944</td><td>3.375012</td><td>2.374191</td><td>1.438902</td><td>0.603676</td><td>-0.173665</td><td>-0.930428</td><td>-1.617219</td><td>-2.284518</td><td>-2.797098</td><td>-3.156353</td><td>-3.366056</td><td>-3.481461</td><td>-3.525668</td><td>-3.454358</td><td>-3.384439</td><td>-3.113365</td><td>-2.909109</td><td>-2.618813</td><td>-19.714897</td><td>-7.008516</td><td>5.098422</td><td>16.692305</td><td>6.039838</td><td>14.258133</td><td>0.530596</td><td>0.054057</td><td>-1.110178</td><td>-0.972205</td><td>0.754306</td><td>0.995024</td><td>0.239525</td><td>0.393146</td><td>1.070035</td><td>1.412289</td><td>2.565232</td><td>2.234606</td><td>0.058334</td><td>-5.735292</td><td>-10.932171</td><td>-12.986231</td><td>-12.884899</td><td>-12.573914</td><td>-11.847338</td><td>-10.107544</td><td>-6.941908</td><td>-2.603484</td><td>1.049267</td><td>3.723152</td><td>5.048724</td><td>5.296405</td><td>5.235892</td><td>4.808592</td><td>4.236725</td><td>3.625706</td><td>3.015351</td><td>2.605133</td><td>2.349206</td><td>2.155275</td><td>2.017206</td><td>1.857969</td><td>1.687204</td><td>1.401592</td><td>1.006144</td><td>0.514713</td><td>-0.013485</td><td>-0.566443</td><td>-1.0878</td><td>-1.582165</td><td>-2.035671</td><td>-2.415875</td><td>-2.685827</td><td>-2.897569</td><td>-3.029162</td><td>-3.13676</td><td>-3.149133</td><td>-3.058256</td><td>-2.957907</td><td>-2.749553</td><td>98463.738907</td><td>525.702085</td><td>132.107937</td><td>18.355525</td><td>0.061175</td><td>0.06211</td><td>0.373663</td><td>0.142697</td><td>0.183234</td><td>0.058991</td><td>0.094476</td><td>460.091223</td><td>0.0</td><td>0.401465</td><td>0.598535</td><td>0.0</td><td>3.0046e-7</td><td>5.4114e-7</td><td>9.6993e-7</td><td>0.000002</td><td>0.000003</td><td>0.000005</td><td>0.000009</td><td>0.000014</td><td>0.000015</td><td>0.000015</td><td>0.000014</td><td>0.000012</td><td>0.000009</td><td>0.000006</td><td>0.000003</td><td>0.000002</td><td>9.3332e-7</td><td>4.0457e-7</td><td>1.9842e-7</td><td>1.2794e-7</td><td>9.8775e-8</td><td>8.2898e-8</td><td>7.2434e-8</td><td>6.5700e-8</td><td>6.2111e-8</td><td>6.0836e-8</td><td>6.0226e-8</td><td>6.0869e-8</td><td>6.1481e-8</td><td>6.2038e-8</td><td>6.2637e-8</td><td>6.3359e-8</td><td>6.4105e-8</td><td>6.5292e-8</td><td>6.6596e-8</td><td>6.7845e-8</td><td>6.8833e-8</td><td>6.9828e-8</td><td>7.0558e-8</td><td>7.0732e-8</td><td>7.0900e-8</td><td>7.1004e-8</td><td>6.9089e-8</td><td>6.7275e-8</td><td>6.5551e-8</td><td>6.4353e-8</td><td>6.4043e-8</td><td>6.3750e-8</td><td>6.3476e-8</td><td>6.3262e-8</td><td>6.3129e-8</td><td>6.3004e-8</td><td>6.2884e-8</td><td>6.2481e-8</td><td>6.1659e-8</td><td>6.0837e-8</td><td>5.9535e-8</td><td>5.7958e-8</td><td>5.7958e-8</td><td>5.7958e-8</td><td>1.8050e-7</td><td>2.0730e-7</td><td>2.3782e-7</td><td>2.7224e-7</td><td>3.1057e-7</td><td>3.5252e-7</td><td>3.9751e-7</td><td>4.4472e-7</td><td>4.9324e-7</td><td>5.4230e-7</td><td>5.9139e-7</td><td>6.4024e-7</td><td>6.8876e-7</td><td>7.3681e-7</td><td>7.8414e-7</td><td>8.3035e-7</td><td>8.7502e-7</td><td>9.1783e-7</td><td>9.5869e-7</td><td>9.9766e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>9.9861e-7</td><td>3.3986e-8</td><td>4.2184e-8</td><td>5.2268e-8</td><td>6.4547e-8</td><td>7.9282e-8</td><td>9.6621e-8</td><td>1.1655e-7</td><td>1.3886e-7</td><td>1.6322e-7</td><td>1.8926e-7</td><td>2.1667e-7</td><td>2.4525e-7</td><td>2.7487e-7</td><td>3.0538e-7</td><td>3.3655e-7</td><td>3.6802e-7</td><td>3.9938e-7</td><td>4.3031e-7</td><td>4.6058e-7</td><td>4.9014e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td><td>4.9086e-7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (625_000, 557)\n",
       "\n",
       " sam  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  sta  pbu  pbu  pbu  pbu  pbu  pbu  cam  cam  cam  cam  cam  cam  cam  cam  cam  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu  pbu \n",
       " ple  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  te_  f_S  f_L  f_S  f_T  f_T  f_C  _in  _in  _in  _in  _in  _in  _in  _in  _in  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_o  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_C  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N  f_N \n",
       " _id  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  t_9  t_1  t_1  t_1  t_1  t_1  t_1  t_1  t_1  t_1  t_1  t_2  t_2  t_2  t_2  t_2  t_2  t_2  t_2  t_2  t_2  t_3  t_3  t_3  t_3  t_3  t_3  t_3  t_3  t_3  t_3  t_4  t_4  t_4  t_4  t_4  t_4  t_4  t_4  t_4  t_4  t_5  t_5  t_5  t_5  t_5  t_5  t_5  t_5  t_5  t_5  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  q00  u_0  u_1  u_2  u_3  u_4  u_5  u_6  u_7  u_8  u_9  u_1  u_1  u_1  u_1  u_1  u_1  u_1  u_1  u_1  u_1  u_2  u_2  u_2  u_2  u_2  u_2  u_2  u_2  u_2  u_2  u_3  u_3  u_3  u_3  u_3  u_3  u_3  u_3  u_3  u_3  u_4  u_4  u_4  u_4  u_4  u_4  u_4  u_4  u_4  u_4  u_5  u_5  u_5  u_5  u_5  u_5  u_5  u_5  u_5  u_5  v_0  v_1  v_2  v_3  v_4  v_5  v_6  v_7  v_8  v_9  v_1  v_1  v_1  v_1  v_1  v_1  v_1  v_1  v_1  v_1  v_2  v_2  v_2  v_2  v_2  v_2  v_2  v_2  v_2  v_2  v_3  v_3  v_3  v_3  v_3  v_3  v_3  v_3  v_3  v_3  v_4  v_4  v_4  v_4  v_4  v_4  v_4  v_4  v_4  v_4  v_5  v_5  v_5  v_5  v_5  v_5  v_5  v_5  v_5  v_5  ps   OLI  HFL  HFL  AUX  AUY  OSZ  _AL  _AL  _AS  _AS  _LW  _IC  _LA  _OC  _SN  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  zon  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  H4_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_  2O_ \n",
       " ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  01_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  02_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  03_  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    ---  N    X    X    ---  ---  RS   DIF  DIR  DIF  DIR  UP   EFR  NDF  NFR  OWH  e_0  e_1  e_2  e_3  e_4  e_5  e_6  e_7  e_8  e_9  e_1  e_1  e_1  e_1  e_1  e_1  e_1  e_1  e_1  e_1  e_2  e_2  e_2  e_2  e_2  e_2  e_2  e_2  e_2  e_2  e_3  e_3  e_3  e_3  e_3  e_3  e_3  e_3  e_3  e_3  e_4  e_4  e_4  e_4  e_4  e_4  e_4  e_4  e_4  e_4  e_5  e_5  e_5  e_5  e_5  e_5  e_5  e_5  e_5  e_5  0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59  \n",
       " str  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  f64  ---  ---  ---  f64  f64  ---  ---  ---  ---  ---  ---  AC   RAC  AC   LAN  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    0    1    2    3    4    5    6    7    8    9    ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  --- \n",
       "                                                        f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---                                                    f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64                                                    f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64       f64  f64  f64            f64  f64  f64  f64  f64  f64  ---  ---  ---  D    f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64 \n",
       "                                                                                                                                                                                                                                                                                                                  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      f64  f64  f64  ---                                                    f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64  f64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         f64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       " tes  209  220  227  241  254  262  261  254  243  236  229  225  221  215  208  203  199  198  196  198  201  205  209  214  219  224  228  233  237  241  246  249  253  256  260  263  266  269  271  274  276  278  279  281  282  283  284  285  286  287  288  288  289  290  291  292  293  294  295  296  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.7  9.7  9.5  9.0  8.8  8.7  7.4  5.7  4.1  2.5  9.0  8.8  3.9  5.2  3.8  0.0  0.0  0.0  0.0  0.0  0.0  1.2  0.0  0.0  0.0  0.0  0.0  6.3  8.4  1.0  2.3  5.3  0.0  1.7  1.9  5.1  0.0  3.3  5.4  5.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.4  7.4  2.4  7.5  3.2  3.5  3.9  4.7  5.1  5.5  5.5  5.0  4.1  3.0  1.2  1.2  5.5  8.9  2.3  0.0  1.6  5.6  8.7  3.3  1.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  1.7  6.1  9.7  3.2  2.4  7.7  1.6  2.6  5.0  5.2  3.7  7.3  5.2  3.7  3.7  2.3  2.7  6.0  1.4  4.6  0.0  3.0  1.9  2.0  1.2  1.2  -12  -35  -39  -26  -27  -26  -25  -24  -21  -18  -13  -7.  -2.  0.4  6.0  9.9  14.  20.  24.  28.  31.  34.  35.  35.  33.  31.  29.  27.  25.  23.  21.  20.  18.  16.  15.  13.  12.  11.  9.9  8.9  8.0  7.1  6.2  5.4  4.7  4.1  3.6  3.0  2.5  2.0  1.5  1.1  0.6  0.2  -0.  -0.  -1.  -1.  -1.  -1.  11.  6.0  11.  2.3  -3.  0.9  5.2  5.2  1.6  0.2  1.0  1.3  0.9  3.4  4.7  2.9  3.0  2.9  2.0  1.9  2.1  2.0  1.5  1.5  1.5  1.3  1.1  0.8  0.5  0.3  0.3  0.5  0.8  1.0  1.0  0.9  0.7  0.4  0.3  0.2  0.2  0.2  0.2  0.1  0.0  0.0  0.1  0.1  0.2  0.2  0.3  0.3  0.3  0.4  0.4  0.5  0.5  0.5  0.5  0.1  100  0.0  4.3  -0.  0.0  -0.  0.0  1.0  1.0  1.0  1.0  438  0.0  0.0  1.0  0.0  2.6  4.7  8.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.4  4.8  3.3  2.4  1.8  1.3  1.0  8.9  8.0  7.4  7.1  6.9  6.8  6.7  6.6  6.5  6.4  6.3  6.3  6.2  6.0  5.9  5.7  5.5  5.4  5.2  5.0  4.7  4.4  4.2  4.0  3.8  3.6  3.4  3.3  3.2  3.2  3.1  3.0  3.0  2.9  2.9  2.8  1.7  1.9  2.2  2.5  2.9  3.3  3.7  4.2  4.6  5.1  5.5  6.0  6.5  6.9  7.4  7.8  8.2  8.6  9.0  9.4  9.7  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  2.7  3.3  4.2  5.3  6.5  8.1  9.8  1.1  1.4  1.6  1.8  2.1  2.4  2.7  3.0  3.3  3.5  3.8  4.1  4.4  4.7  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_1  .80  .69  .78  .38  .60  .31  .30  .06  .90  .30  .97  .22  .04  .82  .99  .05  .04  .23  .02  .16  .19  .36  .87  .74  .54  .30  .95  .36  .65  .95  .02  .80  .45  .97  .29  .40  .45  .33  .93  .34  .36  .19  .77  .11  .21  .13  .58  .66  .68  .64  .31  .95  .68  .47  .37  .34  .43  .43  .38  .39  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  002  003  005  007  010  012  015  018  022  025  030  036  043  049  056  063  071  080  084  090  095  100  108  117  123  129  134  138  142  146  153  159  013  067  320  000  489  347  539  589  264  798  800  615  948  197  237                                248                           843  492  226  281  019       424  801  508       095  720  558            000  000  000  000  000  000  000  000  000  000  000  000  000  000  976  434  470  538  430  474  380  086  482  466  631  086  181  849  191  796  664  162  687       279  999  989  620  776                                          000  000  000  000  476  979  143  138  653  650  237  265  423  995  668  720  597  720  769  740  163  407  542  243  001       952  876  088  196  907  .62  .26  .28  .10  .02  .97  .68  .11  .75  .17  .10  057  783  827  971  447  382  153  696  106  409  470  623  262  688  477  394  362  418  586  805  051  329  628  029  527  198  006  460  864  468  550  987  704  675  921  544  921  461  538  821  254  726  108  256  681  069  349  312  116  938  370  908  858  281  800  559  414  898  146  398  178  711  721  068  373  556  771  583  352  346  245  546  813  788  842  284  486  405  491  760  674  443  546  909  892  312  758  051  695  565  709  187  021  211  684  373  814  285  812  189  464  862  376  929  250  192  278  264  696  600       472  246  015  000                           .75                      356  469  083  000  000  000  000  000  000  000  000  000  000  000  000  000  000  424  861  337  476  146  446  342  834  409  827  977  728  498  445  510  643  768  963  137  009  600  198  647  942  316  773  042  254  604  089  019  186  475  873  392  752  130  515  900  344  831  312  908  047  578  460  711  331  293  542  000  582  216  852  466  047  585  055  419  638  682  540  221  737  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  090  989  566  122  920  133  783  873  068  434  941  572  315  157  073  032  994  925  805  625  387  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 696  259  821  328  681  296  906  836  203  442  240  212  744  378  810  889  978  129  371  433  542  474  750  569  896  213  586  289  575  552  655  205  825  960  127  439  025  399  523  161  591  486  915  496  445  624  486  385  046  269  736  939  487  652  467  432  796  973  872  002  144  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   06   11   2    32   42   59   91   52   57   98   75   98   29   47   08   51   17   93   69   37   23   14   55   82   61   9    22   62   33   61   51   23   1    01   13   63   26   91   9    26   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-5                                e-4                           e-3  e-2  e-2  e-1  e-2       e-7  e-7  e-7       e-7  e-7  e-7            01   04   01   04   03   06   03   02   03   07   05   1    1    04   e-7  e-7  e-7  e-9  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2       e-2  e-2  e-1  e-1  e-1                                          01   02   01   02   e-7  e-7  e-7  e-8  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-9  e-9  e-1  e-1  e-1  e-1       e-1  e-1  e-1  e-1  e-1  109  499  828  145  725  712  103  690  385  445  203  726  299  7    61   5    375  91   414  28   991  184  454  472  433  991  856  967  579  494  095  06   192  743  511  132  934  181  34   05   04   62   09   31   71   04   44   11   25   15   54   12   71   09   035  038  693  754  553  497  681  94   263  34   934  46   62   53   35   83   91   28   6    31   99   7    34   36   52   15   94   51   7    69   97   52   16   05   27   01   2    14   42   74   06   44   3    87   24   1    86   5    26   73   38   25   98   8    99   41   75   98   31   31   04   97   03   45   96   98   .02       25   807  45   6                             855                      e-7  e-7  e-7  02   03   05   08   12   13   14   13   12   1    07   05   03   02   e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 51   3    3    9    2    2    3    8    5         1    3    4    5    4    9    2    3    5    5    1    2    5    9    4    1    5    1    2         1    5    5    8    8    7    8    5    5    1    7    7    1    9    7    2    1    6    3    4    8    2    4    2         1    8         4         6                                                                                                                                                                                                                                                                                                                7    7    7    7    7    7    7    7    7    7    8    9    3    9    5                                  9                             1    7    8    8    0                                                                                                                                                1    1    1    1    1    1    1    1    1    1    1    2    7    3    4         4    0    7    1    0                                                                                                                                                     0    1    1    2         3    3    3    4    2    8    4    6    7    1    4    3    4    4    8    9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     796                                                    4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  208  219  228  242  256  263  261  253  243  236  231  226  223  218  213  207  203  200  198  200  202  205  209  213  217  221  226  230  234  238  242  246  249  253  256  260  263  265  268  271  273  275  276  278  279  280  281  283  284  285  286  287  287  288  289  290  291  292  292  293  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.7  9.6  9.5  9.3  9.4  9.2  8.1  6.6  5.3  3.9  1.4  1.5  2.9  4.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.4  3.4  1.2  0.0  6.3  8.4  1.0  2.3  5.3  0.0  0.0  0.0  0.0  0.0  0.0  1.3  7.9  1.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.6  3.2  3.4  3.9  4.5  5.0  5.5  5.7  5.4  4.8  4.1  1.8  2.3  5.3  2.3  7.4  3.2  1.6  2.5  2.4  1.5  3.5  1.5  6.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.1  0.0  0.0  0.0  8.6  1.9  1.0  1.4  2.1  2.1  2.4  1.4  2.6  6.8  7.9  4.7  4.8  3.8  2.0  1.9  6.0  5.5  7.8  8.6  2.1  1.1  1.0  7.0  9.8  -29  -39  -34  -21  -18  -19  -19  -18  -15  -12  -8.  -3.  2.2  5.3  8.9  14.  18.  22.  25.  28.  31.  32.  32.  32.  31.  30.  29.  28.  27.  25.  24.  22.  21.  19.  18.  17.  16.  15.  14.  13.  12.  11.  10.  10.  9.4  8.8  8.3  7.7  7.2  6.8  6.4  6.0  5.7  5.4  5.1  4.9  4.6  4.4  4.3  3.5  3.4  7.9  14.  7.8  0.8  2.7  5.3  4.4  0.9  -1.  -0.  0.8  0.9  1.4  4.6  5.3  5.1  6.1  7.3  8.2  8.7  9.0  8.9  8.4  7.6  6.8  6.3  5.9  5.5  5.0  4.6  4.1  3.5  2.9  2.2  1.6  1.2  0.9  0.8  0.6  0.4  0.2  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -1.  -1.  101  0.0  4.3  -1.  -0.  0.0  0.0  1.0  1.0  1.0  1.0  420  0.0  0.0  1.0  0.0  2.6  4.7  8.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  6.0  4.2  3.2  2.4  1.7  1.2  1.0  9.0  8.1  7.6  7.3  7.0  6.8  6.7  6.6  6.4  6.3  6.2  6.0  5.8  5.7  5.5  5.3  5.2  5.0  4.8  4.5  4.3  4.1  3.9  3.7  3.5  3.3  3.2  3.1  3.0  3.0  2.9  2.9  2.9  2.8  2.8  1.6  1.9  2.2  2.5  2.8  3.2  3.7  4.1  4.5  5.0  5.5  5.9  6.4  6.8  7.3  7.7  8.1  8.5  8.9  9.2  9.6  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  2.5  3.2  4.0  5.0  6.3  7.7  9.5  1.1  1.3  1.5  1.8  2.0  2.3  2.6  2.9  3.2  3.5  3.7  4.0  4.3  4.6  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_5  .36  .23  .26  .09  .16  .78  .83  .63  .78  .96  .25  .82  .08  .78  .04  .42  .25  .94  .00  .70  .26  .75  .20  .18  .39  .70  .12  .24  .35  .46  .44  .28  .96  .49  .89  .03  .10  .96  .65  .11  .15  .05  .85  .32  .62  .89  .97  .15  .20  .30  .42  .16  .94  .71  .54  .33  .22  .07  .77  .55  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  002  003  004  006  008  011  014  017  021  026  031  036  043  049  055  062  068  072  078  082  086  089  093  099  105  109  113  118  123  128  135  140  233  757  398  217  038  493  177  361  593  227  220  365  856  280                                               922  044  520       843  492  226  281  019                                582  312  835  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  578  606  582  890  706  204  300  235  381  958  832  482  615  417  347  363  001  279  878  028  160  728  177  076                                          687                 779  253  758  760  158  672  495  108  630  928  143  101  511  275  314  337  377  262  513  035  053  164  425  639  122  .89  .55  .52  .92  .22  .48  .54  .39  .39  .17  241  019  689  477  205  489  426  032  706  965  166  408  771  556  808  882  846  652  302  846  259  670  183  844  700  666  707  681  583  460  467  563  787  096  708  975  348  660  499  114  307  740  488  444  647  166  894  547  328  686  074  275  541  704  513  006  131  246  470  047  371  492  349  964  863  750  837  104  737  737  835  703  064  173  643  644  868  449  246  894  633  878  884  098  545  566  027  450  163  911  929  107  034  223  304  306  312  318  329  361  399  439  485  515  548  609  656  768  045  761  156       900  169  011  052                           .97                      128  059  348  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  761  808  513  502  848  772  575  547  658  913  041  865  947  412  084  747  495  212  667  916  175  445  750  134  600  114  698  400  220  122  149  305  580  174  576  994  419  844  475  160  842  615  823  321  165  373  945  855  049  448  970  543  118  671  193  671  082  389  552  543  351  983  453  771  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  728  356  614  801  180  926  069  448  589  899  351  928  618  408  275  186  102  992  832  617  344  013  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 248  610  871  501  977  427  321  538  617  346  162  401  132  941  173  472  873  856  598  837  875  036  988  474  826  103  9    444  439  492  673  084  031  263  178  060  806  518  934  106  589  630  413  929  368  552  540  650  341  756  452  289  537  861  026  178  858  891  505  240  279  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   05   08   12   19   25   32   45   75   42   44   56   84   32   44   19   35   96   97   35   62   62   97   92   98   73   98   66   11   64   04   09   54   41   09   07   43   47        84   99   58   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-5                                               e-4  e-3  e-3       e-3  e-2  e-2  e-1  e-2                                e-7  e-8  e-7  02   12   37   2    14   33   27   29   05   07   22   28   15   13   07   02   01   03   e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-2  e-2  e-1  e-1  e-1  e-9  e-8  e-1                                          e-7                 e-1  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-9  e-8  e-9  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  336  484  252  828  378  342  360  817  236  546  326  894  72   9    04   345  973  316  922  752  77   526  722  432  454  596  462  634  33   441  796  197  674  425  619  287  244  082  848  464  512  934  628  707  2    09   15   36   28   81   5    11   27   43   2    59   4    72   37   85   41   05   661  68   83   54   24   51   4    635  438  79   26   84   53   17   25   77   43   74   96   92   93   56   39   43   36   48   17   4    17   5    59   8    11   92   01   96   15   56   47   38   511  913  56   024  83   147  014  018  833  263  964  237  946  114  643  443  759  068  .33       58   037  148  13                            127                      e-7  e-7  e-7  01   03   04   07   12   13   14   13   12   1    07   05   03   02   01   e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 62   1    1    7    2         1    4    2    7    5    8    8    6    6    9    8    9    8    7    1    9    7    6    4    8         9         7    2    3    8    2    4    7    2    3    5    7    5    4         9    3    6    6    3    9    3    7    1    3    9    5    6    2    7    1    5    6                                                                                                                                                                                                                                                                                                                7    7    7    7    7    7    7    7    7    7    7    8    3    1                                                 0    5    0         1    7    8    8    0                                                                                                                                                1    1    1    1    1    1    1    1    1    1    1    2    7    5    5    0    4    4    2    0              0                                                                0                                                                               0    1    2    0    2    2    2    4    3    1    4    4    9    7         6    9    8    4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          813                                                    5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  213  229  233  242  252  260  260  253  240  232  226  221  217  213  206  197  190  189  191  192  198  204  210  216  222  228  233  238  243  247  251  255  258  261  265  268  270  272  275  277  279  280  281  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.7  9.6  9.4  8.6  7.4  7.0  5.8  3.7  1.8  2.6  1.5  4.6  2.0  1.7  6.7  0.0  0.0  0.0  0.0  0.0  1.0  7.5  1.6  2.8  1.5  5.5  1.2  8.0  2.1  5.8  1.3  3.2  8.0  2.9  1.7  2.0  0.0  0.0  6.1  5.5  5.5  8.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.7  1.5  1.9  1.7  3.1  3.5  3.9  4.6  4.7  4.9  4.6  3.4  2.3  6.1  2.7  7.5  2.6  2.6  5.1  2.6  9.8  3.8  1.0  1.0  1.5  1.9  6.5  0.0  0.0  0.0  0.0  0.0  0.0  8.7  0.0  0.0  0.0  6.3  5.3  2.7  2.3  2.4  1.2  1.0  0.0  1.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.8  0.0  0.0  0.0  0.0  0.0  16.  -11  -24  -20  -37  -43  -43  -41  -38  -33  -25  -18  -14  -9.  -2.  1.0  5.2  10.  13.  17.  22.  27.  28.  27.  24.  20.  16.  13.  10.  8.3  6.7  5.5  4.3  3.1  2.0  0.9  0.0  -0.  -0.  -1.  -1.  -1.  -2.  -2.  -3.  -3.  -3.  -4.  -4.  -5.  -5.  -5.  -5.  -6.  -6.  -6.  -6.  -6.  -6.  -5.  20.  5.2  4.4  -3.  -5.  -0.  5.0  5.3  1.7  0.7  2.0  1.6  0.5  2.8  3.7  3.0  2.6  1.5  -0.  -2.  -4.  -5.  -6.  -5.  -3.  -2.  -1.  -1.  -1.  -2.  -2.  -1.  -1.  -0.  -0.  0.2  0.3  0.3  0.4  0.5  0.7  0.8  0.9  1.0  1.0  1.0  1.0  1.0  1.0  0.9  0.9  0.8  0.8  0.7  0.7  0.7  0.7  0.7  0.7  0.8  991  0.0  70.  -0.  0.0  -0.  0.0  1.0  1.0  1.0  1.0  455  0.0  0.2  0.7  0.0  2.7  4.9  8.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.4  3.2  2.1  1.4  1.0  8.7  7.7  7.4  7.2  7.1  7.0  7.0  7.0  7.0  7.0  7.0  7.0  6.9  6.7  6.6  6.4  6.3  6.1  5.8  5.4  5.2  4.8  4.4  4.1  3.7  3.5  3.3  3.1  2.9  2.8  2.7  2.6  2.6  2.5  2.4  2.4  2.4  2.4  1.7  2.0  2.3  2.6  3.0  3.4  3.8  4.3  4.8  5.3  5.7  6.2  6.7  7.2  7.6  8.1  8.5  9.0  9.4  9.7  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.1  3.9  4.8  6.0  7.4  9.0  1.1  1.3  1.5  1.8  2.0  2.3  2.6  2.9  3.2  3.5  3.8  4.1  4.4  4.7  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_6  .07  .42  .36  .68  .91  .95  .97  .05  .92  .91  .80  .93  .64  .24  .34  .76  .74  .53  .34  .50  .06  .08  .65  .96  .76  .26  .52  .41  .08  .40  .55  .27  .69  .98  .12  .07  .62  .98  .17  .32  .23  .73  .94  .24  .54  .84  .92  .87  .97  .86  .71  .69  .54  .17  .03  .01  .04  .15  .15  .14  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  001  003  004  006  007  010  011  013  014  017  021  027  033  038  043  050  062  075  083  090  098  107  114  121  128  131  135  143  150  155  159  162  166  173  196  819  737  130  553  430  079  533  347  609  455  986  639  214  973                           323  774  654  840  806  908  081  682  577  245  243  364  410  317  491  769  000  000  060  421  637  581  000  000  000  000  000  000  000  000  000  000  000  000  000  000  045  553  860  795  575  242  756  787  899  267  184  904  260  475  373  081  179  664  042  412  686  788  422  476  938  242  540  000  000  000  000  000  000  136  000  000  000  892  047  988  266  639  039  484       023                                                              992                           386  .51  .81  .79  .66  .57  .41  .31  .49  .17  .49  .30  .22  429  728  835  465  596  751  168  897  350  289  581  350  201  367  093  395  392  831  107  299  965  718  491  713  575  958  161  364  642  055  538  052  529  950  343  695  011  323  662  971  224  444  645  783  794  425  456  983  496  796  157  787  279  393  442  703  143  770  394  899  069  287  329  877  516  080  551  148  943  167  431  508  183  755  703  861  088  089  826  271  607  032  874  641  894  369  545  255  768  852  269  470  802  904  642  271  947  456  698  139  656  399  221  251  586  801  218  86.       755  892  332  005                           .79       981  018       551  620  940  000  000  000  000  000  000  000  000  000  000  000  000  000  000  335  919  876  950  933  582  536  163  076  080  781  635  695  802  773  642  367  130  862  464  919  383  233  041  994  104  473  674  062  635  023  181  460  850  331  395  727  068  409  823  685  545  471  700  328  320  696  454  568  980  609  367  178  992  782  540  251  892  424  804  003  009  831  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  350  069  601  254  293  874  000  148  501  024  686  469  359  342  395  483  566  609  593  509  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 341  377  155  845  378  278  398  731  988  989  791  359  733  686  573  662  253  862  427  047  400  498  369  925  595  587  443  828  498  483  786  908  033  243  807  583  896  784  131  815  678  369  774  689  825  198  623  888  416  811  487  582  913  741  027  824  840  993  961  971  650  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   01   01   01   02   04   09   23   49   79   29   97   01   21   15   99   1    95   34   78   18   79   6    17   39   86   77   77   19   27   56   29   64   35   43   5    19        71   46   51   51   06   89   09   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-5                           e-5  e-4  e-4  e-3  e-3  e-2  e-2  e-2  e-1  e-1  e-8  e-7  e-7  e-7  e-7  e-7  03   04   e-7  e-7  e-7  e-7  06   04   03   04   06   75   17   14   18   07   04   03   02   02   e-7  e-7  e-8  e-9  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1  e-1  e-1  e-9  e-8  e-7  e-7  02   02   06   09   09   06   e-7  03   03   02   e-7  e-7  e-7  e-7  e-8  e-9  e-9       e-9                                                              e-1                           828  245  713  656  945  750  409  412  889  397  320  158  911  682  357  7    48   983  444  239  938  709  143  126  959  542  193  764  265  74   11   04   76   51   99   27   86   528  757  323  285  227  358  765  665  975  433  366  296  322  996  009  492  755  167  438  85   718  409  625  392  65   47   389  299  474  68   92   46   79   56   21   09   33   32   48   94   57   055  87   039  878  202  529  39   702  617  193  061  447  375  21   17   347  128  5    68   45   52   93   87   4    13   65   64   41   79   47   41   94   11   93   89   46        11   07   54        63   272       197  743  05   67                            343       89   11        e-7  e-7  e-7  02   03   05   08   13   14   15   15   13   1    07   04   02   01   e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 29   1    1    5    8    6    1         5              8    3         5    7    2    6    8    2    4    5    9    6    4    2    2    4         9    4    2         9    5         4    9    7    8    4    8    9    3         1    7    6    4         5    2    3    2    4    1    6    9    5    9    1                                                                                                                                                                                                                                                                                                                7    7    7    7    7    7    7    7    7    8    8    9    2    8    4                             2    8    2    7    2    8    4    0    5    1                                                                                                                                                          1    1    1    1    1    1    1    1    1    2    2    3    6    2    5    8    4    1    0                                                                                                                                                                                   2                                  8    1    3    4    6    6    9    9    3    9    5    8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           487                                                    9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       " tes  212  226  230  241  252  262  261  253  241  233  227  222  218  214  208  201  194  191  191  193  198  204  210  215  221  226  231  236  241  245  249  253  257  260  263  266  269  271  274  276  278  279  281  282  284  285  286  287  288  289  290  291  292  293  293  294  295  296  297  298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.7  9.6  9.4  8.5  7.7  7.7  6.7  4.5  2.4  5.1  2.5  6.4  8.9  1.0  5.2  0.0  0.0  0.0  0.0  0.0  2.6  2.2  6.9  0.0  0.0  0.0  8.0  6.2  1.6  4.5  1.0  1.4  7.4  7.5  3.2  9.1  0.0  0.0  0.0  8.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.8  2.0  1.8  3.1  3.4  3.9  4.7  4.9  5.1  5.1  4.1  2.8  1.0  4.0  1.0  7.9  1.2  1.6  0.0  2.6  8.6  1.6  7.3  5.2  1.7  1.6  0.0  0.0  0.0  8.3  0.0  0.0  0.0  0.0  2.1  6.5  0.0  3.9  2.4  6.2  1.5  4.4  0.0  0.0  0.0  8.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  -0.  -29  -34  -26  -32  -37  -40  -38  -34  -29  -23  -16  -9.  -5.  0.2  6.7  11.  17.  21.  26.  31.  32.  32.  30.  27.  24.  22.  19.  17.  15.  13.  11.  9.9  8.1  6.5  5.2  4.2  3.4  2.7  2.0  1.3  0.7  0.2  -0.  -0.  -1.  -1.  -1.  -2.  -2.  -2.  -2.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  13.  4.8  15.  4.3  -4.  -1.  4.3  5.3  1.5  -0.  1.3  2.5  0.5  0.2  3.3  3.9  4.2  4.1  4.0  4.2  4.6  5.8  6.4  6.6  6.0  4.9  4.0  3.4  3.0  2.8  2.8  2.7  2.5  2.1  1.6  1.1  0.8  0.6  0.5  0.5  0.4  0.2  0.0  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  101  0.0  80.  5.5  0.0  0.0  0.0  1.0  1.0  1.0  1.0  460  0.0  0.0  1.0  0.0  2.7  4.9  8.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.7  3.5  2.5  1.8  1.3  9.8  8.0  7.5  7.2  7.0  7.0  7.0  7.1  7.1  7.2  7.2  7.1  7.1  7.0  6.8  6.6  6.5  6.2  5.9  5.5  5.2  4.7  4.2  3.7  3.3  3.1  3.0  2.9  2.8  2.7  2.6  2.6  2.5  2.5  2.4  2.4  2.4  2.4  1.7  2.0  2.3  2.6  3.0  3.4  3.8  4.3  4.7  5.2  5.7  6.2  6.6  7.1  7.6  8.0  8.4  8.9  9.3  9.6  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.0  3.7  4.6  5.8  7.1  8.8  1.0  1.2  1.5  1.7  2.0  2.2  2.5  2.8  3.1  3.4  3.7  4.0  4.3  4.6  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_4  .04  .00  .98  .05  .82  .18  .90  .18  .18  .42  .55  .87  .35  .29  .69  .11  .08  .57  .43  .61  .68  .36  .18  .80  .31  .57  .73  .62  .20  .49  .65  .51  .17  .61  .84  .71  .36  .65  .01  .07  .18  .56  .09  .39  .09  .41  .76  .86  .82  .78  .64  .33  .20  .04  .90  .53  .25  .30  .49  .72  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  001  002  003  003  004  006  008  011  015  021  028  035  041  047  058  070  081  087  094  099  103  107  110  116  125  131  136  143  152  160  164  165  166  321  702  260  773  302  039  762  470  676  243  684  836  827  076  435                           641  396  463                 829  168  750  542  455  923  443  741  564  671  000  000  000  192  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  136  159  606  498  882  500  076  022  808  092  047  809  113  288  157  655  778  981       637  123  406  944  734  848  908  000  000  000  423  000  000  000  000  885  565  000  603  611  521  304  849                 317                                                                                       895  .09  .20  .29  .09  .21  .73  .77  .37  .95  .99  .31  717  701  822  108  787  137  384  894  373  788  527  910  796  692  061  825  776  818  885  877  191  203  523  702  656  455  018  102  532  505  500  209  618  001  411  795  144  465  706  893  064  239  402  687  894  934  892  790  161  682  499  046  137  489  185  869  368  516  753  564  841  866  240  947  137  468  861  755  855  381  435  910  570  677  697  033  537  736  587  637  482  332  601  544  082  200  613  176  252  577  128  239  408  510  610  685  777  891  998  148  303  469  610  783  781  651  540  441  096       041  958  204  067                           .81                      210  007  841  000  000  000  000  000  000  000  000  000  000  000  000  000  000  756  455  498  384  208  328  853  393  067  594  437  536  091  728  022  139  999  021  019  628  962  306  554  008  624  410  203  258  556  094  591  384  257  202  346  792  253  721  189  827  477  131  187  527  129  092  434  156  230  598  182  893  658  424  168  878  544  140  627  964  122  089  873  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  161  656  929  286  990  206  694  801  113  595  217  960  812  760  778  833  886  902  860  753  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 035  863  386  060  618  963  577  074  719  631  780  160  035  556  387  633  957  237  424  963  432  012  157  116  456  720  277  690  780  245  183  132  150  599  964  016  538  056  341  747  255  162  668  230  622  860  697  609  135  794  314  174  108  081  151  280  867  198  872  353  311  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   06   11   22   42   61   85   1    7    44   22   95   91   27   33   11   5    22   7    17   73   84   47   88   75   92   77   74   89   89   92   04   46   01   55   22   4    34   53   92   83   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-5                           e-5  e-4  e-4                 e-2  e-2  e-1  e-1  e-9  e-8  e-8  e-7  e-7  e-7  05   03   07   e-7  08   07   73   54   11   16   2    12   05   04   01   21   26   23   17   11   03   e-7  e-7  e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2       e-1  e-1  e-1  e-9  e-8  e-7  e-7  03   05   03   e-7  02   05   03   02   e-7  e-8  01   e-7  e-7  e-7  e-7  e-8                 e-9                                                                                       688  759  382  252  222  264  699  608  860  886  579  366  851  623  12   35   517  322  466  039  947  974  44   859  946  624  817  191  414  793  651  284  14   61   6    32   99   89   8    72   32   37   07   561  921  443  117  138  735  519  805  646  147  915  748  885  445  927  879  526  184  9    55   59   783  619  81   5    53   866  16   8    75   61   75   14   56   25   54   49   94   7    13   64   19   96   45   13   14   47   52   81   21   87   12   26   14   16   83   67   16   51   08   181  705  568  817  114  895  026  847  757  713  805  964  924  503  063  45   145  .42       768  57   11   81                            059                      e-7  e-7  e-7  02   03   05   08   12   14   15   14   13   1    07   04   02   01   e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 72   6    4    9    7    4         5    6    1    1    2    7    4    8    2    9    3    4    4    5    6    6    2    1    5    3    7    9    8    8    9    2    8              6    2    3    5    7    9    4    2    8    6    7         9    2    5         5    9    2    7    5    6    2    8    5                                                                                                                                                                                                                                                                                                                7    7    7    7    7    7    7    7    7    8    8    9    3    7    4                             3    8    4                   6    1    6    2                                                                                                                                                          1    1    1    1    1    1    1    1    1    1    2    2    7    1    4         4    2    1                                                                                                                                                                                                                      2    2    9    9    9    5    4    8    7    7    7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                541                                                    4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  207  216  227  243  257  265  262  253  243  236  231  227  224  221  216  211  207  204  200  202  202  205  208  211  214  218  222  226  230  235  238  242  246  250  253  256  259  262  265  267  269  271  273  275  276  277  278  280  281  282  283  284  285  286  287  287  288  289  290  291  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.7  9.6  9.5  9.5  9.6  9.5  8.6  7.1  6.0  4.8  1.8  2.0  1.0  1.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.7  3.3  3.3  1.5  5.2  0.0  0.0  0.0  0.0  2.4  3.7  0.0  4.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.8  2.7  3.2  3.4  4.0  4.4  4.8  5.4  5.7  5.6  5.2  4.8  2.4  3.2  1.6  1.1  0.0  0.0  0.0  0.0  4.9  1.2  4.1  5.1  6.2  4.8  8.5  1.5  2.0  3.8  8.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.1  6.5  5.6  1.6  1.6  1.3  0.0  0.0  1.6  1.4  6.6  3.7  5.3  1.6  -40  -40  -31  -18  -12  -14  -15  -13  -10  -7.  -5.  -1.  3.7  8.5  11.  15.  20.  23.  26.  29.  31.  31.  32.  32.  31.  30.  29.  28.  27.  25.  24.  23.  22.  21.  21.  20.  18.  17.  16.  15.  14.  13.  12.  11.  11.  10.  9.8  9.3  8.8  8.4  8.1  7.7  7.4  7.1  6.8  6.5  6.3  6.2  5.8  3.8  -0.  5.8  13.  11.  5.1  3.8  4.5  3.1  0.3  -1.  -1.  -0.  0.6  0.2  1.1  4.2  5.2  5.7  6.6  7.8  8.3  8.4  8.2  7.8  7.4  7.1  6.8  6.4  5.8  5.1  4.3  3.4  2.7  2.2  1.8  1.6  1.4  1.2  0.9  0.5  0.0  -0.  -1.  -1.  -1.  -2.  -2.  -3.  -3.  -3.  -3.  -4.  -4.  -4.  -4.  -4.  -4.  -4.  -5.  -5.  101  0.0  11.  -2.  -0.  0.0  0.0  1.0  1.0  1.0  1.0  407  0.0  0.0  1.0  0.0  2.6  4.6  8.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  6.6  4.6  3.5  2.7  2.0  1.4  1.1  9.5  8.4  7.9  7.4  7.2  7.0  6.8  6.6  6.5  6.3  6.2  6.0  5.8  5.7  5.5  5.3  5.1  4.9  4.7  4.4  4.2  4.0  3.8  3.6  3.4  3.3  3.1  3.1  3.0  3.0  2.9  2.9  2.9  2.8  2.8  1.6  1.9  2.1  2.5  2.8  3.2  3.6  4.1  4.5  5.0  5.4  5.9  6.3  6.8  7.2  7.6  8.0  8.4  8.8  9.2  9.5  9.8  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  2.4  3.1  3.9  4.9  6.1  7.5  9.2  1.1  1.3  1.5  1.7  2.0  2.3  2.5  2.8  3.1  3.4  3.7  4.0  4.2  4.5  4.8  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_4  .92  .69  .82  .55  .49  .29  .88  .70  .45  .89  .65  .56  .41  .18  .80  .96  .47  .36  .80  .55  .75  .46  .13  .35  .99  .81  .90  .87  .92  .01  .95  .83  .55  .10  .43  .55  .53  .38  .07  .55  .74  .38  .09  .01  .45  .96  .97  .09  .15  .39  .62  .62  .48  .26  .13  .89  .75  .68  .58  .27  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  001  002  003  005  007  009  012  015  019  023  027  032  037  045  050  054  059  062  065  070  074  076  080  083  087  092  095  100  105  110  113  118  128  051  452  362  512  605  557  741  128  712  578  251  646  382                                                         811  669  602  452  629                      444  406       136  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  231  059  707  833  651  067  119  224  644  434  800  358  067  542  799  093                      935  625  767  563  808  014  056  604  746  882  910  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  042  356  581  544  084  615            286  384  560  114  211  815  .51  .03  .82  .52  .83  .17  .04  .74  .58  645  238  475  027  420  115  118  132  884  822  354  015  800  027  053  697  933  808  541  194  891  681  639  746  928  076  061  849  503  179  007  075  291  557  871  154  471  891  470  792  805  099  694  509  456  510  795  718  621  186  053  848  097  565  622  921  320  215  576  644  681  608  285  598  528  247  570  244  101  796  533  949  250  186  648  525  431  866  793  976  571  282  962  861  407  787  557  962  962  782  253  134  525  048  479  861  312  749  113  449  714  954  173  351  488  584  650  742  905  619  524  283       273  447  020  308                           .40                      040  899  061  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  758  964  960  383  008  038  380  438  788  227  739  256  076  281  685  100  685  235  612  859  117  244  288  422  651  129  702  394  204  262  457  770  193  927  338  764  197  631  297  010  719  492  681  158  978  159  701  578  736  098  582  116  652  167  651  091  465  735  863  820  596  198  638  928  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  879  335  392  344  457  905  726  179  285  559  976  519  175  931  766  646  533  395  210  971  676  324  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 845  609  734  159  501  852  425  689  213  369  184  155  529  584  496  906  263  477  333  428  814  009  502  513  207  225  660  796  96   690  568  799  895  266  023  465  874  158  753  465  576  391  381  660  915  490  139  562  628  836  887  094  805  523  746  615  663  309  756  775  449  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   04   07   1    14   18   22   3    43   69   17   82   65   74   15   18   6    45   63   48   65   93   36   83   26   84   18   33   33   91   64   59   69   08   62   96   1    81   69   81   49   45   4    e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4                                                         e-4  e-3  e-2  e-2  e-1                      e-8  e-8       e-7  03   03   04   22   83   24   24   23   19   39   5    19   11   12   1    08   13   13   08   02   e-7  e-7  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2                      e-1  e-1  e-9  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  01   01   02   01   01   01   01   02   02   03   03   03   03   03   02   01   01   e-7  e-7  e-9  e-8  e-8  e-9            e-1  e-1  e-1  e-1  e-1  e-1  467  241  841  496  023  348  724  384  702  994  524  789  35   01   44   564  157  062  964  526  959  139  682  957  876  278  729  658  826  485  64   124  605  873  565  099  762  729  417  115  261  599  146  78   768  049  61   19   16   35   97   93   09   93   57   24   34   67   72   06   759  36   142  131  77   22   55   26   5    811  787  473  45   71   26   13   26   01   22   95   54   32        57   11   94   08   43   73   22   24   79   17   91   76   45   2    32   17   21   48   427  021  973  84   476  788  212  63   691  645  306  14   054  904  776  572  684  688  507  .48       749  222  809  42                            612                      e-7  e-7  e-7  01   03   04   07   12   13   13   13   11   1    07   05   03   02   01   e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 78   7    8         1    5    7    2    2    5    1    4    2    7    7    6    9    3    3    2    5    7    3         2    3    2    8         6    9    9    1    3    3    3    6    8    9    4    6    3         3         7    9    8    5    7    2    1         1    2    2    6    1    9         9                                                                                                                                                                                                                                                                                                                7    7    7    7    7    7    7    7    7    7    7    8    2    9                                                           0    3    9    3    8                                                                                                                                                          1    1    1    1    1    1    1    1    1    1    1    2    6    3                        3    1                                                                                                                                                                              1    6    2    0    0    0    1    7    9    7    5    6    4    3    1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               235                                                    1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       " tes  216  228  236  248  256  262  257  247  237  229  223  218  214  210  203  199  190  187  189  192  198  204  211  216  222  229  234  238  242  246  249  253  256  259  261  263  265  268  271  274  276  278  279  280  281  282  284  285  286  287  288  289  290  291  291  292  293  295  296  297  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  9.6  8.5  7.4  6.6  5.5  5.2  3.8  3.3  1.9  1.7  9.5  5.7  1.2  5.4  0.0  0.0  0.0  0.0  0.0  9.9  2.2  3.0  3.2  7.4  3.0  1.3  1.9  5.8  1.2  6.1  1.8  0.0  0.0  6.4  5.2  7.0  0.0  0.0  0.0  0.0  6.5  6.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.2  0.0  0.0  5.3  4.9  1.4  5.0  6.1  6.5  6.8  7.7  8.1  8.3  6.3  4.2  2.4  1.3  7.4  3.4  1.9  4.1  5.6  3.4  5.5  8.8  1.0  8.7  2.6  0.0  0.0  3.5  0.0  0.0  0.0  0.0  0.0  0.0  8.7  6.0  2.2  2.2  3.2  6.0  1.1  1.9  1.5  2.1  1.5  1.3  6.8  7.3  3.4  6.8  3.6  2.8  9.6  2.1  0.0  0.0  4.3  8.7  2.2  0.0  0.0  0.0  0.0  0.0  51.  42.  37.  35.  34.  27.  29.  22.  14.  8.4  6.6  6.8  4.7  0.1  -1.  2.4  7.7  15.  26.  34.  40.  46.  48.  48.  48.  47.  46.  43.  39.  33.  28.  24.  22.  20.  18.  16.  15.  13.  12.  10.  8.4  6.6  5.0  3.9  3.3  2.5  1.6  0.7  -0.  -0.  -1.  -2.  -3.  -4.  -4.  -4.  -4.  -4.  -4.  -4.  18.  -22  -17  4.9  3.7  10.  -5.  -4.  -1.  0.7  1.5  1.2  1.4  0.5  -1.  -1.  -3.  -6.  -9.  -9.  -11  -5.  5.4  8.2  6.3  2.5  -2.  -7.  -11  -11  -10  -6.  -3.  -0.  0.7  2.2  3.4  3.7  3.5  2.9  2.4  2.1  2.4  2.7  2.5  2.2  1.9  1.8  1.6  1.5  1.5  1.4  1.2  1.0  0.3  0.0  -0.  -0.  -0.  -0.  100  898  98.  5.0  0.0  0.0  0.6  0.0  0.0  0.0  0.0  452  0.0  0.0  1.0  0.0  3.0  5.4  9.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.2  1.8  1.1  8.8  7.2  6.2  5.6  5.5  5.5  5.6  5.7  5.8  6.0  6.2  6.3  6.5  6.6  6.7  6.7  6.7  6.6  6.5  6.4  6.3  6.2  6.0  5.9  5.8  5.7  5.7  5.6  5.6  5.6  5.6  5.6  5.6  5.6  5.6  5.7  5.7  5.6  5.6  5.6  1.8  2.0  2.3  2.7  3.1  3.5  3.9  4.4  4.9  5.4  5.9  6.4  6.9  7.4  7.8  8.3  8.7  9.2  9.6  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.4  4.3  5.3  6.5  8.0  9.8  1.1  1.4  1.6  1.9  2.1  2.4  2.7  3.0  3.3  3.7  4.0  4.3  4.6  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_5  .89  .91  .65  .99  .38  .36  .79  .51  .79  .43  .84  .98  .45  .25  .82  .01  .87  .29  .28  .70  .79  .75  .28  .76  .77  .00  .25  .51  .56  .31  .83  .30  .47  .04  .27  .22  .78  .67  .68  .39  .69  .63  .65  .50  .79  .94  .16  .33  .56  .63  .56  .56  .32  .00  .72  .79  .87  .05  .26  .49  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  002  004  004  003  003  002  003  006  011  019  025  029  030  031  033  035  042  054  065  072  080  087  089  092  098  102  109  119  130  134  138  139  139  140  255  831  105  848  743  362  533  248  658  240  839  738  609  903  957                           242  527  741  172  653  357  435  898  084  265  973  722            923  863  171  000  000  000  000  247  122  000  000  000  000  000  000  000  000  000  000  283  000  000  185  293  069  483  028  125  174  943  095  080  072  078  430  018  322  289  021  825  206  514  620  530  012  511  333            540  000  000  000  000  000  000  206  768  670  869  742  206  166  000  679  275  721  016  575  267  838  420  097  057  009  185            523  768  667                           209  419  281  648  451  809  607  240  340  704  837  570  378  683  061  119  352  792  320  285  828  740  625  333  036  471  671  513  277  958  955  944  012  014  198  515  106  656  162  343  655  114  512  861  115  292  528  810  040  972  912  710  392  092  537  577  500  448  368  239  015  .50  .31  184  657  192  364  811  973  201  112  775  268  147  303  287  506  756  073  815  .01  496  248  626  256  711  623  753  .24  .85  .22  929  456  885  845  803  677  678  059  427  050  770  063  659  917  398  785  103  712  977  153  318  408  981  684  581  301  440  531  596  806  .33  588  338  236  031  385  6    481  6    481  .46                      087  188  127  000  000  000  000  000  000  000  000  000  000  000  000  000  000  125  415  659  404  384  100  742  711  654  333  553  866  504  214  771  290  536  011  497  260  603  950  816  419  086  839  843  898  999  167  854  560  286  029  050  322  588  850  079  031  982  929  862  144  838  905  365  218  435  957  702  579  511  445  356  232  062  819  464  954  259  366  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  774  110  352  808  738  287  843  097  555  180  942  819  798  866  997  158  306  409  445  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 782  081  196  469  930  063  062  187  648  522  697  084  714  515  341  449  891  713  040  349  395  049  373  941  731  148  249  177  507  807  885  27   477  925  202  533  004  625  643  322  762  573  943  286  606  704  652  368  840  037  629  920  361  391  669  129  075  226  116  222  587  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   04   07   11   33   83   59   92   07   31   65   01   98   8    84   78   32   19   05   77   99   6    08   23   04   93   6    64   69   92   76   07   32   99   14   87   28   01   12   56   4    e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-5                           e-5  e-4  e-4  e-4  e-3  e-3  e-2  e-2  e-1  e-1  e-1  e-8            e-8  e-7  e-7  01   02   03   03   e-8  e-8  02   52   06   17   04   02   1    11   11   06   e-7  02   01   e-7  e-7  e-8  e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1  e-8  e-7  e-8  e-1            e-7  02   04   02   03   02   02   e-7  e-9  e-9  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-1  e-9  e-9  e-9  e-1            e-1  e-1  e-1                           186  439  976  045  416  329  586  679  837  71   59   6    29   45   041  94   85   098  675  7    029  835  057  362  965  239  519  668  116  462  049  975  148  659  438  235  666  904  052  934  71   52   28   9    1    46   07   19   192  076  147  149  743  902  605  928  485  912  188  9    506  621  347  76   59   913  113  059  06   58   61   31   16   34   389  716  73   223  841  227  597  088  86   81   5    59   422  617  074  661  719  758  147  487  14   07   87   92   61        72   2    74   9    51   6    71   35   7    33   46   27   66   05   6    9    357  503  31   656  .04  628  601  12   82   76   27        38        38   519                      e-7  e-7  e-7  02   03   05   09   14   15   15   15   12   09   06   03   02   01   e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 20   5    9    4    1    1    9    3    8    6    3    8    9    2    2    3    6    1    4    3    3    7         5    9    4    8    5    2    7    5              6    1    4    8    9    3    8    7         7    4    3    2    4    2    8    2    9    1    4    8    2    3    9    6    7    6    8                                                                                                                                                                                                                                                                                                                7    8    8    8    8    8    8    8    8    8    8    9    4    7    4                             3    8    4    0    7    2    7    3    9    4    1                                                                                                                                                     2    2    2    2    2    2    2    2    2    2    3    3    8    2    2    6    0                   0                                                                                                                            0                   1              2    0    1                                                                                                                                                                                                                                                                                                                                                                                                                                             7                                       6    4    5                                                                                                                                                     292  6                                                 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  215  229  237  246  259  260  258  253  238  228  223  220  216  210  203  194  188  184  186  191  198  204  209  215  222  228  234  239  244  248  251  254  257  261  263  266  270  273  276  279  281  282  282  283  284  285  286  287  288  289  290  291  292  292  293  294  295  296  297  298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.4  8.2  8.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  9.5  8.4  7.8  6.7  5.3  4.2  3.2  1.7  4.7  1.2  5.4  9.5  1.8  4.0  0.0  0.0  0.0  0.0  0.0  0.0  1.2  2.4  2.7  1.6  4.6  1.5  3.3  6.6  6.7  3.6  2.8  5.0  2.7  1.3  3.3  0.0  0.0  0.0  0.0  0.0  2.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.9  2.8  8.3  6.1  6.5  7.6  9.8  8.5  9.6  7.4  6.0  3.9  1.9  9.4  1.3  6.3  1.0  1.6  1.9  1.1  2.2  7.3  1.3  6.2  1.2  3.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  6.0  8.9  1.5  4.6  2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.1  0.0  0.0  0.0  7.4  1.3  2.2  0.0  20.  11.  -1.  -11  0.5  -13  -0.  -6.  -11  -7.  -3.  2.1  -0.  -3.  -5.  -2.  9.4  16.  20.  24.  28.  29.  29.  30.  30.  30.  29.  26.  24.  21.  20.  18.  17.  15.  12.  9.7  7.3  5.0  2.4  -0.  -3.  -6.  -9.  -11  -11  -10  -9.  -8.  -8.  -7.  -6.  -6.  -5.  -5.  -4.  -4.  -3.  -3.  -2.  -2.  -1.  -16  -2.  11.  1.0  0.5  -4.  -0.  0.2  -0.  1.8  0.9  0.3  -0.  -1.  -0.  3.0  -1.  -11  -17  -20  -16  -11  -11  -13  -14  -15  -15  -14  -11  -9.  -6.  -4.  -2.  -0.  1.1  1.8  1.8  1.7  1.6  1.4  1.2  1.2  1.0  0.6  0.2  -0.  -0.  -1.  -1.  -1.  -1.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  0.0  100  854  49.  2.2  0.0  -0.  0.6  0.0  0.0  0.0  0.0  457  0.0  0.0  1.0  0.0  3.0  5.5  9.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.3  2.5  1.2  8.6  7.2  6.3  5.6  5.1  4.9  4.8  4.7  4.7  4.8  4.9  5.1  5.2  5.4  5.5  5.6  5.7  5.8  5.7  5.7  5.6  5.4  5.2  5.0  5.0  4.9  4.8  4.8  4.7  4.7  4.7  4.7  4.7  4.6  4.6  4.6  4.5  4.5  4.5  4.5  4.5  1.8  2.1  2.4  2.7  3.1  3.6  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0  8.5  8.9  9.4  9.8  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.8  4.7  5.8  7.2  8.7  1.0  1.2  1.5  1.7  2.0  2.3  2.6  2.9  3.2  3.5  3.8  4.1  4.4  4.7  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_3  .41  .13  .97  .21  .18  .83  .33  .23  .78  .37  .06  .78  .49  .73  .95  .71  .66  .44  .86  .69  .85  .82  .95  .45  .53  .90  .61  .63  .02  .02  .40  .36  .64  .00  .93  .80  .14  .53  .59  .32  .59  .52  .73  .41  .32  .38  .56  .78  .78  .61  .44  .25  .03  .83  .70  .59  .36  .33  .44  .65  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  727  581  970  000  000  000  000  000  000  001  002  004  007  009  013  015  017  018  019  019  017  015  014  014  015  024  055  076  092  103  111  115  119  125  130  135  140  146  150  154  162  167  170  170  195  385  813  176  270  512  916  226  679  905  387  993  386  132  479                                306  816  617  750  384  931  353  828  236  194  905  545  186  351  022                           953  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  203  390  071  567  463  386  801  007  855  884  782  052  818  869  513  659  206  182  016  009  732  264  369  237  517  348  000  000  000  000  000  000  000  000  000  101  458  849  950  531                                                                             408                 422  707  438       524  802  470  .65  279  .06  530  100  .25  793  578  557  065  212  278  017  590  106  314  469  051  734  425  037  483  392  191  628  036  964  169  769  378  586  791  593  303  999  627  475  580  801  821  .13  .23  .84  942  949  185  528  958  406  896  402  908  420  736  195  809  640  339  .64  232  885  244  788  366  900  913  477  085  618  699  326  623  154  376  834  .39  .25  .44  .96  .80  .61  .26  .39  .26  .22  .02  .76  361  713  097  069  301  867  407  725  443  268  520  803  355  097  511  341  276  767  052  148  168  085  953  782  562  399  240  113  029  053  671  .94  095  665  114  000  076  6    542  6    542  .90                      920  688  815  000  000  000  000  000  000  000  000  000  000  000  000  000  905  238  105  844  369  178  360  582  421  068  286  727  320  717  208  622  034  389  554  748  021  819  618  221  234  337  674  012  384  787  249  991  749  523  311  032  711  398  090  797  562  324  140  077  501  249  376  904  833  133  745  583  557  586  617  625  597  522  373  110  688  077  265  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  830  824  800  065  863  632  738  081  625  328  159  097  129  240  406  592  759  871  911  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 956  788  775  374  847  907  965  356  163  810  941  990  734  970  083  377  866  637  918  506  884  162  850  030  647  356  614  255  828  186  330  232  898  614  244  14   131  145  537  370  713  78   806  489  881  042  680  887  451  168  291  995  550  860  850  928  754  828  505  006  093  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   01   e-7  e-7  e-7  02   04   05   11   41   88   53   77   99   45   97   12   84   26   56   3    1    77   88   56   28   44   32   59   83   52   12   17   51   78   44   48   56   97   05   51   57   14   36   14   99   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-4  e-5                                e-5  e-4  e-3  e-3  e-3  e-2  e-2  e-1  e-1  e-1  e-8  e-8  e-8  e-8  e-9                           e-7  2    09   16   08   2    13   1    08   11   08   11   17   14   06   02   e-8  e-9  e-9  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1  e-9  e-9  e-8  e-1  e-9  e-8  07   16   18   17   19   2    16   04   02   e-7  e-8  e-8  e-9  e-1                                                                             e-1                 e-1  e-1  e-1       008  658  289  099  38   830  987  695  407  831  696  02   155  529  644  981  58   525  203  862  703  808  833  842  579  413  96   55   171  985  26   084  007  056  198  07   99   18   69   358  935  777  074  747  459  138  276  552  709  274  012  455  166  691  382  759  797  257  322  99   814  409  021  605  42   48   504  303  13   247  5    09   59   075  963  821  85   551  708  599  733  416  124  175  279  923  294  029  861  185  573  737  246  601  056  39   07   13   03   73   98             37   34   68   45   151  228  744  753  72   024  604  62   534  04   426  615  54   .58  506  183  3    82   041  85        07        07   763                      e-7  e-7  e-7  02   03   05   09   14   16   17   16   13   09   06   03   02   e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 95   1    7    5    9    5         9    9    1         9    9    5    1    6    4    1    4    7    1    8    1    2    5    9    7    2    2    3    2    8    6    4    4         6    5    5    5    2         4    5    2    8    3    4    2    8    8    5    6    6    3    5    5    7    4    8    3                                                                                                                                                                                                                                                                                                                7    8    8    8    8    8    8    8    8    9    9    0    5    8    4                                  0    5    9    5    1    6    2    8    4    0                                                                                                                                                     2    2    2    2    2    2    2    2    2    2    3    3    9    2    7    7    1                   0                                                                               0                                                                               2                   4    3    3                        1         8              7                                                                                                                                                                              5    1    4                                                                               6                                                                                    5    7    3    7    5    5    7    5    5    7    7                                                                                                                                                               323                                                    6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  215  232  240  245  259  262  257  250  238  229  224  220  214  209  203  195  190  186  188  191  197  203  209  216  222  228  234  238  243  247  250  253  256  260  263  267  270  273  276  278  280  281  282  283  284  285  286  287  288  289  290  291  291  292  293  294  295  296  297  298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.8  7.9  8.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  9.5  8.6  7.6  6.6  5.3  4.4  3.1  1.7  5.1  1.2  5.1  8.1  9.3  1.5  0.0  0.0  0.0  0.0  0.0  6.8  7.6  7.1  2.3  0.0  0.0  0.0  0.0  0.0  2.1  0.0  0.0  1.7  1.4  1.6  3.9  1.6  0.0  8.2  0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.5  1.4  5.1  7.2  6.2  6.5  7.9  9.4  8.6  1.0  8.7  5.9  2.7  1.5  8.2  1.4  2.1  2.2  1.8  1.9  8.4  1.5  6.6  1.1  6.7  1.3  6.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.2  3.2  5.2  1.9  0.0  2.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.6  1.1  3.6  0.0  0.0  0.0  9.8  18.  17.  -1.  -1.  2.6  -14  -7.  -6.  -8.  -3.  -3.  -2.  -3.  -2.  -3.  -1.  9.9  14.  14.  17.  23.  28.  30.  31.  31.  29.  26.  22.  20.  18.  17.  16.  15.  13.  11.  9.3  6.9  4.2  0.6  -3.  -7.  -9.  -10  -10  -10  -9.  -8.  -7.  -6.  -5.  -5.  -4.  -4.  -3.  -2.  -2.  -1.  -1.  -1.  -0.  -10  -6.  13.  13.  -0.  -0.  -2.  1.5  1.7  -1.  -1.  -0.  0.6  0.2  0.3  -1.  3.0  6.7  -1.  -14  -22  -24  -23  -21  -21  -19  -16  -12  -8.  -5.  -2.  -0.  2.3  3.8  4.1  3.9  3.4  3.1  2.9  2.6  2.4  1.9  1.6  1.2  0.8  0.4  0.0  -0.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -0.  -0.  -0.  100  654  34.  1.6  0.0  0.0  0.4  0.0  0.0  0.0  0.0  458  0.0  0.0  1.0  0.0  3.0  5.5  9.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.1  2.3  1.2  8.8  7.5  6.7  6.1  5.7  5.5  5.4  5.3  5.3  5.4  5.5  5.6  5.7  5.9  6.0  6.2  6.4  6.4  6.4  6.4  6.2  6.0  5.8  5.6  5.5  5.5  5.5  5.5  5.5  5.5  5.5  5.5  5.5  5.4  5.4  5.3  5.3  5.2  5.2  5.1  5.1  1.8  2.1  2.4  2.7  3.1  3.6  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0  8.5  8.9  9.4  9.8  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.8  4.7  5.8  7.1  8.7  1.0  1.2  1.5  1.7  2.0  2.3  2.6  2.9  3.2  3.5  3.8  4.1  4.4  4.7  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_8  .13  .70  .14  .66  .14  .64  .05  .79  .52  .84  .78  .42  .78  .43  .06  .09  .14  .09  .00  .54  .89  .69  .94  .05  .50  .64  .13  .98  .53  .31  .69  .62  .73  .46  .92  .40  .66  .73  .56  .75  .38  .71  .32  .21  .41  .80  .78  .81  .81  .71  .61  .31  .84  .61  .39  .26  .21  .28  .48  .73  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  678  174  687  000  000  000  000  000  000  001  002  004  006  009  012  016  017  017  016  014  013  013  014  020  035  050  073  090  100  108  114  118  123  129  133  137  145  150  155  160  165  168  170  170  149  160  333  879  545  905  734  063  183  117  482  465  227  437  528                           628  737  301  488                           595            669  851  483  592  248       662  000  830  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  911  527  456  670  296  300  199  948  921  068  134  803  788  869  614  209  172  504  651  827  050  358  995  585  242  722  314  000  000  000  000  000  000  000  000  000  000  095  791  340  285       510                                                                        102  740  012                 479  722  760  939  779  647  .81  129  119  868  867  596  567  202  468  474  260  276  317  355  531  833  676  479  606  529  526  226  555  001  341  300  404  029  308  297  167  636  588  719  633  402  951  .98  .95  .45  384  314  342  588  945  321  750  135  448  792  184  566  170  053  982  .32  535  815  229  639  859  666  567  950  969  053  142  559  612  677  940  791  838  096  .55  .15  .85  .14  .89  .15  .66  .87  .76  934  541  916  261  708  730  084  338  935  754  371  487  097  850  202  092  600  755  074  521  020  433  737  921  984  916  683  347  017  675  550  479  274  .96  293  925  037  021  655  6    872  6    872  .87                      892  639  727  000  000  000  000  000  000  000  000  000  000  000  000  000  570  401  068  657  348  453  790  566  494  143  332  835  537  551  623  807  063  487  320  197  768  460  153  693  425  260  232  930  642  369  110  123  151  177  202  037  532  040  554  068  618  167  717  601  497  244  371  898  826  125  735  573  545  573  603  610  581  505  355  090  668  056  243  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  754  736  700  951  736  618  723  065  607  309  140  078  109  220  387  573  740  853  893  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 894  577  476  191  211  696  789  190  876  877  449  855  946  160  019  389  745  843  06   273  113  570  343  093  236  310  634  903  847  586  566  809  759  388  663  118  746  238  471  196  437  076  261  921  974  004  168  303  357  292  316  327  538  848  163  456  286  424  421  699  142  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   01   e-7  e-7  e-7  02   05   08   19   48   98   79   96   66   69   63   98   02   45   47   22   38   29   24   42   77   08   5    34   6    82   34   41   46   97   11   25   66   04   81   99   94   76   94   09   8    e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-4  e-5                           e-5  e-5  e-4  e-4                           e-1            e-8  e-7  e-7  e-8  e-9       e-8  02   e-7  03   03   04   18   03   04   03   13   08   1    12   37   48   16   1    e-7  e-7  e-8  e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1  e-9  e-1  e-8  e-1  e-8  e-8  01   01   01   02   04   05   06   04   03   02   e-7  e-7  e-8  e-9       e-1                                                                        e-1  e-1  e-1                 e-1  166  093  848  939  98   242  902  783  477  754  304  24   521  386  392  273  5    554  944  598  913  099  286  482  93   812  882  32   379  267  985  362  361  423  323  09   97   2    01   249  048  46   188  976  064  851  138  307  866  333  458  257  325  58   322  244  916  932  412  516  659  773  284  978  377  063  459  82   09   189  233  041  43   01   95   656  26   54   384  769  655  133  228  060  972  498  406  676  568  738  226  89   63   87   3    22   04   73   82   15   12   27   82   34   2    84   63   46   854  941  137  7    218  981  63   555  434  494  38   87   .20  609  045  16   38   62   42        62        62   069                      e-7  e-7  e-7  02   03   05   09   14   16   17   16   13   09   06   03   02   e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 2    2    7    2    3    9    5    4         8    9    2    8    1    5    6         1         8    5    4    1    2    5    2    3    8    5    3         7         3    8    8    1    6    8    8    9    1    7    5    4    4    9         8    2    5    2    1    3    3    3    8    7         9    9                                                                                                                                                                                                                                                                                                                7    8    8    8    8    8    8    8    8    9    9    0    4    9    5                             7    4    9    4                             8                                                                                                                                                          2    2    2    2    2    1    2    2    2    2    3    3    7    2    2    7    2         0         0                                                                                         0                                                                          2    3    4                   5                             3                                                                                                                                                                                        7    9    5                                                                               9                                                                                              1    2    4    7    4    7    9    7    9                                                                                                                                                                    343  4                                                 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " tes  214  233  238  248  255  261  257  247  237  228  224  220  214  209  203  199  193  189  190  193  198  204  210  216  222  227  232  236  240  244  247  250  253  256  258  261  264  267  270  273  276  278  279  280  281  282  283  284  285  286  287  288  289  290  292  293  294  295  297  298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  9.7  8.5  7.5  6.6  5.4  5.2  3.8  3.3  2.1  2.0  1.0  5.4  1.1  5.0  0.0  0.0  0.0  0.0  0.0  4.2  1.8  3.9  2.5  8.6  3.1  1.3  1.8  2.9  2.1  0.0  0.0  0.0  3.9  7.2  1.1  1.0  1.7  1.8  0.0  8.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.5  5.0  1.2  3.9  6.9  1.0  3.2  6.1  6.4  6.7  7.6  7.5  8.6  6.2  4.0  2.3  1.2  8.6  3.7  1.7  3.8  5.8  1.0  1.4  1.2  8.0  1.3  6.4  9.6  3.9  6.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  2.8  1.4  1.4  1.1  0.0  1.1  9.3  1.1  7.1  8.4  5.6  3.8  3.3  5.0  4.7  3.0  2.7  2.4  0.0  8.1  2.1  0.0  0.0  0.0  0.0  0.0  51.  37.  38.  41.  38.  30.  30.  25.  18.  12.  8.2  5.3  2.9  1.0  0.7  6.6  13.  20.  27.  33.  40.  47.  51.  51.  50.  47.  43.  37.  30.  25.  22.  21.  20.  19.  17.  16.  14.  12.  10.  8.9  7.1  5.4  4.0  3.0  1.9  0.8  -0.  -1.  -2.  -3.  -4.  -5.  -5.  -5.  -5.  -5.  -4.  -4.  -4.  -4.  -1.  -13  -5.  10.  4.0  13.  -2.  -1.  -0.  1.3  1.7  0.6  0.2  0.3  -0.  -0.  0.1  -3.  -6.  -11  -15  -15  -10  -8.  -9.  -11  -13  -14  -12  -8.  -4.  -1.  0.3  0.9  1.3  1.7  1.7  1.5  1.3  1.3  1.3  1.6  2.0  2.2  2.1  1.6  1.0  0.5  0.0  -0.  -1.  -1.  -2.  -2.  -2.  -2.  -2.  -2.  -2.  -2.  987  724  185  25.  0.0  0.0  0.5  0.0  0.0  0.0  0.0  462  0.0  0.1  0.8  0.0  3.0  5.4  9.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.4  2.0  1.2  9.7  7.9  6.8  6.1  6.0  5.9  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.8  6.8  6.8  6.9  6.8  6.8  6.8  6.8  6.7  6.6  6.6  6.5  6.5  6.5  6.5  6.5  6.5  6.5  6.6  6.7  6.7  6.8  6.8  6.8  1.8  2.0  2.3  2.7  3.1  3.5  3.9  4.4  4.9  5.4  5.9  6.4  6.8  7.3  7.8  8.3  8.7  9.1  9.6  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.4  4.2  5.2  6.4  7.9  9.7  1.1  1.3  1.6  1.9  2.1  2.4  2.7  3.0  3.3  3.6  4.0  4.3  4.6  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_7  .90  .68  .15  .79  .43  .87  .73  .84  .01  .72  .29  .07  .65  .63  .13  .30  .92  .00  .08  .27  .85  .43  .75  .07  .25  .99  .82  .83  .40  .05  .40  .76  .87  .35  .63  .27  .39  .52  .58  .46  .04  .10  .69  .59  .03  .10  .29  .58  .76  .92  .77  .76  .70  .89  .12  .35  .57  .81  .05  .32  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  002  003  003  002  002  002  004  008  014  018  020  021  020  019  020  022  027  037  056  070  080  085  092  097  105  112  122  125  126  126  127  128  128  129  240  601  978  063  207  017  994  816  322  390  676  688  397  950  025                           262  205  505  968  021  059  330  119  816  595                 384  420  810  490  525  097       280  000  000  000  000  000  000  000  000  000  000  000  000  407  271  904  397  656  900  887  281  930  827  458  475  658  857  058  955  466  311  115  075  695  055  269  395  922  708  658  196  679  776  408  000  000  000  000  000  000  000  000  000  622  218  132  059  835       867  804  343  379  519  288  800  601  977  965  257  207  084       785  322                           985  025  208  439  001  190  021  929  735  256  900  552  540  040  584  197  552  842  181  445  895  974  218  514  413  723  560  232  558  744  663  501  784  586  881  178  704  959  988  574  662  211  739  309  353  984  237  384  503  631  770  135  241  185  096  027  974  901  805  648  965  .12  666  940  355  363  621  983  392  798  694  867  768  269  402  026  934  025  598  .72  .12  .23  .11  825  523  .55  .71  .10  .71  868  939  703  219  540  693  359  569  373  179  207  611  268  542  875  286  673  955  523  280  460  217  664  227  426  512  519  584  594  590  542  62.  .00  .43  138  613  335  146  844  934  604  707  .39       247  752       082  180  112  000  000  000  000  000  000  000  000  000  000  000  000  000  000  315  092  922  813  838  105  687  120  512  809  668  699  575  422  365  408  470  327  205  819  903  986  011  856  708  568  128  445  796  180  696  551  416  290  170  273  925  569  213  861  092  143  196  082  767  824  272  112  314  821  550  411  326  243  137  997  811  552  181  656  945  038  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  248  493  630  969  770  180  718  957  400  012  760  624  592  649  771  923  063  159  190  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 938  118  025  687  014  833  309  480  304  243  790  236  939  319  926  867  208  160  081  849  943  863  528  070  604  030  032  634  737  277  168  132  600  800  643  842  543  608  172  689  174  715  473  211  679  419  263  587  184  060  946  447  447  444  118  964  581  525  086  500  99   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   06   07   12   39   84   53   68   59   6    97   75   98   4    63   29   63   66   47   95   96   45   01   25   83   56   51   19   82   66   02   39   49   67   45   26   95   59   06   57   66   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-4  e-5                           e-5  e-4  e-4  e-4  e-3  e-3  e-2  e-2  e-1  e-1                 e-8  e-8  e-7  e-7  e-7  e-7       e-8  02   03   01   08   13   26   15   17   1    11   06   01   e-7  e-7  e-7  e-7  e-8  e-7  e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1  e-8  e-8  e-7  e-9  e-9  e-8  e-7  01   02   02   02   03   04   03   02   01   e-7  e-7  e-7  e-8  e-8       e-8  e-9  e-8  e-9  e-9  e-9  e-9  e-9  e-9  e-9  e-9  e-9  e-9       e-1  e-1                           733  343  433  662  918  197  533  628  208  085  56   6    92   33   37   94   734  486  561  408  726  711  588  585  059  948  689  692  484  329  329  361  877  174  074  472  635  59   883  45   12   42   32   66   02   79   4    259  452  182  764  978  799  748  547  384  548  653  298  118  289  997  485  575  44   701  11   662  965  43   08   81   95   28   234  033  82   883  888  510  488  471  534  838  812  645  486  029  861  519  118  198  15   65   31   42   8    75   13   69   74   89   47   33   86   22   01   84   08   208  157  914  255  104  243  86   885  903  013  154  158  807  786  019  92   24   17   67   85   76   29   474       11   89        e-7  e-7  e-7  02   03   05   09   14   15   15   14   12   09   06   03   02   01   e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 2    4    5         9    8    4    7         8    3    8    6    1    2    5    9    3    1    7         4         7    5    2    2    1    5    4    9    4    3    9    6    2    7    9    9    3    3    2    6    6    1    2    4    8    7    3    2         9    5    8    1         3         1                                                                                                                                                                                                                                                                                                                     7    8    8    8    8    8    8    8    8    8    8    8    4    7    4                             4    9    5    0    7    2    7    3    9    8                                                                                                                                                          2    2    2    2    2    2    2    2    2    2    3    3    8    2    3    7    1                                                                                                                                                                                        0    1                                                                                                                                                                                                                                                                                                                                              9                                                                                         5         3    2              4    8    6    2                                                                                                                                                               347  1    9                                            3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       " tes  214  233  237  248  252  261  257  248  237  229  224  219  214  209  202  198  195  190  190  193  199  204  210  215  221  226  231  235  239  243  246  250  253  256  258  261  264  267  269  272  274  276  278  280  282  283  285  286  287  288  289  290  290  291  292  293  294  296  297  298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  9.6  8.6  7.5  6.5  5.3  5.1  3.8  3.3  2.1  1.9  9.7  5.1  1.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.6  7.1  2.3  0.0  0.0  0.0  0.0  8.1  2.6  1.1  1.1  1.7  0.0  0.0  6.1  6.8  9.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.8  5.4  1.1  3.1  6.2  6.5  6.7  7.5  7.5  8.9  6.4  4.1  2.1  1.2  8.6  3.2  1.7  1.2  1.0  9.7  4.8  0.0  5.4  1.2  2.4  1.4  5.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.4  8.3  1.2  3.8  8.5  8.1  2.4  3.4  4.9  6.3  8.6  9.9  1.0  9.7  8.5  7.7  6.7  1.3  0.0  0.0  5.1  2.2  4.1  6.6  4.8  26.  38.  44.  50.  40.  31.  30.  27.  20.  14.  9.7  4.8  2.6  2.7  2.9  7.0  15.  21.  26.  30.  35.  40.  41.  41.  38.  34.  31.  27.  24.  23.  21.  20.  19.  18.  16.  14.  12.  10.  9.0  7.3  5.8  4.5  3.3  2.3  1.4  0.6  -0.  -0.  -1.  -2.  -2.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -2.  -2.  -19  -7.  5.0  16.  6.0  14.  0.5  0.0  -1.  -0.  0.7  0.9  0.2  0.3  1.0  1.4  2.5  2.2  0.0  -5.  -10  -12  -12  -12  -11  -10  -6.  -2.  1.0  3.7  5.0  5.2  5.2  4.8  4.2  3.6  3.0  2.6  2.3  2.1  2.0  1.8  1.6  1.4  1.0  0.5  -0.  -0.  -1.  -1.  -2.  -2.  -2.  -2.  -3.  -3.  -3.  -3.  -2.  -2.  984  525  132  18.  0.0  0.0  0.3  0.1  0.1  0.0  0.0  460  0.0  0.4  0.5  0.0  3.0  5.4  9.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.3  4.0  1.9  1.2  9.8  8.2  7.2  6.5  6.2  6.0  6.0  6.0  6.1  6.2  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.0  7.0  7.1  6.9  6.7  6.5  6.4  6.4  6.3  6.3  6.3  6.3  6.3  6.2  6.2  6.1  6.0  5.9  5.7  5.7  5.7  1.8  2.0  2.3  2.7  3.1  3.5  3.9  4.4  4.9  5.4  5.9  6.4  6.8  7.3  7.8  8.3  8.7  9.1  9.5  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  9.9  3.3  4.2  5.2  6.4  7.9  9.6  1.1  1.3  1.6  1.8  2.1  2.4  2.7  3.0  3.3  3.6  3.9  4.3  4.6  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9  4.9 \n",
       " t_6  .54  .46  .85  .36  .35  .42  .17  .37  .31  .16  .54  .77  .13  .30  .74  .44  .44  .55  .98  .93  .34  .20  .20  .43  .24  .46  .25  .39  .42  .19  .84  .23  .23  .03  .78  .77  .58  .21  .92  .45  .88  .98  .82  .44  .11  .70  .05  .34  .24  .19  .23  .06  .91  .78  .83  .82  .96  .00  .14  .33  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  001  001  003  004  006  007  009  010  013  016  017  020  024  030  037  044  051  059  066  072  078  085  090  099  106  110  116  123  129  132  137  138  142  143  145  053  761  073  284  289  307  958  087  045  781  838  569  639  710                                     737  301  488                      064  918  465  626  548  000  000  742  795  800  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000  000       502  026  691  324  827  066  600  840  201  972  027  368  367  561  818  672  113  354  416  141  089       496  317  247  823  093  000  000  000  000  000  000  000  000  000  000  000  000  068  139  426  286  483  107  125  059  822  725  643  024  750  164  857  663  589  300            521  926  159  932  389  766  326  364  504  427  241  402  722  383  493  586  785  832  609  635  875  348  242  108  126  599  136  652  121  686  965  105  512  737  037  860  854  693  156  349  623  867  999  428  435  934  539  750  741  389  036  173  930  617  284  797  156  366  481  525  454  384  113  909  618  .71  008  984  692  398  258  305  540  110  972  543  950  395  931  700  122  652  346  583  735  .93  .98  .88  .57  .84  .10  941  603  492  231  487  964  358  085  367  257  153  051  492  552  172  579  872  015  061  147  013  566  087  582  035  415  685  897  029  136  149  058  957  749  63.  .70  .10  355  611  621  736  426  832  589  944  .09       014  985       046  114  993  000  000  000  000  000  000  000  000  000  000  000  000  000  332  457  842  794  775  898  434  700  111  836  226  869  481  038  637  359  105  292  596  845  833  828  558  732  900  004  089  275  551  353  043  750  476  262  129  004  884  481  659  837  535  958  958  958  050  730  782  224  057  252  751  472  324  230  139  024  876  681  414  035  502  783  869  766  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  861  986  184  268  547  282  621  655  886  322  926  667  525  487  538  655  802  938  031  058  014  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086  086 \n",
       " 013  690  088  057  119  375  646  252  148  665  794  255  220  307  952  787  803  095  546  908  086  107  467  923  66   639  801  908  401  433  983  374  620  465  563  251  499  027  350  114  624  559  932  592  445  135  179  509  261  152  072  910  59   658  447  576  904  378  140  843  091  02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   02   03   06   08   18   4    78   27   79   05   57   08   61   09   88   83   72   9    11   33   17   47   49   63   65   54   81   52   6    93   12   05   21   8    62   85   53   96   65   51   63   07   e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-3  e-4  e-5                                     e-5  e-4  e-4                      e-2  e-1  e-1  e-9  e-7  02   02   e-7  e-8  e-8  02   07   06   25   22   15   32   31   22   12   31   42   29   16   14   08   02        e-7  e-8  e-7  e-8  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-1  e-2  e-2  e-1  e-1       e-9  e-7  e-9  e-7  e-7  03   03   01   01   17   34   51   47   34   21   11   06   e-7  e-8  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-8  e-8  e-8  e-8  e-9            e-1  e-1  e-1  e-1  e-1  125  466  158  799  283  199  445  321  986  661  15   46   49   8    64        046  244  509  443  535  581  886  061  804  231  139  96   766  146  157  756  072  475  687  607  586  232  02   2    71   44   12   91   02   76   665  428  219  518  098  353  056  461  668  358  439  365  109  813  489  516  22   305  38   133  96   57   178  205  06   24   25   46   35   89   32   06   34   292  217  623  489  391  733  754  908  484  67   52   24   05   92   92   25   06   51   33   06   75   06   69   04   92   44   13   485  443  8    165  671  875  827  569  162  76   133  256  907  553  738  208  793  525  75   1    63   97   34   91   76   122       65   35        e-7  e-7  e-7  02   03   05   09   14   15   15   14   12   09   06   03   02   e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-8  e-8  e-8  e-8  e-8  e-8  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7  e-7 \n",
       " 50   9    9    5    9    8    4         1    1    1         6    7    9    1    7    3    8         9    2    4    8         1    9    5         6    6    1    7    2         2    8    6    6    9    1    9    1    8    5    8    5    1    9    6         6         5    9    6         5    6         3                                                                                                                                                                                                                                                                                                                7    8    8    8    8    8    8    8    8    8    8    9    5    1                                       4    9    4                        0    6    2                                                                                                                                                     2    2    2    2    2    2    2    2    2    2    3    3    9    5    3    9    7                                                                                                                                                                                                  3    1    1    1    4                                                                                                                                                                                                                                                                                                                7                                                                                                   1    1    9    4    8    4                                                                                                                                                                              907  5    7                                            3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       ""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mbatch\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(batch[0].std(dim=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9721663e-13"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, 180].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8331273364701616e-11"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:, 180].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mdisable(learn\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval(),)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.compiler.disable(learn.model.eval(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='9766' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/9766 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/leroy/conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/loss.py:999: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 368])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Exception occured in `Recorder` when calling event `after_batch`:\n\t==:\n23552\n64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_targs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:308\u001b[0m, in \u001b[0;36mLearner.get_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_loss: ctx_mgrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_not_reduced())\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(ctx_mgrs):\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: act \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    310\u001b[0m     res \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mall_tensors()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:201\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  final()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/foundation.py:156\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:840\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/basics.py:825\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    824\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:176\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:62\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/callback/core.py:60\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run: \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:560\u001b[0m, in \u001b[0;36mRecorder.after_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    559\u001b[0m mets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_mets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_mets\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m mets: \u001b[43mmet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/learner.py:482\u001b[0m, in \u001b[0;36mAvgMetric.accumulate\u001b[0;34m(self, learn)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn):\n\u001b[1;32m    481\u001b[0m     bs \u001b[38;5;241m=\u001b[39m find_bs(learn\u001b[38;5;241m.\u001b[39myb)\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mto_detach(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39mbs\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/metrics.py:288\u001b[0m, in \u001b[0;36mmae\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmae\u001b[39m(inp,targ):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean absolute error between `inp` and `targ`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     inp,targ \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(inp \u001b[38;5;241m-\u001b[39m targ)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastai/torch_core.py:787\u001b[0m, in \u001b[0;36mflatten_check\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck that `inp` and `targ` have the same number of elements and flatten them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m inp,targ \u001b[38;5;241m=\u001b[39m TensorBase(inp\u001b[38;5;241m.\u001b[39mcontiguous())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),TensorBase(targ\u001b[38;5;241m.\u001b[39mcontiguous())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 787\u001b[0m \u001b[43mtest_eq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inp,targ\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/test.py:37\u001b[0m, in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_eq\u001b[39m(a,b):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`test` that `a==b`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mequals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/torch2/lib/python3.10/site-packages/fastcore/test.py:27\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: cname\u001b[38;5;241m=\u001b[39mcmp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cmp(a,b),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Exception occured in `Recorder` when calling event `after_batch`:\n\t==:\n23552\n64"
     ]
    }
   ],
   "source": [
    "preds = learn.get_preds(None, dl=test_loader, with_targs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9766/9766 [12:01<00:00, 13.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_loader):\n",
    "        x = (batch[0][0].cuda().type(torch.float), batch[0][1].cuda())\n",
    "        pred = model(x)\n",
    "        #pred = learn.get_preds\n",
    "        #pred = norm_y.denorm(pred)\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2091, -1.1206, -1.5577,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3850, -1.2824, -1.5098,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1913, -0.1536, -1.0023,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.5073,  0.5929, -0.0575,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.4168, -0.2640, -0.8508,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3559, -0.1311, -0.6344,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 376])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.concatenate([pred.cpu() for pred in predictions], axis=0)# * weights[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = predicts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicts, 'preds6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicts = torch.load('preds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pl.DataFrame(test_df['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (625_000, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;test_169651&quot;</td></tr><tr><td>&quot;test_524862&quot;</td></tr><tr><td>&quot;test_634129&quot;</td></tr><tr><td>&quot;test_403572&quot;</td></tr><tr><td>&quot;test_484578&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;test_578220&quot;</td></tr><tr><td>&quot;test_395695&quot;</td></tr><tr><td>&quot;test_88942&quot;</td></tr><tr><td>&quot;test_79382&quot;</td></tr><tr><td>&quot;test_601350&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (625_000, 1)\n",
       "\n",
       " sample_id   \n",
       " ---         \n",
       " str         \n",
       "\n",
       " test_169651 \n",
       " test_524862 \n",
       " test_634129 \n",
       " test_403572 \n",
       " test_484578 \n",
       "            \n",
       " test_578220 \n",
       " test_395695 \n",
       " test_88942  \n",
       " test_79382  \n",
       " test_601350 \n",
       ""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = TARGET_COLS.index('ptend_q0002_26')\n",
    "col = \"ptend_q0002_26\"\n",
    "norm_y.zero_mask[indx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping ptend_q0001_0\n",
      "skipping ptend_q0001_1\n",
      "skipping ptend_q0001_2\n",
      "skipping ptend_q0001_3\n",
      "skipping ptend_q0001_4\n",
      "skipping ptend_q0001_5\n",
      "skipping ptend_q0001_6\n",
      "skipping ptend_q0001_7\n",
      "skipping ptend_q0001_8\n",
      "skipping ptend_q0001_9\n",
      "skipping ptend_q0001_10\n",
      "skipping ptend_q0001_11\n",
      "skipping ptend_q0002_0\n",
      "skipping ptend_q0002_1\n",
      "skipping ptend_q0002_2\n",
      "skipping ptend_q0002_3\n",
      "skipping ptend_q0002_4\n",
      "skipping ptend_q0002_5\n",
      "skipping ptend_q0002_6\n",
      "skipping ptend_q0002_7\n",
      "skipping ptend_q0002_8\n",
      "skipping ptend_q0002_9\n",
      "skipping ptend_q0002_10\n",
      "skipping ptend_q0002_11\n",
      "skipping ptend_q0002_12\n",
      "skipping ptend_q0002_13\n",
      "skipping ptend_q0002_14\n",
      "skipping ptend_q0002_15\n",
      "skipping ptend_q0002_16\n",
      "skipping ptend_q0002_17\n",
      "skipping ptend_q0002_18\n",
      "skipping ptend_q0002_19\n",
      "skipping ptend_q0002_20\n",
      "skipping ptend_q0002_21\n",
      "skipping ptend_q0002_22\n",
      "skipping ptend_q0002_23\n",
      "skipping ptend_q0002_24\n",
      "skipping ptend_q0002_25\n",
      "skipping ptend_q0002_26\n",
      "skipping ptend_q0003_0\n",
      "skipping ptend_q0003_1\n",
      "skipping ptend_q0003_2\n",
      "skipping ptend_q0003_3\n",
      "skipping ptend_q0003_4\n",
      "skipping ptend_q0003_5\n",
      "skipping ptend_q0003_6\n",
      "skipping ptend_q0003_7\n",
      "skipping ptend_q0003_8\n",
      "skipping ptend_q0003_9\n",
      "skipping ptend_q0003_10\n",
      "skipping ptend_q0003_11\n",
      "skipping ptend_u_0\n",
      "skipping ptend_u_1\n",
      "skipping ptend_u_2\n",
      "skipping ptend_u_3\n",
      "skipping ptend_u_4\n",
      "skipping ptend_u_5\n",
      "skipping ptend_u_6\n",
      "skipping ptend_u_7\n",
      "skipping ptend_u_8\n",
      "skipping ptend_u_9\n",
      "skipping ptend_u_10\n",
      "skipping ptend_u_11\n",
      "skipping ptend_v_0\n",
      "skipping ptend_v_1\n",
      "skipping ptend_v_2\n",
      "skipping ptend_v_3\n",
      "skipping ptend_v_4\n",
      "skipping ptend_v_5\n",
      "skipping ptend_v_6\n",
      "skipping ptend_v_7\n",
      "skipping ptend_v_8\n",
      "skipping ptend_v_9\n",
      "skipping ptend_v_10\n",
      "skipping ptend_v_11\n"
     ]
    }
   ],
   "source": [
    "for n, col in enumerate(TARGET_COLS):\n",
    "    if norm_y.zero_mask[n]:\n",
    "        print(f'skipping {col}')\n",
    "        pl_col = pl.lit(0.0, dtype=pl.Float32).alias(col)\n",
    "    else:\n",
    "        pl_col = pl.Series(col, predicts[:, n],  dtype=pl.Float32)\n",
    "    \n",
    "    output = output.with_columns(pl_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write_csv('submission.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.09812656e+04, 2.25024316e+04, 1.88941465e+04, 1.45142451e+04,\n",
       "       1.09443477e+04, 9.06501074e+03, 9.66366895e+03, 1.26885576e+04,\n",
       "       1.98901719e+04, 2.58313730e+04, 3.38903672e+04, 4.41229414e+04,\n",
       "       5.98112578e+04, 7.94340781e+04, 1.07358812e+05, 1.35720844e+05,\n",
       "       1.49399844e+05, 1.28492953e+05, 9.17462344e+04, 7.27487656e+04,\n",
       "       6.65315391e+04, 6.29323047e+04, 5.66102695e+04, 4.94731445e+04,\n",
       "       4.30291836e+04, 3.69126758e+04, 3.14869316e+04, 2.68980723e+04,\n",
       "       2.33166387e+04, 2.04597305e+04, 1.83856836e+04, 1.71114043e+04,\n",
       "       1.63378096e+04, 1.58577598e+04, 1.55809023e+04, 1.54975908e+04,\n",
       "       1.56122559e+04, 1.57978848e+04, 1.59742188e+04, 1.61303955e+04,\n",
       "       1.62613105e+04, 1.63718926e+04, 1.63970195e+04, 1.63254639e+04,\n",
       "       1.62286406e+04, 1.61918096e+04, 1.63412080e+04, 1.66457109e+04,\n",
       "       1.70054941e+04, 1.74302988e+04, 1.79072402e+04, 1.84315527e+04,\n",
       "       1.90324707e+04, 1.97013555e+04, 2.04082363e+04, 2.09672070e+04,\n",
       "       2.11944277e+04, 2.10885215e+04, 1.94379160e+04, 1.36779023e+04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.71528464e+11, 1.08322180e+12, 1.47034751e+11, 3.55560448e+10,\n",
       "       3.51533711e+10, 4.60863693e+10, 2.46893056e+10, 1.13432771e+10,\n",
       "       5.39662490e+09, 2.44935296e+09, 1.13222592e+09, 5.79547840e+08,\n",
       "       3.30219232e+08, 2.07613936e+08, 1.44580288e+08, 1.09933280e+08,\n",
       "       8.87066000e+07, 7.38197760e+07, 6.36159880e+07, 5.72502640e+07,\n",
       "       5.29760720e+07, 4.96531680e+07, 4.65449760e+07, 4.31676080e+07,\n",
       "       3.97243760e+07, 3.63171760e+07, 3.30575120e+07, 2.98690900e+07,\n",
       "       2.69823860e+07, 2.44162360e+07, 2.22736520e+07, 2.05534260e+07,\n",
       "       1.92162400e+07, 1.81676940e+07, 1.75018560e+07, 1.71699380e+07,\n",
       "       1.70053820e+07, 1.69984760e+07, 1.70828900e+07, 1.72279820e+07,\n",
       "       1.74458240e+07, 1.77574040e+07, 1.83460920e+07, 1.94005740e+07,\n",
       "       2.05067220e+07, 2.24696480e+07, 2.34320320e+07, 2.62041640e+07,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 9.99999987e+14, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 9.99999987e+14, 9.99999987e+14,\n",
       "       9.99999987e+14, 9.99999987e+14, 3.67382993e+12, 3.71405586e+11,\n",
       "       1.42191636e+10, 3.00186291e+09, 1.43276659e+09, 8.84599808e+08,\n",
       "       5.60128000e+08, 3.86052576e+08, 2.87331840e+08, 2.22703664e+08,\n",
       "       1.81069232e+08, 1.54620864e+08, 1.38093776e+08, 1.26605832e+08,\n",
       "       1.17967840e+08, 1.11005816e+08, 1.05186904e+08, 1.00168136e+08,\n",
       "       9.55686480e+07, 9.14574320e+07, 8.88716080e+07, 8.88298000e+07,\n",
       "       9.13981120e+07, 9.65851280e+07, 1.04507696e+08, 1.15895120e+08,\n",
       "       1.31939704e+08, 1.54492944e+08, 1.83147920e+08, 2.15151376e+08,\n",
       "       2.47158320e+08, 2.66792880e+08, 2.79115136e+08, 3.70541504e+08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.77670498e+11, 1.17482691e+12, 1.27060555e+12, 2.17273160e+10,\n",
       "       3.15945677e+09, 1.09065344e+09, 7.27967104e+08, 3.84399552e+08,\n",
       "       2.90787296e+08, 2.32703216e+08, 1.97467456e+08, 1.74310896e+08,\n",
       "       1.60536432e+08, 1.53567104e+08, 1.52120128e+08, 1.53115568e+08,\n",
       "       1.53955552e+08, 1.53734672e+08, 1.54798672e+08, 1.63346208e+08,\n",
       "       1.80013136e+08, 2.00324352e+08, 2.20754608e+08, 2.41290928e+08,\n",
       "       2.62868928e+08, 2.84448896e+08, 3.05681088e+08, 3.27605088e+08,\n",
       "       3.50473312e+08, 3.73964608e+08, 3.98396928e+08, 4.23528352e+08,\n",
       "       4.50447040e+08, 4.78856992e+08, 5.08200320e+08, 5.37309632e+08,\n",
       "       5.66854592e+08, 5.94618816e+08, 6.19715904e+08, 6.41395456e+08,\n",
       "       6.63290048e+08, 6.89274880e+08, 7.18208896e+08, 7.43951232e+08,\n",
       "       7.61776128e+08, 7.72911232e+08, 8.04001152e+08, 7.72448768e+08,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.61382350e+06, 1.99930888e+06, 9.04636250e+05, 4.33823625e+05,\n",
       "       2.07201391e+05, 1.07836094e+05, 5.76479141e+04, 4.06065234e+04,\n",
       "       4.77398672e+04, 5.16693555e+04, 5.64381992e+04, 6.04474570e+04,\n",
       "       6.52514141e+04, 7.19208828e+04, 7.85295781e+04, 8.34223047e+04,\n",
       "       8.70369844e+04, 9.03897266e+04, 9.39823906e+04, 9.75780078e+04,\n",
       "       1.01428211e+05, 1.04630695e+05, 1.05685047e+05, 1.03962586e+05,\n",
       "       9.96503203e+04, 9.42905000e+04, 8.95148984e+04, 8.59054609e+04,\n",
       "       8.27849844e+04, 7.91522891e+04, 7.48478125e+04, 7.03788203e+04,\n",
       "       6.54200469e+04, 5.99537500e+04, 5.47642812e+04, 5.03625117e+04,\n",
       "       4.62125703e+04, 4.19975273e+04, 3.76920508e+04, 3.38347344e+04,\n",
       "       3.18460977e+04, 3.19341465e+04, 3.14548125e+04, 3.01054082e+04,\n",
       "       2.69578301e+04, 2.77600449e+04, 2.98533750e+04, 1.91334297e+04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.61994050e+06, 3.14839450e+06, 1.30841500e+06, 5.40515750e+05,\n",
       "       2.15237109e+05, 1.02546727e+05, 6.84536719e+04, 5.06925898e+04,\n",
       "       5.14875195e+04, 5.21047695e+04, 5.40193906e+04, 5.58560234e+04,\n",
       "       6.03473008e+04, 6.89909609e+04, 7.90968906e+04, 8.75743359e+04,\n",
       "       9.41585625e+04, 1.01903633e+05, 1.11746977e+05, 1.22460656e+05,\n",
       "       1.32086688e+05, 1.41041484e+05, 1.46354094e+05, 1.45953094e+05,\n",
       "       1.39496797e+05, 1.28508852e+05, 1.16665516e+05, 1.07458398e+05,\n",
       "       1.00259969e+05, 9.41089844e+04, 8.84398984e+04, 8.27349062e+04,\n",
       "       7.70610859e+04, 7.13335312e+04, 6.59997266e+04, 6.17989961e+04,\n",
       "       5.82373555e+04, 5.47151016e+04, 5.08258438e+04, 4.60591758e+04,\n",
       "       4.07402617e+04, 3.63358008e+04, 3.39815742e+04, 3.35897148e+04,\n",
       "       3.39888867e+04, 3.62729375e+04, 4.11833438e+04, 2.91941230e+04,\n",
       "       4.05361364e-03, 1.38824238e-02, 1.35129888e+08, 1.22197180e+07,\n",
       "       9.07052774e-03, 8.58988520e-03, 2.15368196e-02, 3.36321294e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, targets = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptend_t_0 : 0.01796277053654194\n",
      "ptend_t_1 : 0.01580028049647808\n",
      "ptend_t_2 : 0.005049095023423433\n",
      "ptend_t_3 : 0.0012899171561002731\n",
      "ptend_t_4 : 0.0006612258148379624\n",
      "ptend_t_5 : 0.000465095741674304\n",
      "ptend_t_6 : 0.00043876576819457114\n",
      "ptend_t_7 : 0.0007029272383078933\n",
      "ptend_t_8 : 0.0008303220965899527\n",
      "ptend_t_9 : 0.0010582703398540616\n",
      "ptend_t_10 : 0.0018357549561187625\n",
      "ptend_t_11 : 0.0019260403933003545\n",
      "ptend_t_12 : 0.002866466762498021\n",
      "ptend_t_13 : 0.003381755668669939\n",
      "ptend_t_14 : 0.006356712430715561\n",
      "ptend_t_15 : 0.01387377455830574\n",
      "ptend_t_16 : 0.1253134310245514\n",
      "ptend_t_17 : 0.3069222569465637\n",
      "ptend_t_18 : 0.533825159072876\n",
      "ptend_t_19 : 0.5077593922615051\n",
      "ptend_t_20 : 0.4755598306655884\n",
      "ptend_t_21 : 0.44834259152412415\n",
      "ptend_t_22 : 0.3704204261302948\n",
      "ptend_t_23 : 0.301801860332489\n",
      "ptend_t_24 : 0.26321732997894287\n",
      "ptend_t_25 : 0.2153325378894806\n",
      "ptend_t_26 : 0.17556500434875488\n",
      "ptend_t_27 : 0.14571933448314667\n",
      "ptend_t_28 : 0.12255648523569107\n",
      "ptend_t_29 : 0.10464970022439957\n",
      "ptend_t_30 : 0.0986165851354599\n",
      "ptend_t_31 : 0.09943924099206924\n",
      "ptend_t_32 : 0.10381776094436646\n",
      "ptend_t_33 : 0.11422466486692429\n",
      "ptend_t_34 : 0.12497811764478683\n",
      "ptend_t_35 : 0.14388370513916016\n",
      "ptend_t_36 : 0.16816966235637665\n",
      "ptend_t_37 : 0.1987522691488266\n",
      "ptend_t_38 : 0.2286107987165451\n",
      "ptend_t_39 : 0.25997981429100037\n",
      "ptend_t_40 : 0.2961488962173462\n",
      "ptend_t_41 : 0.33176904916763306\n",
      "ptend_t_42 : 0.36067894101142883\n",
      "ptend_t_43 : 0.3792122006416321\n",
      "ptend_t_44 : 0.4003381133079529\n",
      "ptend_t_45 : 0.42244037985801697\n",
      "ptend_t_46 : 0.44774961471557617\n",
      "ptend_t_47 : 0.46505022048950195\n",
      "ptend_t_48 : 0.4864519238471985\n",
      "ptend_t_49 : 0.5009571313858032\n",
      "ptend_t_50 : 0.5133023262023926\n",
      "ptend_t_51 : 0.5093532204627991\n",
      "ptend_t_52 : 0.5029786825180054\n",
      "ptend_t_53 : 0.49633169174194336\n",
      "ptend_t_54 : 0.48975905776023865\n",
      "ptend_t_55 : 0.47651633620262146\n",
      "ptend_t_56 : 0.4542097747325897\n",
      "ptend_t_57 : 0.4425702393054962\n",
      "ptend_t_58 : 0.39093196392059326\n",
      "ptend_t_59 : 0.2095847725868225\n",
      "ptend_q0001_0 : 8.587687261751853e-06\n",
      "ptend_q0001_1 : 5.340450570656685e-06\n",
      "ptend_q0001_2 : 9.093670087167993e-06\n",
      "ptend_q0001_3 : 8.505003279424272e-06\n",
      "ptend_q0001_4 : 9.545564353174996e-06\n",
      "ptend_q0001_5 : 8.692614756000694e-06\n",
      "ptend_q0001_6 : 8.076121048361529e-06\n",
      "ptend_q0001_7 : 6.556381777045317e-06\n",
      "ptend_q0001_8 : 6.7751634560409e-06\n",
      "ptend_q0001_9 : 8.937179700296838e-06\n",
      "ptend_q0001_10 : 1.5109410469449358e-06\n",
      "ptend_q0001_11 : 4.854899543715874e-06\n",
      "ptend_q0001_12 : 0.027017856016755104\n",
      "ptend_q0001_13 : 0.06375064700841904\n",
      "ptend_q0001_14 : 1.4740583896636963\n",
      "ptend_q0001_15 : 0.22232435643672943\n",
      "ptend_q0001_16 : 0.21864566206932068\n",
      "ptend_q0001_17 : 0.27730515599250793\n",
      "ptend_q0001_18 : 0.5147328972816467\n",
      "ptend_q0001_19 : 0.39965057373046875\n",
      "ptend_q0001_20 : 0.383223295211792\n",
      "ptend_q0001_21 : 0.30502018332481384\n",
      "ptend_q0001_22 : 0.2481049746274948\n",
      "ptend_q0001_23 : 0.1984248012304306\n",
      "ptend_q0001_24 : 0.1683114916086197\n",
      "ptend_q0001_25 : 0.14461641013622284\n",
      "ptend_q0001_26 : 0.13899387419223785\n",
      "ptend_q0001_27 : 0.1316319704055786\n",
      "ptend_q0001_28 : 0.1345384120941162\n",
      "ptend_q0001_29 : 0.1448090821504593\n",
      "ptend_q0001_30 : 0.1570761501789093\n",
      "ptend_q0001_31 : 0.17811191082000732\n",
      "ptend_q0001_32 : 0.20814916491508484\n",
      "ptend_q0001_33 : 0.2668791115283966\n",
      "ptend_q0001_34 : 0.32782602310180664\n",
      "ptend_q0001_35 : 0.3823545575141907\n",
      "ptend_q0001_36 : 0.4306071400642395\n",
      "ptend_q0001_37 : 0.48373451828956604\n",
      "ptend_q0001_38 : 0.5268698930740356\n",
      "ptend_q0001_39 : 0.5697033405303955\n",
      "ptend_q0001_40 : 0.5917365550994873\n",
      "ptend_q0001_41 : 0.6009968519210815\n",
      "ptend_q0001_42 : 0.6004118919372559\n",
      "ptend_q0001_43 : 0.60442715883255\n",
      "ptend_q0001_44 : 0.6065268516540527\n",
      "ptend_q0001_45 : 0.6028252243995667\n",
      "ptend_q0001_46 : 0.613685131072998\n",
      "ptend_q0001_47 : 0.6220695972442627\n",
      "ptend_q0001_48 : 0.631276547908783\n",
      "ptend_q0001_49 : 0.6428205370903015\n",
      "ptend_q0001_50 : 0.6435698866844177\n",
      "ptend_q0001_51 : 0.6344837546348572\n",
      "ptend_q0001_52 : 0.6183449625968933\n",
      "ptend_q0001_53 : 0.5944846868515015\n",
      "ptend_q0001_54 : 0.5696128010749817\n",
      "ptend_q0001_55 : 0.5379188060760498\n",
      "ptend_q0001_56 : 0.4848650097846985\n",
      "ptend_q0001_57 : 0.457197904586792\n",
      "ptend_q0001_58 : 0.3936512768268585\n",
      "ptend_q0001_59 : 0.368914932012558\n",
      "ptend_q0002_0 : 8.138318662531674e-06\n",
      "ptend_q0002_1 : 9.615941962692887e-06\n",
      "ptend_q0002_2 : 8.32778914627852e-06\n",
      "ptend_q0002_3 : 6.585183655261062e-06\n",
      "ptend_q0002_4 : 8.829120815789793e-06\n",
      "ptend_q0002_5 : 7.701221875322517e-06\n",
      "ptend_q0002_6 : 3.6680730772786774e-06\n",
      "ptend_q0002_7 : 8.278887435153592e-06\n",
      "ptend_q0002_8 : 5.947088084212737e-06\n",
      "ptend_q0002_9 : 7.497388196497923e-06\n",
      "ptend_q0002_10 : 3.7821862406417495e-06\n",
      "ptend_q0002_11 : 7.236420060507953e-06\n",
      "ptend_q0002_12 : 1.4507040759781376e-06\n",
      "ptend_q0002_13 : 6.597384071937995e-06\n",
      "ptend_q0002_14 : 8.752737812756095e-06\n",
      "ptend_q0002_15 : 8.742944373807404e-06\n",
      "ptend_q0002_16 : 4.204177912470186e-06\n",
      "ptend_q0002_17 : 3.90713285014499e-06\n",
      "ptend_q0002_18 : 5.226392204349395e-06\n",
      "ptend_q0002_19 : 9.594779839972034e-06\n",
      "ptend_q0002_20 : 9.515611054666806e-06\n",
      "ptend_q0002_21 : 9.705990123620722e-06\n",
      "ptend_q0002_22 : 9.74902377492981e-06\n",
      "ptend_q0002_23 : 9.471959856455214e-06\n",
      "ptend_q0002_24 : 9.143805073108524e-06\n",
      "ptend_q0002_25 : 9.392786523676477e-06\n",
      "ptend_q0002_26 : 9.72382076724898e-06\n",
      "ptend_q0002_27 : 0.023450469598174095\n",
      "ptend_q0002_28 : 0.05640455707907677\n",
      "ptend_q0002_29 : 0.21784748136997223\n",
      "ptend_q0002_30 : 0.3848077058792114\n",
      "ptend_q0002_31 : 0.5561830997467041\n",
      "ptend_q0002_32 : 0.5945393443107605\n",
      "ptend_q0002_33 : 0.5838892459869385\n",
      "ptend_q0002_34 : 0.5774684548377991\n",
      "ptend_q0002_35 : 0.5712723731994629\n",
      "ptend_q0002_36 : 0.5670346617698669\n",
      "ptend_q0002_37 : 0.5722234845161438\n",
      "ptend_q0002_38 : 0.5860705375671387\n",
      "ptend_q0002_39 : 0.590247392654419\n",
      "ptend_q0002_40 : 0.5830978155136108\n",
      "ptend_q0002_41 : 0.5754106640815735\n",
      "ptend_q0002_42 : 0.5774444341659546\n",
      "ptend_q0002_43 : 0.591089129447937\n",
      "ptend_q0002_44 : 0.6039829850196838\n",
      "ptend_q0002_45 : 0.6021322011947632\n",
      "ptend_q0002_46 : 0.6017800569534302\n",
      "ptend_q0002_47 : 0.5981848239898682\n",
      "ptend_q0002_48 : 0.6023703217506409\n",
      "ptend_q0002_49 : 0.607934296131134\n",
      "ptend_q0002_50 : 0.6126512289047241\n",
      "ptend_q0002_51 : 0.612880289554596\n",
      "ptend_q0002_52 : 0.6054807901382446\n",
      "ptend_q0002_53 : 0.5958830714225769\n",
      "ptend_q0002_54 : 0.5486251711845398\n",
      "ptend_q0002_55 : 0.4494171142578125\n",
      "ptend_q0002_56 : 0.3332922160625458\n",
      "ptend_q0002_57 : 0.21290767192840576\n",
      "ptend_q0002_58 : 0.12958943843841553\n",
      "ptend_q0002_59 : 0.09645377099514008\n",
      "ptend_q0003_0 : 5.819930265715811e-06\n",
      "ptend_q0003_1 : 8.388437890971545e-06\n",
      "ptend_q0003_2 : 1.0411281436972786e-05\n",
      "ptend_q0003_3 : 9.210354619426653e-06\n",
      "ptend_q0003_4 : 7.604169695696328e-06\n",
      "ptend_q0003_5 : 6.711194600939052e-06\n",
      "ptend_q0003_6 : 6.972745723032858e-06\n",
      "ptend_q0003_7 : 8.396597877435852e-06\n",
      "ptend_q0003_8 : 3.5031073366553755e-06\n",
      "ptend_q0003_9 : 7.493234988942277e-06\n",
      "ptend_q0003_10 : 1.0192224181082565e-05\n",
      "ptend_q0003_11 : 6.687492714263499e-06\n",
      "ptend_q0003_12 : 0.017481261864304543\n",
      "ptend_q0003_13 : 0.02655377984046936\n",
      "ptend_q0003_14 : 0.050433892756700516\n",
      "ptend_q0003_15 : 1.1975325345993042\n",
      "ptend_q0003_16 : 2.121037244796753\n",
      "ptend_q0003_17 : 0.8343653082847595\n",
      "ptend_q0003_18 : 0.29284873604774475\n",
      "ptend_q0003_19 : 0.40244659781455994\n",
      "ptend_q0003_20 : 0.35151922702789307\n",
      "ptend_q0003_21 : 0.3424481153488159\n",
      "ptend_q0003_22 : 0.40080514550209045\n",
      "ptend_q0003_23 : 0.4280017912387848\n",
      "ptend_q0003_24 : 0.4621414542198181\n",
      "ptend_q0003_25 : 0.4813304543495178\n",
      "ptend_q0003_26 : 0.5059670209884644\n",
      "ptend_q0003_27 : 0.49002212285995483\n",
      "ptend_q0003_28 : 0.5002546310424805\n",
      "ptend_q0003_29 : 0.49004897475242615\n",
      "ptend_q0003_30 : 0.4513409733772278\n",
      "ptend_q0003_31 : 0.4208166003227234\n",
      "ptend_q0003_32 : 0.4178207516670227\n",
      "ptend_q0003_33 : 0.4253040552139282\n",
      "ptend_q0003_34 : 0.421541303396225\n",
      "ptend_q0003_35 : 0.40971747040748596\n",
      "ptend_q0003_36 : 0.38497796654701233\n",
      "ptend_q0003_37 : 0.397562175989151\n",
      "ptend_q0003_38 : 0.4211099147796631\n",
      "ptend_q0003_39 : 0.4538787603378296\n",
      "ptend_q0003_40 : 0.4787750840187073\n",
      "ptend_q0003_41 : 0.4864274263381958\n",
      "ptend_q0003_42 : 0.49884992837905884\n",
      "ptend_q0003_43 : 0.5248963832855225\n",
      "ptend_q0003_44 : 0.5262092351913452\n",
      "ptend_q0003_45 : 0.5320965647697449\n",
      "ptend_q0003_46 : 0.5173166990280151\n",
      "ptend_q0003_47 : 0.4901857078075409\n",
      "ptend_q0003_48 : 0.4564543068408966\n",
      "ptend_q0003_49 : 0.40745100378990173\n",
      "ptend_q0003_50 : 0.37155836820602417\n",
      "ptend_q0003_51 : 0.3292009234428406\n",
      "ptend_q0003_52 : 0.27844950556755066\n",
      "ptend_q0003_53 : 0.2311905473470688\n",
      "ptend_q0003_54 : 0.18908695876598358\n",
      "ptend_q0003_55 : 0.155936136841774\n",
      "ptend_q0003_56 : 0.12714883685112\n",
      "ptend_q0003_57 : 0.1024840921163559\n",
      "ptend_q0003_58 : 0.08912813663482666\n",
      "ptend_q0003_59 : 0.07357762008905411\n",
      "ptend_u_0 : 4.404862011142541e-06\n",
      "ptend_u_1 : 4.060190804011654e-06\n",
      "ptend_u_2 : 6.11784344073385e-06\n",
      "ptend_u_3 : 6.392143404809758e-06\n",
      "ptend_u_4 : 9.415293789061252e-06\n",
      "ptend_u_5 : 9.195406164508313e-06\n",
      "ptend_u_6 : 5.585679900832474e-06\n",
      "ptend_u_7 : 5.619472176476847e-06\n",
      "ptend_u_8 : 9.343561941932421e-06\n",
      "ptend_u_9 : 8.980468010122422e-06\n",
      "ptend_u_10 : 9.10574453882873e-06\n",
      "ptend_u_11 : 8.327555406140164e-06\n",
      "ptend_u_12 : 0.06768877804279327\n",
      "ptend_u_13 : 0.06633441150188446\n",
      "ptend_u_14 : 0.05851114168763161\n",
      "ptend_u_15 : 0.06066855415701866\n",
      "ptend_u_16 : 0.24678504467010498\n",
      "ptend_u_17 : 0.11805107444524765\n",
      "ptend_u_18 : 0.052681416273117065\n",
      "ptend_u_19 : 0.07343871891498566\n",
      "ptend_u_20 : 0.10580067336559296\n",
      "ptend_u_21 : 0.14549261331558228\n",
      "ptend_u_22 : 0.17301318049430847\n",
      "ptend_u_23 : 0.19562312960624695\n",
      "ptend_u_24 : 0.22531959414482117\n",
      "ptend_u_25 : 0.2666533887386322\n",
      "ptend_u_26 : 0.297401487827301\n",
      "ptend_u_27 : 0.3188321888446808\n",
      "ptend_u_28 : 0.3543592095375061\n",
      "ptend_u_29 : 0.378635048866272\n",
      "ptend_u_30 : 0.40618741512298584\n",
      "ptend_u_31 : 0.4270399212837219\n",
      "ptend_u_32 : 0.4283556640148163\n",
      "ptend_u_33 : 0.45072391629219055\n",
      "ptend_u_34 : 0.4771459102630615\n",
      "ptend_u_35 : 0.5025888085365295\n",
      "ptend_u_36 : 0.4661293029785156\n",
      "ptend_u_37 : 0.45029938220977783\n",
      "ptend_u_38 : 0.4429944157600403\n",
      "ptend_u_39 : 0.45230743288993835\n",
      "ptend_u_40 : 0.47054916620254517\n",
      "ptend_u_41 : 0.46149304509162903\n",
      "ptend_u_42 : 0.4577620029449463\n",
      "ptend_u_43 : 0.4578808844089508\n",
      "ptend_u_44 : 0.4413842558860779\n",
      "ptend_u_45 : 0.4035293757915497\n",
      "ptend_u_46 : 0.3604867458343506\n",
      "ptend_u_47 : 0.31267356872558594\n",
      "ptend_u_48 : 0.27217361330986023\n",
      "ptend_u_49 : 0.2233748435974121\n",
      "ptend_u_50 : 0.1772409975528717\n",
      "ptend_u_51 : 0.13996997475624084\n",
      "ptend_u_52 : 0.1222052350640297\n",
      "ptend_u_53 : 0.12162958830595016\n",
      "ptend_u_54 : 0.11953070014715195\n",
      "ptend_u_55 : 0.11607675999403\n",
      "ptend_u_56 : 0.09671073406934738\n",
      "ptend_u_57 : 0.10037703067064285\n",
      "ptend_u_58 : 0.12210890650749207\n",
      "ptend_u_59 : 0.041311219334602356\n",
      "ptend_v_0 : 8.09752327768365e-06\n",
      "ptend_v_1 : 2.6170662295044167e-06\n",
      "ptend_v_2 : 1.0611969628371298e-05\n",
      "ptend_v_3 : 2.209233571193181e-06\n",
      "ptend_v_4 : 8.016309948288836e-06\n",
      "ptend_v_5 : 9.490809134149458e-06\n",
      "ptend_v_6 : 5.024875918024918e-06\n",
      "ptend_v_7 : 9.200684871757403e-06\n",
      "ptend_v_8 : 6.886073151690653e-06\n",
      "ptend_v_9 : 9.301483260060195e-06\n",
      "ptend_v_10 : 6.6889915615320206e-06\n",
      "ptend_v_11 : 7.740640285192057e-06\n",
      "ptend_v_12 : 0.0696830078959465\n",
      "ptend_v_13 : 0.0633772760629654\n",
      "ptend_v_14 : 0.052547257393598557\n",
      "ptend_v_15 : 0.06518992781639099\n",
      "ptend_v_16 : 0.06335733085870743\n",
      "ptend_v_17 : 0.07634377479553223\n",
      "ptend_v_18 : 0.06251033395528793\n",
      "ptend_v_19 : 0.0840507447719574\n",
      "ptend_v_20 : 0.11159807443618774\n",
      "ptend_v_21 : 0.12358283251523972\n",
      "ptend_v_22 : 0.14357522130012512\n",
      "ptend_v_23 : 0.15240751206874847\n",
      "ptend_v_24 : 0.16887111961841583\n",
      "ptend_v_25 : 0.19781838357448578\n",
      "ptend_v_26 : 0.22950199246406555\n",
      "ptend_v_27 : 0.2452600598335266\n",
      "ptend_v_28 : 0.22696281969547272\n",
      "ptend_v_29 : 0.22090375423431396\n",
      "ptend_v_30 : 0.22840425372123718\n",
      "ptend_v_31 : 0.2600022256374359\n",
      "ptend_v_32 : 0.26171860098838806\n",
      "ptend_v_33 : 0.2633908987045288\n",
      "ptend_v_34 : 0.28401774168014526\n",
      "ptend_v_35 : 0.3012414872646332\n",
      "ptend_v_36 : 0.29520589113235474\n",
      "ptend_v_37 : 0.2725425362586975\n",
      "ptend_v_38 : 0.25751993060112\n",
      "ptend_v_39 : 0.25825372338294983\n",
      "ptend_v_40 : 0.27146026492118835\n",
      "ptend_v_41 : 0.2831685245037079\n",
      "ptend_v_42 : 0.2899341285228729\n",
      "ptend_v_43 : 0.2944375276565552\n",
      "ptend_v_44 : 0.2964748442173004\n",
      "ptend_v_45 : 0.28732776641845703\n",
      "ptend_v_46 : 0.27669426798820496\n",
      "ptend_v_47 : 0.26370635628700256\n",
      "ptend_v_48 : 0.24933890998363495\n",
      "ptend_v_49 : 0.22235476970672607\n",
      "ptend_v_50 : 0.19688890874385834\n",
      "ptend_v_51 : 0.1663772612810135\n",
      "ptend_v_52 : 0.13035818934440613\n",
      "ptend_v_53 : 0.104792021214962\n",
      "ptend_v_54 : 0.089270681142807\n",
      "ptend_v_55 : 0.08425319194793701\n",
      "ptend_v_56 : 0.08115401118993759\n",
      "ptend_v_57 : 0.082495778799057\n",
      "ptend_v_58 : 0.08986340463161469\n",
      "ptend_v_59 : 0.03776145353913307\n",
      "cam_out_NETSW : 0.005439972039312124\n",
      "cam_out_FLWDS : 0.005486462265253067\n",
      "cam_out_PRECSC : 0.05558308586478233\n",
      "cam_out_PRECC : 0.08726556599140167\n",
      "cam_out_SOLS : 0.01344381831586361\n",
      "cam_out_SOLL : 0.02369607612490654\n",
      "cam_out_SOLSD : 0.01945192925632\n",
      "cam_out_SOLLD : 0.07778350263834\n"
     ]
    }
   ],
   "source": [
    "for n, col in enumerate(TARGET_COLS):\n",
    "    r = mse(preds[:, n:n+1], targets[:, n:n+1])\n",
    "    print(f'{col} : {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7032)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared(preds, targets)#, mask=~norm_y.zero_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3763, -1.1669, -0.4884,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1509, -1.2552, -1.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.3688,  0.5166,  0.1309,  ...,  0.0021,  0.1160,  0.0994],\n",
       "        ...,\n",
       "        [ 0.2467,  0.1420, -0.0272,  ...,  0.0320,  0.2035,  0.2017],\n",
       "        [ 1.3127, -0.3512,  1.1094,  ...,  1.6892,  0.9116,  0.2239],\n",
       "        [-0.6008, -0.4522, -0.5116,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
